Chapter 8. Security, Maintainability, and Reliability
This chapter confronts a critical aspect of vibe coding and AI-assisted engineering—ensuring that the code you produce with AI assistance is secure, reliable, and maintainable. Speed and productivity mean little if the resulting software is riddled with vulnerabilities or prone to crashing.

First, I’ll examine common security pitfalls that arise in AI-generated code, from injection vulnerabilities to secrets leakage. You’ll learn techniques for auditing and reviewing AI-written code for such issues, effectively acting as the security safety net for your AI pair programmer.

Next, I’ll discuss building effective testing and QA frameworks around AI-generated code to catch bugs and reliability issues early. Performance considerations will also be covered. AI might write correct code, but it’s not always the most efficient code, so I’ll outline how to identify and optimize performance bottlenecks. I’ll also explore strategies to ensure maintainability, such as enforcing consistent styles or refactoring AI code, since AI suggestions can sometimes be inconsistent or overly verbose.

I’ll show you how to adapt your code-review practices to an AI-assisted workflow, highlighting what human reviewers should focus on when reviewing code that was partially or wholly machine-generated. Finally, I’ll round up best practices for deploying AI-assisted projects with confidence, from continuous integration pipelines to monitoring in production. By the end of this chapter, you’ll have a toolkit of approaches to keep your AI-accelerated development safe and robust.

Common Security Vulnerabilities in AI-Generated Code
AI coding assistants, while powerful, can inadvertently introduce security issues if not guided properly. They learn from lots of public code—which includes both good and bad practices—and may regurgitate insecure patterns if the prompt or context doesn’t steer them away. It’s vital for you to know these common pitfalls so you can spot and fix them. This can include using both manual and automated means to detect potential security issues (see Figure 8-1).


Figure 8-1. AI-introduced security vulnerabilities: AI-generated code may contain subtle security flaws that require careful review and automated security scanning to identify and remediate.
Some typical security issues observed in AI-generated code include:

Hard-coded secrets or credentials
Sometimes AI outputs API keys, passwords, or tokens in code, especially if similar examples were in its training data. For instance, if you ask it to integrate with AWS, it might put a dummy AWS secret key directly in the code. This is dangerous if left in—it could leak sensitive info if the code is shared. Always ensure that secrets are properly managed via environment variables or config files. If an AI suggests something like api_key = "ABC123SECRET", treat it as a flag—real keys should not be in source code.

SQL injection vulnerabilities
If you have your AI model generate SQL queries or ORM usage, check that it’s not constructing queries by concatenating user input directly. For example, an insecure pattern would be:

sql = "SELECT * FROM users WHERE name = '" + username + "'";
This is susceptible to injection attacks. An AI might produce this if you don’t specifically tell it to parameterize queries. Always use prepared statements or parameter binding. Many AI assistants will do so if they recall best practices (like using ? or placeholders for user inputs in SQL), but it’s not guaranteed. It’s on you to verify and ask the AI to fix it if needed:

Modify this query to use parameters to prevent SQL injection.
Cross-site scripting (XSS) in web apps
When generating web code, AI tools don’t always automatically escape user input in outputs. For example, your AI might produce a templating snippet that directly inserts {{comment.text}} into HTML without escaping, which could allow a malicious script placed in a comment to run. If using frameworks, AIs often escape by default, but if they’re handling raw HTML construction, be careful. Implement output encoding or sanitization routines. You can prompt the AI:

Add sanitization for user inputs to prevent XSS.
Many modern frameworks have built-in mechanisms, so ensure that the AI uses them, like innerText versus innerHTML in Document Object Model (DOM) manipulation.

Improper authentication and authorization
AIs can write authentication flows, but subtle mistakes might creep in: for instance, generating a JSON Web Token (JWT) without a sufficiently strong secret or not checking a password hash correctly.

The same is true for authorization: an AI might not automatically enforce that an action (like deleting a resource) is limited to the user who owns that resource. These logic issues are hard to catch automatically—they require thinking through the security model. When writing such code, specify clearly:

Ensure that only the owner of the resource can delete it. Add checks for user ID.
Then test those conditions. It’s easy for an AI to omit a check because it doesn’t truly “understand” the context unless told.

Insecure defaults or configurations
AI might choose convenience over security unless prompted to do otherwise. Examples include:

Using HTTP instead of HTTPS for API calls (if TLS is not specified)

Not validating SSL certificates (some code examples on the internet use verify=false in requests, which AI might copy)

Widely enabling CORS for all origins and methods without restriction (potentially opening the app to any cross-origin requests)

Choosing outdated cryptography (like MD5 or SHA1 for hashes, which are weak, instead of SHA-256/Bcrypt/Argon2 for passwords)

These issues are often subtle, which is one reason it’s good to audit your configuration files and initialization code. If the AI sets up something like app.UseCors(allowAll) or chooses an old cipher, you should spot that and correct it.

Error handling revealing sensitive info
AI-generated error handling might print or return stack traces. For example, a Node.js API might catch an error and do res.send(err.toString()), which could leak internal details. Ensure that error messages to users are sanitized and logs are properly handled. Adjust as needed to avoid giving attackers clues like full error messages or file paths.

Dependency management and updates
If the AI adds dependencies (such as libraries) to your project, ensure that they’re up to date and from reputable sources. An AI might pick a library that was popular in its training data, but that is no longer maintained or has known vulnerabilities. For instance, if it suggests using an older version of a package, you should bump it to the latest stable. Running npm audit or equivalent after generation is wise too. Or ask the AI:

Is this library still maintained and secure?

It might not fully know, but it could tell you if there’s a known deprecation.

A 2023 large-scale analysis of GitHub Copilot in real-world projects revealed that as much as 25%–33% of generated code—depending on language—contained potential security weaknesses, including high-severity CWEs such as command injection, code injection, and cross-site scripting. These findings underscore that Copilot reflects insecure patterns present in its training data, as opposed to intentionally producing flawed code. The consistent recommendation? Developers must stay alert: manually review AI-generated code, use security-aware tooling, and maintain strict code hygiene. Especially during “vibe coding,” the speed and scope of AI-generated content demand even more vigilance. More code in less time means more surface area to audit.

Let’s look at a short example.

Improper Authentication and Authorization
Imagine you ask an AI to create a login route in an Express app. It might produce something like this:

// Insecure example
app.post('/login', async (req, res) => {
  const { username, password } = req.body;
  const user = await Users.findOne({ username: username });
  if (!user) return res.status(401).send("No such user");
  if (user.password === password) { // plain text password comparison
    res.send("Login successful!");
  } else {
    res.status(401).send("Incorrect password");
  }
});
What are the issues here?

It compares passwords directly, implying that the password is stored in plain text in the database—a big no-no.

It sends very generic responses, which may be appropriate for security but could also inadvertently expose sensitive information.

Consider authentication  error messages as a critical example. A properly secure system should return a generic message like “Invalid credentials” when login fails, regardless of whether the username or password was incorrect. However, AI-generated code might produce more specific errors such as “Username not found” or “Incorrect password for this user.”

These specific messages create a security vulnerability by confirming to potential attackers which piece of information they have correct. If an attacker receives “Incorrect password” as an error, they now know they have discovered a valid username in your system. This enables them to build a list of legitimate usernames through repeated attempts, then focus their efforts on cracking passwords for those confirmed accounts. This technique, known as user enumeration, transforms a guessing game into a more targeted attack. The AI’s tendency toward helpful, specific error messages inadvertently aids malicious actors unless you explicitly instruct it to maintain appropriately vague responses for security-sensitive operations.

While it isn’t shown in the code, no rate limiting or other protections are included here.

A secure implementation would incorporate several critical safeguards:

First and foremost, passwords should never be stored in plain text. Instead, the system should store cryptographically hashed passwords and use a secure comparison function to verify login attempts.

This comparison process must employ constant-time algorithms to prevent timing attacks, where attackers analyze response times to deduce information about password correctness. Many security-focused libraries provide these constant-time comparison functions specifically to address this vulnerability.

Additionally, the authentication system should implement rate limiting or throttling mechanisms to prevent brute-force attacks. Failed login attempts should be logged for security monitoring, allowing administrators to detect and respond to suspicious patterns. These measures work together to create a defense-in-depth approach that protects user credentials even if one security layer is compromised.

You can ask AI to help fix this:

Improve the login route to use bcrypt to hash and compare passwords, and ensure the password in the database is hashed.

It might then output:

const bcrypt = require('bcrypt');

app.post('/login', async (req, res) => {
  const { username, password } = req.body;
  const user = await Users.findOne({ username });
  if (!user) {
    return res.status(401).send("Invalid credentials");
  }
  const match = await bcrypt.compare(password, user.passwordHash);
  if (!match) {
    return res.status(401).send("Invalid credentials");
  }
  res.send("Login successful!");
});
This is better: it uses bcrypt to compare against a hashed password (assuming the variable user.passwordHash stores that). When creating users, you’d also want to make sure to use bcrypt.hash to hash their passwords.

With a bit of guidance, the AI can do the right thing, but its initial naive output might well be insecure. This underscores the pattern: review and refine.

Package Management Issues
Another common vulnerability category is package management. AI sometimes invents a library or misremembers a name, a problem known as package hallucination. Such a package might not exist, but an attacker could, theoretically, publish packages under commonly hallucinated names that contain malicious code. If you install such a package without confirming that it both exists and is the correct package, you could be introducing serious risk. If you’re not sure about a particular package, try a quick web search or check npm/PyPI directly.

Additionally, the AI might inadvertently produce code that is identical to a licensed snippet from training data. This is more an intellectual property concern than a security issue, but it warrants careful attention. GitHub Copilot, for instance, includes a duplicate detection feature that can flag when generated code closely matches public repositories, helping developers avoid potential licensing conflicts. Similar tools are emerging to address this specific challenge of AI-generated code provenance. Chapter 9 will delve into licensing and intellectual property considerations in more detail, providing comprehensive guidance on navigating these complex issues.

In summary, the main message remains—and yes, I realize I’ve emphasized this point throughout the book to the point where you could probably recite it in your sleep—that AI output requires the same careful review you would apply to a junior developer’s code. The repetition is intentional, because this principle underpins virtually every aspect of safe and effective AI-assisted development. Whether you’re prototyping, building backends, or implementing security features, this mental model provides the right balance of trust and verification to make AI a powerful ally rather than a risky shortcut. It can write a lot of code fast, but you need to instill security best practices into it and double-check for vulnerabilities. Novelist Frank Herbert put it this way in an often-quoted line from God Emperor of Dune (Putnam, 1981): “They increase the number of things we can do without thinking. Things we do without thinking—there’s the real danger.”

Using AI can lull you into doing less thinking about routine code, and you should be consciously thinking about how to apply a security-review mindset. It’s crucial for catching those “things we can do without thinking.”

Security Audits
Given the types of vulnerabilities outlined, how can you effectively audit and secure our AI-generated code? This section looks at several techniques and tools you can employ.

Leverage Automated Security Scanners
Static analysis tools (SASTs) can scan your code for known vulnerability patterns; for example:

ESLint + security plug-ins can detect insecure functions or unsanitized input in JavaScript and Node code.

Bandit for Python can flag uses of assert in production, weak cryptography, hard-coded secrets, and more.

GitHub CodeQL lets you run queries across your codebase to find SQL injection, XSS, and other common patterns.

Semgrep has rules for many languages, including community-maintained ones for JavaScript, Python, Java, Go, and more, and can spot top issues out of the box.

You can integrate these tools into your CI/CD or dev pipelines. Run them on your AI-generated code—it won’t catch everything, but it will probably flag the obvious mistakes (e.g., plain-text password checks, unsanitized SQL, insecure crypto). It’s a solid safety net.

Use a Separate AI as a Reviewer
Two distinct approaches can leverage AI for security review of generated code, each with unique advantages. The first involves using the same AI model that generated the code, asking it to switch perspectives and audit its own output. After generating code, you can prompt the model with something like this:

Review this code for security vulnerabilities and explain any issues you find.

This approach often yields surprisingly effective results, as the model can identify common security problems such as plain-text password storage, missing input validation, or potential SQL injection vulnerabilities.

The second approach employs a different AI model as an independent reviewer. For instance, if you generated code using ChatGPT, you might paste that code into Claude or Gemini for security analysis. This cross-model review can surface different perspectives and catch issues the original model might have overlooked, much like how different security tools or human reviewers bring varying expertise and focus areas. Different models may have been trained with different emphases or datasets, potentially catching distinct categories of vulnerabilities.

Both techniques serve as valuable additional layers of security review, complementing but never replacing proper security testing and human expertise. While AI reviewers may occasionally flag false positives or miss subtle vulnerabilities, they excel at catching common security antipatterns quickly. Think of this process as automated pair programming focused specifically on security considerations. The key lies in treating these AI-generated security reviews as another input to your security assessment process rather than as definitive security clearance.

Perform a Human Code Review with a Security Checklist
If you’re in a team, have a checklist for reviewing code with an eye to security. AI often produces code that “works” for the expected case but isn’t hardened to deal with malicious cases. For AI-generated code, be sure to consider:

Authentication flows: Are they solid?

Any place data enters the system: Are we validating inputs?

Any place data leaves the system: Are we sanitizing outputs? Are we protecting sensitive data?

Use of external APIs: Are we handling failures? Are we exposing keys?

Database access: Are we using ORMs safely? Are we using parameterized queries?

Memory management in low-level code: If AI is writing C/C++ or Rust, are there overflows? Is there any misuse?

Penetration Testing and Fuzzing
Use dynamic approaches. For fuzz testing, feed random or specially crafted inputs into your functions or endpoints to see if they break or do weird things. AI can help generate fuzz cases, or you can use existing fuzz tools, such as OSS Fuzz by Google.

Running penetration-testing tools like OWASP’s ZAP against your AI-made web app can automate scanning for things like XSS and SQL injection vulnerabilities. For example, ZAP might attempt to inject a script and get it reflected, and detect that a certain input isn’t sanitized.

If you’re building an API, tools like Postman or custom scripts can try sending ill-formed data to see how the system behaves: does it throw a 500 error or handle errors gracefully?

Add Security-Focused Unit Tests
For critical pieces of code, write tests that assert security properties. For instance, you might test that your login rate limiter triggers after X bad attempts, or that certain inputs (like "<script>alert(1)</script>") come out escaped in the response. To test that unauthorized users cannot access a protected resource, simulate both authorized and unauthorized calls and ensure the app behaves correctly.

You can ask the AI to help generate these tests:

Write tests to ensure an unauthorized user gets 403 on the /deleteUser endpoint.

And then run the tests.

Provide Updates to Compensate for Training Cutoffs
AI models possess a fundamental limitation that directly impacts security: their knowledge freezes at a specific point in time. When a model completes training, it cannot learn about vulnerabilities discovered afterward, security patches released subsequently, or new best practices that emerge. This knowledge cutoff creates a critical gap between what the AI knows and current security standards.

Consider a model trained in 2023 generating code in 2025. During those intervening years, numerous security vulnerabilities have been discovered, patched, and documented. New attack vectors have emerged, frameworks have added security features, and best practices have evolved. The AI, however, remains unaware of these developments unless you explicitly provide updated information within your prompts.

This limitation becomes particularly acute with rapidly evolving security standards and vulnerability databases. The OWASP Top 10, for instance, undergoes periodic updates to reflect the changing threat landscape. If you prompt an AI to “write a secure file upload function,” it might implement reasonable protections based on its training data—perhaps including file type validation, size limits, and storage outside the web root. However, it could miss recently discovered attack vectors or fail to implement newly recommended mitigations.

The solution involves actively supplementing the AI’s knowledge with current security information. When requesting security-sensitive code, include references to current best practices in your prompts. For example, rather than simply asking for secure code, you might prompt:

Write a file upload function that addresses the security concerns in the 2025 OWASP Top 10, particularly focusing on injection attacks and server-side request forgery.

This approach grounds the AI’s response in current security standards rather than potentially outdated training data.

Similarly, framework-specific security features often emerge after an AI’s training cutoff. Express.js applications, for instance, benefit significantly from the Helmet middleware for setting security headers. An AI trained before Helmet became standard practice might generate Express applications without this crucial security layer. By explicitly mentioning current security tools and practices in your prompts, you help the AI generate code that aligns with contemporary security standards rather than historical ones.

Optimize Your Logging Practices
Ensure that the code (AI and human) has good logging, especially around critical operations or potential failure points. This helps in debugging issues in production. If an AI wrote a section with minimal logs, consider adding more. For example, if there’s an AI-generated catch block that just swallows an error, change it to log the error (and maybe some context) for visibility. Also, sanitize the logs so they contain no sensitive info.

Use Updated Models or Tools with a Security Focus
Some AI coding tools aim to blend code generation with built-in security scanning. Snyk is a prime example: it uses a hybrid approach combining LLM-generated suggestions with rule-based taint analysis. According to Snyk, when you request code (even from LLM libraries like OpenAI, Anthropic, or Hugging Face), Snyk Code tracks potentially unsafe data flows and flags untrusted inputs before they reach sensitive sinks. In practice, that means if an AI suggests a database query, Snyk ensures it’s parameterized, preventing SQL injection—even if you forget to do so yourself. This kind of tool is particularly useful because it works to avoid introducing insecure code through AI-generated suggestions.

Pay Attention to Warnings in Context
If you’re using an IDE, often you’ll see warnings or squiggly lines to highlight suspicious code. Modern IDEs with IntelliSense can sometimes catch, for instance, a string concatenation of SQL that looks suspicious. Don’t ignore those warnings and flags just because the AI writes them—address the issue. The AI doesn’t have the benefit of those real-time warnings when generating the code.

Slow Down
After using AI to generate a lot of code quickly, shift gears and slow down when it’s time for auditing. When you can produce features fast, it’s tempting to chase the next one, but schedule time for a thorough review. Think of it as “AI-accelerated development, human-accelerated security.” Snyk’s best practices recommend scanning AI code right in the IDE, and caution against letting AI’s speed outpace your security checks. In other words, integrate security scanning into your dev loop, so you can catch vulnerabilities as soon as the code is written.

In summary, when you audit AI-generated code, you’ll use many of the same tools you use in traditional development—static analysis, dynamic testing, code review—but you might apply them more frequently, because code is produced more quickly. Treat every AI output as needing inspection.

Building Effective Testing Frameworks for AI-Generated Systems
While security forms one pillar of reliability, the broader concept encompasses the fundamental dependability of your software system. Reliability, in software architecture terms, addresses critical questions about system failure and its consequences. Does your system need to be fail-safe? Is it mission critical in ways that could affect human lives or safety? If the system fails, will it result in significant financial losses for your organization? These considerations determine the rigor required in your development and testing practices.

When you’re building with AI assistance, these reliability stakes remain unchanged. A banking application generated with AI assistance carries the same requirements for transaction accuracy and data integrity as one written entirely by humans. A healthcare system must meet identical standards for patient safety regardless of how its code originated. The AI’s involvement in code generation does not diminish these fundamental reliability requirements.

This reality underscores why comprehensive testing becomes even more critical in AI-assisted development. A strong testing framework ensures that your code performs its intended functions correctly and maintains that correctness as the project evolves. While testing AI-generated code follows the same fundamental principles as testing human-written code, certain nuances and opportunities emerge from the AI development process that warrant specific attention.

The following sections explore how to leverage AI not just in generating code but in creating robust test suites that validate reliability, maintain system stability, and provide confidence that your software will perform correctly when the stakes are highest.

First, embrace automated testing early and often. It’s easy to skip writing tests when development is slow because you want to push features. Ironically, when development is fast (with AI), it’s also easy to skip tests, because new features keep coming at you. But when code is churned out rapidly, that’s precisely when you most need tests to catch regression or integration issues. So after implementing a feature with AI help, get into the habit of immediately writing tests for it (or even using AI to write those tests). This verifies the feature and also guards it as you change things later.

A 2022 study found that developers who were using an AI assistant were more confident in the security of the code they wrote even when it was objectively less secure than code written by those without AI assistance. You need to counteract that overconfidence with actual tests.

As I noted in Chapter 4, you can use the AI not just to generate the code but also to produce a suite of tests. This way, AI helps double-check itself. It’s like having it do both the implementation and an initial pass at validation. For example, after writing a new module, you could ask:

Write unit tests for this module, covering edge cases.

If they pass, great. If they fail, either there’s a bug or the tests expected something else. Investigate and fix either code or test as appropriate.

Be cautious that the AI may assume some output or behavior incorrectly; treat its tests, like its code, as suggestions, not the ground truth. You might need to adjust the test’s expectations to match the intended behavior—but even that process is valuable, because it forces you to define the intended behavior clearly.

Incorporate your test suite into a CI pipeline that runs on every commit. This way, whenever AI-generated code is added or changed, all tests run automatically. If something breaks, you’ll catch it early. Sometimes AI might introduce subtle breaking changes (like changing a function signature or output format slightly), and a robust test suite will detect that. Include security scans in the CI too (like npm audit or static analysis) so that any new introduction of a risky pattern is flagged. Types of tests to try include:

Property-based testing and fuzzing
Property-based testing (with tools like Hypothesis for Python or fast-check for JavaScript) is another valuable technique. Instead of writing individual test cases with specific inputs and expected outputs, you define high-level properties that your code should always satisfy. The framework then generates a wide range of inputs to check whether those properties hold.

Take sorting as an example. Rather than asserting that sort([3, 1, 2]) === [1, 2, 3], you can define properties:

The output should be in order

It should contain the same elements as the input

The tool then generates dozens or hundreds of input arrays to test those conditions—and finds edge cases you might not think of manually.

This can be especially useful for AI-generated code. If your AI writes a function to normalize email addresses (such as by lowercasing the domain), a property test might check that the output is idempotent—meaning running the function twice gives the same result as running it once. If an edge case violates that invariant, the test framework will generate a counterexample to help you diagnose the bug.

Load and performance testing
AI might write code that’s not optimized. It’s a good idea to test your system under load. This is reliability in terms of performance. Use tools like JMeter, Locust, or k6 to simulate many requests or heavy data and see if the system holds up. If not, identify the bottlenecks.

For instance, maybe the AI writes a naive O(n^2) algorithm that works fine on 100 items but will tank at 10,000. Without performance tests, you might not notice that until it’s in production. So incorporate some performance scenarios, if applicable. Time some critical operations with increasing input sizes, or use profiling tools to see where CPU time or memory goes for heavy tasks.

Error handling
Intentionally cause errors to ensure the system responds gracefully, such as:

For an API, shut down the database and see if the API returns a friendly error or crashes. If it crashes, add code (or ask AI to add code) to handle DB connection errors.

For the frontend, simulate the backend returning 500 errors and ensure the UI shows an error message, not a blank page or infinite spinner.

AI might not think of these failure modes on its own when writing code, so you have to test them and then refine. Testing these scenarios will improve reliability by prompting you to add proper fallback logic, retries, or user feedback.

Monitoring and logging
Incorporate logging and perhaps use the logs in tests for verification. For instance, if a certain action should trigger an audit log entry, test for that. AI can generate log lines; verify they print out as expected.

Also, think about setting up monitoring (like an in-memory simulation of how your service will be monitored in production). For example, you might track if any uncaught exceptions are logged during test runs. If yes, treat it as a test failure; that means there’s some case not properly handled.

Maintainability
Maintainability testing, like ensuring code style and standards, is important. Use linters and formatters to keep code consistent, since AI can produce slightly different styles from different prompts. A formatting tool like Prettier or Black (for Python) can unify style. For more logical consistency and to catch overly complex AI-generated code that might need refactoring, consider adding linting rules that enforce things like function complexity limits. (See “Ensuring Maintainability in AI-Accelerated Codebases” for more.)

Once your tests are in place, you can refactor AI code more confidently. Perhaps the AI produces a working but clunky solution; you can improve it and rely on tests to ensure you haven’t broken its behavior. You might even ask AI to refactor its own code:

Refactor this function for clarity while keeping it passing the current tests.

If your tests are good, you can check that the refactoring didn’t break anything.

Understanding nondeterminism in AI systems requires distinguishing between two fundamentally different scenarios. When AI operates at runtime in production systems, such as a chatbot responding to customer queries or a recommendation engine personalizing content, the outputs can vary even with identical inputs. This variability stems from factors like model temperature settings, random seeds, or evolving model states. Testing such systems requires specialized approaches that account for acceptable variation ranges rather than expecting exact matches.

However, AI-assisted code generation presents a different paradigm entirely. Once an AI generates code and that code is committed to your repository, it becomes as deterministic as any human-written code. The function that calculates tax rates will produce the same output for the same input every time, regardless of whether a human or AI originally wrote it. This determinism is crucial for system reliability and makes traditional testing approaches entirely applicable to AI-generated code.

The more subtle challenge emerges when integrating multiple AI-generated components, each potentially created in isolation with different implicit assumptions. Consider a concrete example from an ecommerce system. You might prompt an AI to generate an order processing module, instructing it to handle international orders. Separately, you ask the AI to create a shipping calculation service for the same system. The order processing module, following American conventions, formats dates as “12/25/2024” for December 25. Meanwhile, the shipping service, perhaps influenced by European examples in its generation, expects dates formatted as “25/12/2024.” Both components function perfectly in isolation, passing their individual unit tests.

The mismatch only surfaces during integration testing when the order processor passes a date to the shipping calculator. The shipping service interprets “12/01/2024” as January 12 rather than December 1, potentially calculating shipping times based on the wrong month entirely. This type of assumption mismatch is particularly common with AI-generated components because the AI might draw from different examples or conventions when generating each piece independently. Comprehensive integration testing that exercises the actual data flow between components becomes essential for catching these subtle incompatibilities before they cause production failures.

The QA process for AI-assisted projects might require a bit more creativity, since AI can introduce unusual edge cases. For instance, an AI might output a feature you didn’t explicitly consider—if so, test that as well. If it added a hidden behavior, either remove it or properly test it.

Finally, if possible, test your application in an environment similar to production, with a realistic data load. Sometimes performance issues only appear with larger data volumes or higher concurrency. Use those test results to pinpoint inefficiencies.

Performance Optimization
While the AI often writes correct code, it may not always write optimal code. LLMs don’t inherently do performance analysis; they typically reproduce what is common in their training data. Therefore, be vigilant about potential performance issues, especially in critical paths or for large-scale use.

You can even chat with the AI for hints about performance optimization:

What is the complexity of this code? Can it be improved?

This function is slow—any ideas on how to make it faster?

It might not always be right, but it can sometimes give useful suggestions or at least confirm your thinking.

That said, don’t overoptimize, and don’t optimize prematurely or where it’s not needed. Sometimes the AI solution is perfectly fine, if the data sizes are small or the operation infrequent. Use your profiling data to focus on real bottlenecks and optimize the parts that really need it. The advantage of vibe coding is that you haven’t spent a ton of time handcrafting code from scratch, so you can afford to let some noncritical parts be simple and not superoptimized, as long as they don’t impact user experience or cost. This approach aligns with agile practices: make it work, then make it fast (if needed).

Here are some areas to cover as you ensure your AI-augmented project runs efficiently:

Complexity analysis
When the AI generates an algorithm, take a moment to consider its complexity. Sometimes it will use a brute-force solution where a more efficient algorithm exists. For example, it might double-sort a list because it didn’t recall a single-step method, resulting in O(n log n × 2) where O(n log n) could do (the capital O stands for memory usage). Or it might use nested loops that make an operation O(n2) when there’s a known O(n) approach. If you spot something like that, ask for improvements:

Can we optimize this to avoid nested loops? Perhaps use a set for lookups.

The AI often will oblige and give a better solution if you hint at the approach. If not, you might have to implement that part manually.

To identify slow functions, run a profiler or measure execution time of key code paths with representative or worst-case data. If something is too slow, you can attempt to optimize manually or with AI assistance:

Optimize this function, which is currently a bottleneck; try to reduce its complexity.

The AI might restructure the code for performance. Use tests to make sure it still works.

For critical algorithms, write a small benchmark harness. If AI gives you a piece of code to, say, compute something, test it against another approach, or at least measure how it scales with input size. You might decide to rewrite in a more efficient way if needed.

Memory usage, leaks, and retention
AI-generated solutions might use more memory than necessary: reading entire files into memory instead of streaming, for example, and thus holding large data structures. If your use case involves big data, check your system’s memory usage and optimize by streaming or chunking if needed. For instance, if you need to process millions of records, you’d want to refactor your AI-generated function loadAllRecords() to process them in batches or stream from the database.

Also check that the AI-generated code is releasing resources. In languages like Java or C#, maybe it opens a file or DB connection and doesn’t close it. In a frontend single-page app, maybe event listeners aren’t removed, leading to leaks. Tools can help (like Chrome dev tools’ Memory Inspector for frontends or Valgrind for C++ leaks), but often just reading the code helps. Identify these and fix them. If you see an open file handle not closed, add a close in a finally block.

Concurrency and parallelism
If you’re using languages that support threads or async, look for places where the AI code might be single-threaded when it could be parallel. AI might not automatically use async/await where appropriate, and may not know to offload a heavy CPU task to a worker thread. Identify such opportunities. For example, for I/O-bound tasks in Node or Python, ensure asynchronous usage so that the system doesn’t block. For CPU-bound tasks, maybe the AI can’t help much in code, but you might decide to implement in a more performant language or offload to a background job.

Caching
A common performance optimization that AI doesn’t always automatically add is to cache results of expensive operations. Look at your code: is it recalculating something repeatedly? If so, implement caching (either in-memory or using an external cache like Redis). You can prompt AI:

Add caching to this function to avoid redundant calculations.

It may implement a simple memorization or suggest using a caching library.

Database query optimization
If your application uses a database, examine the queries the AI creates. Are they using indexes properly? Perhaps the AI wrote SELECT * where only a few columns are needed. Or it’s fetching extensive data to filter in code, creating performance bottlenecks like the N + 1 query problem. These inefficiencies require optimization by pushing more work to the database or leveraging proper indexing.

For instance, if the generated code calls findOne repeatedly within a loop, resulting in multiple database round trips, you can refactor this into a single batch query using WHERE id IN (...). Similarly, if the AI omitted index creation in a migration for frequently queried fields, adding those indexes becomes essential for maintaining acceptable performance. The AI often generates functionally correct but suboptimal database interactions that require human expertise to identify and resolve.

To illustrate, let’s take an example. Suppose AI writes you a function that merges two sorted arrays by simply concatenating and sorting the result: (O(n log n))—even though there’s a known linear algorithm it could be using to merge two sorted lists (like merge step or merge sort, O(n)). In code review, you realize this could be a bottleneck for large arrays, so you prompt AI to implement the linear merge:

Optimize the mergeSortedArrays function to perform the merge in linear time without using built-in sort.

The AI recognizes this as the classic merge algorithm and writes it. The solution passes your tests, so congratulations: you gained performance without sacrificing correctness.

AI-assisted development doesn’t remove the need for performance tuning; it just shifts when you do that tuning. You’ll often get a correct solution first (which is extremely valuable), then turn your attention to measuring and optimizing targeted parts. When you do need to optimize something, the AI can help, as long as you guide it on what you need.

Ensuring Maintainability in AI-Accelerated Codebases
A codebase’s maintainability describes how easy it is to modify, extend, and comprehend over time. Some worry that AI-generated code could be messy or inconsistent, especially if multiple suggestions have varying styles or patterns. This section covers several practices you can use to address these concerns and keep your vibe-coded project clean and maintainable.

While Prompting
As you prepare your prompts, a few things to keep in mind:

Use consistent coding standards
Use linters and formatters to enforce a consistent style. As mentioned, AI might sometimes use different naming conventions or formatting in different outputs. Running a formatter (like Prettier for JS, Black for Python, gofmt for Go, etc.) on all code after generation ensures it conforms to a unified style. This makes reading code much easier (no cognitive load switching styles). Additionally, define naming conventions for your project and stick to them. If the AI outputs get_user_data in one place and fetchUserData in another, decide which convention you prefer (snake_case versus camelCase, etc.) and refactor to one style.

Use architectural patterns to encourage modularity and avoid sprawl
Encourage the AI to write modular code by prompting it to separate concerns. For example, instead of asking it to write one huge file implementing everything, break the work into tasks:

Create a UserService class for user logic.

Create a separate module for sending emails.

This leads to a codebase that’s logically divided. It’s easier to maintain when each module has a clear responsibility. You can guide the architecture:

Put database access code in a separate file or class from the API routing code.

Because it’s so very easy to add features when using AI, it’s crucial to guard against feature creep and code sprawl. Without disciplined architectural thinking, you risk your codebase devolving into what software architects call a big ball of mud: an antipattern where code lacks clear structure or boundaries. This risk intensifies with AI assistance, as the friction traditionally associated with adding features disappears, potentially accelerating architectural decay.

To combat this, ground your AI-assisted development in proven architectural patterns and principles. When instructing AI, explicitly reference the patterns your project follows:

Add this new feature following the repository/service pattern used in the project.

Implement this using the hexagonal architecture established in our domain layer.

This specificity helps maintain consistency even as features accumulate rapidly.

For developers seeking deeper architectural grounding, several foundational texts provide essential guidance:

Design Patterns: Elements of Reusable Object-Oriented Software (Addison-Wesley, 1994) by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides (the “Gang of Four”) remains the definitive catalog of reusable design solutions.

Fundamentals of Software Architecture: An Engineering Approach by Mark Richards and Neal Ford offers comprehensive coverage of architectural patterns and principles across technology stacks.

Domain-Driven Design: Tackling Complexity in the Heart of Software by Eric Evans (Addison-Wesley, 2003) provides crucial techniques for aligning software design with business domains—particularly valuable when AI generates code that must reflect complex business logic.

These resources equip you to guide AI tools effectively, ensuring generated code adheres to sound architectural principles rather than contributing to technical debt. Remember: AI excels at implementing patterns but cannot determine which patterns are appropriate for your specific context. That architectural judgment remains fundamentally human.

Working with Code Output
Once the AI responds with generated code, maintainability techniques to use include the following:

Refactor continuously
Don’t hesitate to refactor AI-generated code when needed. Sometimes the first pass is correct but not ideally structured: for example, the AI might write a very long function or duplicate its logic in two places. A common challenge is unintentionally duplicated code: the AI might not realize two functions do similar things and create both. If you notice similar blocks, refactor to one. Tools like code linters can detect duplicates (there are linters for too-similar code). Running those could highlight places to “DRY out” (don’t repeat yourself).

To ask the AI to help refactor, you could prompt:

Refactor this code to remove duplication and improve clarity.

It might create helper functions or simplify some logic. Always test after refactoring.

Test
This chapter has already covered testing, so I’ll just note that a good test suite makes maintenance easier. When you or others modify code in the future (possibly with AI again), your tests will catch if the changes break anything, so you can refactor or change implementations with peace of mind. Testing decouples “what it does” from “how it does it,” giving you flexibility to maintain or improve “how” without altering “what.”

Avoid excessive complexity or overrelying on AI-specific constructs
Sometimes the AI might use a clever trick or less common function that other developers might not know. While that’s not inherently bad, consider maintainability: if an average developer would scratch their head at the code, maybe simplify it. For instance, if AI uses a bit of regex magic or list comprehension that’s too terse, rewrite it in a more explicit loop for clarity (or at least comment it).

Similarly, an AI trying to be helpful might overengineer a solution, like adding layers that aren’t needed. For instance, maybe a direct approach was fine, but the AI introduced an abstraction that isn’t pulling its weight. Remove it to keep things straightforward. Simpler code is usually easier to maintain.

Build in resilience and fallbacks
Think about fallback strategies in case of failures. For example, if an AI-coded component calls an external API and that API is down or returns unexpected data, do we have a fallback (like using cached data or a default response)? Implementing such resilience patterns (circuit breakers, retries with backoff, etc.) can make the system more robust. The AI likely won’t do this on its own unless asked. Ensure the system can handle partial failures gracefully. One microservice going down shouldn’t take the whole app down, if possible. Use timeouts and fallback logic.

Follow-Up
Once you’re satisfied with the code, a few more practices help to keep it maintainable:

Provide thorough documentation and comments
Make sure the code is properly documented. AI often writes minimal comments unless prompted. You can request docstrings or comments with prompts:

Add comments to explain the purpose of each section in this code.

Write a docstring for this function.

These can save future readers time. The AI can usually generate fairly good explanations but sometimes misexplains subtle points, so review for accuracy.

Also consider maintaining a high-level documentation (like a README or design doc) for the project, describing its architecture, main components, and so on. You can largely write this yourself, but AI can help by summarizing the codebase if needed.

If you encounter some quirk like “The AI always names this parameter weirdly,” mention it in your dev notes for others. It’s part of the new collaborative environment. If it’s just you using the AI-generated code, a few quirks are fine—but if others join the project, they might wonder, “Why is this thing named like that?” Perhaps just standardize those names.

There’s also an aspect of maintainability in terms of knowing which pieces of code were AI-generated and which were human-written. It’s not strictly necessary to label, but some teams might comment, “Generated with the help of GPT-4 on 2025-05-01” for traceability. Ideally, flag anything you’re unsure about in your PR description: “Used ChatGPT to help with this function; it seems to work, but please check the error-handling logic carefully.”

This isn’t a widespread practice. It can be helpful during code review, but you might not need it if a human has already reviewed the code and it’s now just code. If you do keep any transcripts or prompts, you could link them in comments for complicated code: “This algorithm derived via GPT-4, based on prompt X; see docs for derivation.” A reviewer doesn’t need to treat it differently in terms of scrutiny (you should scrutinize all code), but it can help to understand the context. For example, if code has a certain style mismatch or an odd idiom, knowing it came from AI might clue the reviewer in that this isn’t a deliberate authorial choice but an AI artifact.

Code reviews and team norms
If you’re working in a team, have all team members review code—even if one person and AI cowrote it. They might spot awkward patterns or things that break team norms. Over time, you’ll develop a sense of how to prompt the AI to match your team’s style (maybe including specifics in system prompts or initial guidelines). If multiple developers use AI, make sure everyone knows the desired style patterns so they can prompt accordingly (like “Write this in functional style” or “Use async/await, not callbacks”). See the next section for some tips on code review with AI code.

Track technical debt
If, during development, you accept an AI solution that you know isn’t ideal, track it as technical debt in your comments or the project to-dos: “TODO: The solution works but is O(n2); if data grows, optimize this,” or “TODO: This uses a global variable for simplicity; refine this later.” The AI can even insert TODO comments itself if you ask:

If there are any areas that need future improvement, add to-do comments.

Just address those to-dos eventually.

Learn from AI patterns
If AI introduces a design pattern or library you’re not familiar with, take time to learn more about it rather than ignoring it. Understanding a particular caching approach or a library it uses will help you maintain or modify that part confidently in the future. If it’s too arcane, you might decide to remove it in favor of something you know—but sometimes AI can pleasantly surprise you with a useful library or pattern you didn’t know. If it’s a well-known solution that you and the team can learn, this can even improve maintainability.

In practice, maintainability comes down to applying the same good software-engineering principles as always—just applying them to code that was partially written by AI. Fortunately, because AI reduces the grunt work, you may have more time to focus on cleaning up the code and writing docs, which improves maintainability.

Some companies report that after an initial burst of generating code with AI, they invest time in a “hardening sprint” to refactor and document it all. Consider alternating between generation-heavy sprints and cleanup sprints as a potential strategy.

Code Review Strategies
As discussed in Chapter 4, code review is a critical process in traditional development and remains so in AI-assisted development. This section discusses some nuances to consider when a chunk of the code under review is machine-suggested. Because AI can produce code so quickly, it’s reasonable to worry that code review will become a bottleneck—but don’t let that worry hamper the review process. It’s crucial to allocate proper time for reviews. Don’t skimp on the assumption that “we wrote it fast, let’s merge fast.” If anything, commit smaller changes more frequently to make reviews easier (generally a good practice anyway). Frequent, smaller pull requests (PRs) are easier to review thoroughly than one giant PR. The AI can help break tasks into smaller PRs as well, if you plan accordingly.

Don’t assume code is correct just because “the AI wrote it and the tests pass.” Think critically and try to reason through the logic. If possible, test it mentally or with additional cases outside the provided tests, because tests might not cover everything. You can also run the code and even experiment by running a snippet with a tricky input to see if it behaves.

Code reviews can also be important learning moments. If the AI introduces a novel solution that is actually good, the reviewer might learn something new while verifying its correctness. Similarly, if the AI/human combination does something suboptimal, the reviewer can explain a better approach. Over time, this feedback loop can improve how the team uses AI (like helping everyone understand which things to avoid or ask differently). In a sense, code review helps to close the human learning loop, since the human author should learn and understand anything the AI wrote that is new to them.

When you review code, your first priority should be making sure it meets the requirements and intended design. Does this code do what the feature/bugfix is supposed to? Does it cover any edge cases mentioned in the specifications? If the prompt is off, AI might solve a slightly different problem: maybe it handles a case that wasn’t needed or misses a case. This is normal, but watch that the developer didn’t just accept AI output that only partially addresses the issue. For example, an AI might produce code to format a date but assume a certain time zone, which might or might not align with requirements.

If something in the code isn’t obvious, ask the author to explain how it works or why it’s done that way. If they struggle to explain or reach for “the AI did it and I assume it’s right,” that’s a red flag. The team should understand everything in the codebase. Encourage the author to double-check with the AI or documentation and provide a proper explanation, possibly as a comment in code.

Pay attention to the security and performance vulnerabilities discussed earlier in this chapter, too, and if any known best practice is violated, call it out—like if output isn’t escaped (in web dev) or if you find credentials in the code.

Request changes or refactoring if you see code that works but could be simpler or more in line with team style:

The AI created 3 separate functions for different user roles that mostly duplicate each other. Can we merge these into one function with a parameter for role?

The code’s author can then do so (maybe with AI’s help). If the AI suggestion didn’t use the team’s consistent style or standard libraries, mention that too:

We usually use the requests library for HTTP calls, but this code is using http.client. Let’s stick to requests for consistency.

The author can then prompt the AI to rewrite using the preferred library.

If the AI has written something really complex, like a tricky algorithm, consider discussing it with another reviewer or the team for a deeper review.

You may want to try some of the emerging tools that use AI to assist in code review—like GitHub’s Copilot for Pull Requests, which can generate summaries and flag potential bugs and other issues. Such a tool might highlight something like “This code snippet is similar to one in module X with slight differences” (pointing out possible duplication). These hints can complement the human review but should not replace it.

Finally, be respectful and constructive in your reviewing, even when the code has flaws due to AI. Avoid blaming the developer for what could be an AI artifact: while they are still responsible for their code, recognize the context. AI is a tool, and both author and reviewer are working with it. The goal is to improve the code and share knowledge, not point fingers. For example: “This part seems to have a security issue⁠—likely an oversight from the AI suggestion; let’s fix it.”

Ultimately, code review in vibe coding is how we fully exercise the human intelligence side of the human/AI partnership. It’s where oversight and expertise come in to catch what the AI might miss and to keep the quality bar high. It’s also a knowledge-sharing moment for the team, since discussing code in reviews spreads understanding of both the domain and how to best use AI.

Code review also formalizes the concept of “developers as editors” introduced by Grant Gross in CIO: the reviewer is an editor, making sure the code is polished and fit for production. This aligns perfectly with vibe coding as a concept, where the vibes (AI suggestions) are there but human judgment refines them.

Best Practices for Reliable Deployment
Once you know your code is secure, tested, and maintainable, you need to deploy it and keep it running reliably in production.

While AI-assisted development doesn’t alter the core principles of software deployment, it does introduce considerations around deployment velocity and operational complexity. For those seeking comprehensive coverage of deployment fundamentals, The DevOps Handbook (IT Revolution Press, 2016), by Gene Kim, Jez Humble, Patrick Debois, John Willis, and Nicole Forsgren, provides the definitive guide, covering everything from continuous integration and deployment pipelines to monitoring, security, and organizational transformation. This foundational knowledge becomes even more critical when AI accelerates your ability to generate deployable code, as the principles ensure your deployment practices can scale with your increased development velocity.

Before and During Deployment
As you ramp up to deployment, consider the following best practices:

Automate your CI/CD pipeline
Given the fast pace of AI development, a robust continuous integration/continuous deployment (CI/CD) pipeline is valuable. Every commit (with or without AI-generated code) should be built, tested, and potentially deployed through an automated pipeline. This reduces human error and confirms that all deployment steps (tests, lint, security scans) are consistently run. If AI code introduces something that breaks the build or fails the tests, the CI will catch it immediately. Also, an automated CI/CD pipeline allows for quick iteration, so you can patch any AI-introduced issues and deploy fixes rapidly.

Infrastructure as code
Use infrastructure as code (Terraform, CloudFormation, etc.) to define your deployment environment. While not directly related to AI coding, it’s part of reliable deployments. You could even use AI to help write Terraform scripts, but treat those with the same caution and testing as other AI code, including perhaps testing them in a sandbox before applying them to production. A valuable starting point is the book Terraform: Up & Running (O’Reilly, 2022), by Yevgeniy Brikman, which provides a comprehensive introduction to the principles and practices of IaC with Terraform.

Use staged rollouts—and have a rollback plan
Use staged rollout strategies like deploying to a staging environment or a canary release before full production rollout. This way, you can catch anything you’ve overlooked before it affects all users. For example, you might deploy a new AI-coded feature to 5% of users and monitor (with metrics and logs) for any errors or performance issues. If all is good, roll it out to 100% of users.

Always have a rollback plan. Despite all tests and reviews, sometimes things slip through. If a new release goes wrong, be ready to revert to the last stable version. If you’re using a containerization strategy like Kubernetes, maintain previous deployments for quick switchback. If it’s a serverless function, keep the previous version alive until you’re confident in the new one.

Set up observability
Set up comprehensive monitoring in production, of both system metrics and application logs:

Use tools like Sentry to track errors and capture exceptions. If the AI code throws an unexpected error in production (perhaps an edge case wasn’t covered), you’ll get an alert so you can fix it.

Use performance-monitoring tools like application performance monitoring (APM) to track response times, throughput, and memory usage. This will show you if any code in the new deployment has introduced a slowdown or memory leak.

Monitor availability: for instance, ping the service endpoints to confirm they’re up. If something crashes (maybe due to some untested scenario), an alert should fire, so you can react quickly.

Stay vigilant about security
Make sure that secrets like API keys are handled properly in deployment. For example, if your AI wrote code that expects a secret in an environment variable, set up that secret in the CI/CD or cloud config, so it’s not accidentally logged or exposed. Use secret management tools like HashiCorp Vault (HashiCorp Vault offers secrets management, key management, and more with many integrations) or AWS Secrets Manager (AWS Secrets Manager allows you to securely store and rotate secrets like database credentials, API keys, and tokens, and can integrate with CI/CD tools like GitHub). Also, if you’re using container images, scan them for vulnerabilities.

Test using techniques like blue-green deployments or shadow testing
For major changes, consider a blue-green deploy. This involves setting up two identical production environments: “blue” (the current live version) and “green” (the new version). Traffic is initially directed to the blue environment. Once the green environment is ready and tested, traffic is switched over to it. If any issues arise with the green environment, traffic can be quickly rerouted back to the blue environment, minimizing downtime and risk. This method tests the new version in a full production setting before making it the sole live version.

Alternatively, if a specific AI-coded algorithm change is risky or you want to validate its behavior with real-world data without impacting users, you could shadow test it. This involves deploying the new version alongside the current live version. Real production inputs are fed to both versions in parallel. However, only the current version’s outputs are shown to users. The outputs from the new (shadow) version are collected and compared against the current version’s results to evaluate its performance, accuracy, and stability. If the shadow version’s results are satisfactory and performance is good, you can then confidently switch it to be the active version.

Ongoing Best Practices
After deployment, these strategies can help keep everything running reliably:

Create operational runbooks
Provide runbooks for the ops team that describe any special aspects of the AI-generated parts of the code: “This service uses an AI model for X; if the model output seems erroneous, try restarting service or check the model’s version.” Or “Feature Y heavily uses caching to perform well; if performance issues arise, check the cache hit rate.” Essentially, document any operational considerations that might not be obvious. If AI has introduced a dependency (like using a temp file), note that, so ops will know to monitor disk space and the like.

Test in production
In addition to testing during development and as part of the rollout, some companies do testing in production (TiP) in safe ways, like running continuous small experiments. For instance, you might use feature flags to turn on an AI-generated feature for a small subset of users and see if any error rates change. This overlaps with canary releases, but you can make it more granular using feature toggles.

Audit regularly
Schedule periodic security and performance audits of the codebase, especially as more AI contributions accumulate. This is similar to managing tech debt: it helps you catch things that were fine at first but that could turn problematic as the scale or context changes. Watch for “drift,” too—if AI code is generating SQL queries, make sure that your migrations and code stay in sync and that the deployment runs migrations properly before new code takes traffic.

Keep humans in the loop
The theme continues—humans should monitor the automations. AI might help you write code, but it won’t fix a production incident at 2 a.m. Have someone on call who understands the system. Over time, you might enlist AI for troubleshooting help like analyzing logs (a feature of some emerging tools), but at the end of the day, a human should make decisions about fixes.

Learn from failures
No process is 100% perfect. If an error gets through your defenses and causes an incident, do a postmortem. Identify if the problem was related to AI usage (like “We trusted the AI code here and it failed under scenario X”), and update your processes and tests to prevent that class of issue. Doing this kind of analysis every time continuously improves reliability.

Reliability isn’t just about code, of course; it also involves the infrastructure and operations around the code. AI helps mostly on the code side. Robust operational practices (which can be partially assisted by AI) keep the overall system reliable.

In essence, treat an AI-heavy project the same as any high-quality software project when it comes to deployment: employ thorough testing, roll out gradually, monitor heavily, and make sure you can roll back quickly. Because AI can create changes faster, you may end up deploying more frequently (which is fine, if your CI/CD pipeline is good). Frequent small deployments are actually known to reduce risk compared to infrequent big ones. The reason is that each individual change is smaller, making it easier to identify and fix any issues that arise. If a problem occurs, rolling back a small change is also simpler and faster. This approach contrasts with large, infrequent releases where numerous changes are bundled together, making it difficult to pinpoint the cause of any problems and increasing the potential impact of a failed deployment.

By following these best practices, you can be confident that even though a lot of its code was machine-generated, your system as a whole will behave reliably for users. The combination of automated testing, careful deployment, and monitoring closes the loop to catch anything that slipped through earlier stages. As a result, you can reap the speed and productivity benefits of AI development without sacrificing your ability to trust your software in production.

Summary and Next Steps
In summary, vibe coding does not remove the need for engineering rigor—it amplifies the productivity of the engineers who apply that rigor. Your mantra should be the old Russian proverb: Trust but verify. Trust the AI to handle the grunt work, but verify everything with your tools and expertise.

Security and reliability are one dimension of responsible development; ethics is another. AI-assisted coding raises important questions about intellectual property, bias, the impact on developer jobs, and more. Chapter 9 will delve into those broader implications. How can you use AI coding tools responsibly and fairly? How do you deal with licensing of AI-generated code and ensure your models and prompts are used ethically?


Chapter 9. The Ethical Implications of Vibe Coding
As AI-assisted development becomes increasingly commonplace, it’s critical to address the ethical and societal implications of this new paradigm. This chapter steps back from the technical details to examine vibe coding through an ethical lens: these new development methods can be effective, but they also need to be implemented responsibly and to benefit individuals and society at large.

I begin with questions of intellectual property (IP). Who owns the code that AI generates, and is it permissible to use AI outputs that may be derived from open source code without attribution? From there, I consider bias and fairness. Transparency is another focus: should developers disclose which parts of a codebase were AI-generated, and how can teams ensure accountability for code quality and bugs?

I outline responsible development practices in AI usage, from establishing transparency and accountability to avoiding sensitive data in prompts to ensuring accessibility and inclusivity. The chapter finishes with a set of guidelines for using AI tools responsibly.

Legal Disclaimer
The following section touches on complex legal topics, particularly concerning copyright and intellectual property law, from a primarily US perspective. Legal systems and interpretations are evolving worldwide, especially concerning artificial intelligence. This information is for educational purposes only and does not constitute legal advice. You should consult with a qualified intellectual property lawyer before making any decisions based on this information, especially if you have concerns about the ownership or licensing of code you or an AI tool generates.

Intellectual Property Considerations
Who owns AI-generated code? And does using it respect the licenses and copyrights of the source material on which the AI was trained? AI models like GPT have been trained on huge swaths of code from the internet, including open source repositories with various licenses (MIT, GPL, Apache, etc.). If the AI generates a snippet that is very similar (or identical) to something from a GPL-licensed project, using that snippet in a proprietary codebase could inadvertently violate the GPL, which generally requires sharing derivative code.

According to open source norms and general copyright principles, small snippets of a few lines might not be copyrightable if they lack sufficient originality to be considered an independent creative work, or their use could potentially be considered de minimis (too trivial to warrant legal concern). However, anything substantial or expressing a unique creative choice is more likely to be protected by copyright. It’s crucial to understand that “open source” does not mean “public domain.” By default, creative work, including code, is under exclusive copyright by its author. Open source licenses explicitly grant permissions that would otherwise be restricted by copyright law.

If you want to know more about open source norms, good places to start include the following:

The Open Source Initiative
The OSI defines and promotes open source software, maintains the Open Source Definition, and approves licenses that meet its criteria.

The Free Software Foundation (FSF)
The FSF advocates for “free software” (which has a strong overlap with open source principles) and is the steward of licenses like the GNU General Public License (GPL).

Project-specific documentation
Individual open source projects typically include LICENSE files, README files, and CONTRIBUTING guidelines that detail the terms of use and contribution for that specific project.

Community and legal resources
Websites like GitHub offer extensive documentation and discussions on open source practices. Organizations like the Linux Foundation and legal information sites also provide valuable resources on open source compliance and legal aspects.

The question of whether using small code snippets overlaps with the fair use doctrine (in the US; “fair dealing” in many other jurisdictions) is complex and highly fact-dependent. Fair use permits limited use of copyrighted material without permission for purposes such as criticism, comment, news reporting, teaching, scholarship, or research. US courts typically consider four factors to determine fair use:

The purpose and character of the use (commercial versus nonprofit, transformative versus duplicative)

The nature of the copyrighted work (highly creative versus factual)

The amount and substantiality of the portion used in relation to the copyrighted work as a whole

The effect of the use upon the potential market for or value of the copyrighted work

While some might argue that copying very small, functional code snippets for interoperability or to access uncopyrightable ideas could fall under fair use, especially if the use is transformative, this is not a clearly settled area of law for code, and there’s no universally agreed-upon number of lines that is definitively “fair use” or de minimis. The safest course is often to get permission or to understand the underlying idea and rewrite the code in your own way. The U.S. Supreme Court case Google LLC v. Oracle America, Inc. addressed fair use in the context of software APIs, finding Google’s reimplementation of Java API declaring code to be fair use, but this was a specific and complex ruling focused on API declarations, not all code. It’s generally understood that copyright protects the specific expression of an idea, not the idea, procedure, or method of operation itself.

Typically, the developer using the AI is considered the “author” in the sense that the AI is a tool, similar to a compiler or a word processor. Thus, if code is generated in a work context, the developer’s company would likely own the code produced by the developer using the tool, subject to the AI tool’s terms of service and underlying IP issues. However, the terms of service (ToS) of AI tools are critical. Most ToS grant the user rights to the output they generate. OpenAI’s ToS, for instance, states, “You own the outputs you create with GPT-4, including code.”

This “ownership,” however, needs careful consideration. It generally means that the AI provider isn’t claiming ownership of what you create with their tool. But this assumes you have the rights to the inputs you provide, and it doesn’t automatically mean the output is itself eligible for copyright protection or that it’s free from third-party intellectual property claims. If you input your own original code to the tool for modification or extension, the output is most likely yours (or your employer’s), again, subject to how the AI processes it and what it incorporates from its training data. But if you input someone else’s copyrighted code to fix or transform, the output might be considered a derivative work of that third-party code.

In the US and many other jurisdictions, whether AI-generated output that is substantially similar to training data, or output based on copyrighted input, constitutes a derivative work is a subject of ongoing legal debate and lacks full clarity. Don’t feed large chunks of copyrighted code that isn’t yours (or licensed appropriately) into an AI tool, because the output could be deemed a derivative work and thus fall under the license of that original copyrighted code.

Given these uncertainties, to be safe, treat AI-generated code as if it’s under an ambiguous license, and only use it if you are comfortable that it doesn’t infringe on existing copyrights and that you can comply with any potential open source license obligations. Regarding the copyright status of the AI output itself, the US Copyright Office has stated that works generated solely by AI without sufficient human authorship are not copyrightable. If a human significantly modifies or arranges AI-generated material in a creative way, that human contribution might be copyrightable but not the AI-generated elements standing alone. Thus, it’s often wise to assume that purely AI-generated outputs might not be copyrightable by anyone or that copyright would extend only to the human’s creative contributions.

This is not a hypothetical worry. In fact, there’s ongoing legal debate. A prominent class-action lawsuit, Doe v. GitHub, Inc., was filed against GitHub, Microsoft, and OpenAI, claiming that GitHub Copilot produces code that is too similar to licensed open source code without proper attribution or adherence to license terms.  While some claims in this case have been dismissed or are under appeal (as of mid-2025, the case involves ongoing proceedings, including an appeal to the Ninth Circuit regarding DMCA claims and remaining breach of contract claims), it highlights a genuine concern: AI can and sometimes does regurgitate or closely paraphrase copyrighted code from its training data.1

An older (but still relevant and later substantiated) study by GitHub itself noted that, in some cases, Copilot’s output included suggestions that matched training data, including rare instances of longer verbatim snippets. While most AI tools are designed to avoid direct, extensive copying of identifiable code unless specifically prompted or dealing with very standard algorithms, the risk exists. Furthermore, it’s not just open source code that’s a concern; numerous lawsuits have been filed by authors, artists, and media companies alleging that their fully copyrighted, privately owned intellectual property was used without permission or compensation to train large language models and other generative AI systems. The challenge with proprietary code is that, unlike open source, it’s often not publicly visible, making it harder for an end user to confirm if an AI’s output is inadvertently similar to such private code.

Nevertheless, the ethical and prudent practice is to act as if any code you accept from an AI tool is your responsibility. Thoroughly review, test, and understand any AI-generated code before incorporating it into your projects, and ensure its use complies with all applicable licenses and copyright laws.

What to Do If You Get Suspicious Output
If an AI output seems like a verbatim or near-verbatim copy of known code (especially if it includes distinctive comments or author names), treat it carefully. Consider running a similarity check using a plagiarism detector tool, or do a web search for unique strings to see if you find any matches that could indicate copying.

Another principle to follow is When in doubt, leave it out. Either avoid using the output or make sure it’s under a compatible license and give attribution if required. For example, if Copilot spits out a well-known algorithm implementation that you recognize from Stack Overflow or an open source project, cite the source or rewrite it in your own way, using the AI’s answer as a guide but not quoting it verbatim.

If you suspect the output matches an existing library solution, consider including the library itself instead (with proper license). You can also prompt the AI:

Please provide an original implementation rather than one copied from a library.

It might then synthesize a more unique solution. (There’s no guarantee it won’t be influenced by its training code, but at least it will try to not copy outright).

The ethics here also touch on not using AI to willfully strip attribution. For example, it would be unethical to copy code from Stack Overflow via AI without attribution to circumvent a policy that you should credit the answer. That erodes trust in the open knowledge ecosystem. It’s better to incorporate the material with proper credit. Depending on the circumstances, that might mean the following:

If an AI writes a code comment from some source that has an author’s name (like copying a snippet with “John Doe 2018” in a comment), you should keep that or move it to a proper attribution section with a full citation rather than deleting it. That respects the original author’s credit.

If an AI provided a solution that you know comes from a known algorithm or code snippet, cite that source as you normally would if you had looked it up yourself.

If an AI tool creates something arguably creative (like a unique approach or text for documentation), acknowledge its contribution. Though it doesn’t have rights, it’s about transparency (and maybe a nod to the tech).

Some open source licenses (like MIT) are permissive enough that including copied code with attribution would satisfy the license. Others, like GPL or AGPL, would “infect” your whole codebase if you include that code, which is undesirable for closed projects.

In short: if you suspect the AI has given you something that might cause IP issues, either avoid using it or transform it sufficiently to ensure you’re complying with any possible license.

Gray Areas
Even as I write this, AI tools continue to raise new questions about IP, copyright, and ethics. For instance:

If your vibe coding includes using AI to generate noncode assets like documentation text, config files, or images, similar IP questions arise. For instance, if you generate an icon image via an AI tool that was trained on copyrighted images, who owns that new image?

If an AI writes a significant part of a software product, should the original authors of the code on which the AI was trained get credit?

Could someone claim that your AI-generated code infringes on their copyright because it looks similar to theirs? If sections of nontrivial lengths are possibly identical, this is where similarity checking comes in.

There’s an emerging notion that AI companies might need to implement license-respecting filters or allow teams to opt out of their code being included in AI training data. It’s evolving, but developers on the ground should act conservatively to not violate rights.

It will take time for courts to settle all of the legal issues, but in the meantime, intellectual honesty and respect should guide us. If AI uses a known algorithm from a published paper, cite the paper in a comment. If it uses a common open source helper code, credit the project. It’s about respect for authorship. If you recognize where something came from, err on the side of giving credit. It’s a good practice that fosters transparency.

Remember that under the hood, the AI’s knowledge comes from thousands of developers who shared their code publicly. Ethically, the software industry owes that community the respect of upholding open source licenses and norms. Give credit where it’s due and don’t abuse others’ work under the guise of “the AI wrote it, not me.”

Transparency and Attribution
Transparency refers to being open about the use of AI in your development process and outputs, and attribution refers to giving proper credit when AI-derived code comes from identifiable sources.

Transparency is important for the sake of accountability. For example, if AI-generated code introduces a bug or security flaw, being transparent that “this code was AI-suggested” might help you analyze the root cause—perhaps an ambiguous prompt should be rewritten. In code comments or a project’s README or documentation, you might mention generally that “this project was built with assistance from AI tools like ChatGPT.” Or get more specific: “Added a function to parse CSV (generated with ChatGPT’s help, then modified).” It’s a bit like acknowledging your use of frameworks or libraries.

Transparency is also key to trust: stakeholders (your team, clients, end users, or industry regulators) might want to know how your software was developed and validated. If an AI was involved in code generation, some stakeholders might wrongly trust it too much or too little. Transparency allows a conversation about reliability: “Yes, we used AI, but we tested it thoroughly” or “This part was tricky—we had AI generate the initial code, but we’ve since verified it.”

Attributions are also expected or required in many academic venues. Some open source projects restrict or even forbid AI contributions due to IP concerns, so check the contributor guidelines before using AI. Being transparent with maintainers if a patch was AI-generated helps them evaluate it, especially if licensing is a worry.

In fact, some highly regulated industries require software vendors to disclose any AI use for auditing purposes. The EU’s AI Act mandates transparency for automated decision making that affects individuals (such as credit-scoring algorithms). If vibe coding leads to such systems, it becomes a legal/ethical necessity to inform users that “recommendations are generated automatically and may reflect patterns in data.”

Similarly, if your product feeds user data or proprietary data like user-provided code examples into an AI model to fine-tune it and help program its analysis, you might need to say in the privacy policy that user data may be used with permission to improve AI models (as always, do consult a lawyer for legal matters). Transparency intersects with privacy here.

It’s also just generally ethical to acknowledge the tools and sources you use. If 30% of your code was generated by Copilot, it’s fair to mention that in your documentation or internal communication—not to diminish your own role but to be honest about the process.

Some developers might fear admitting that AI helped, worried that it could undermine their perceived contribution or skill or be seen as “cheating.” As vibe coding becomes more normalized, this stigma should decrease; eventually, you might be seen as behind the times if you’re not using the AI available to you. We need to normalize AI as a tool—it’s no more “cheating” than using Stack Overflow or an IDE.

On the flip side, providing too many disclaimers could cause undue worry. If you tell a client, “We used AI to code this product,” they might question its safety (even if that’s due to misconceptions). It’s important how you phrase it. Emphasize quality measures in the same breath: “We utilized advanced coding assistants to speed up development, and all AI-generated code was rigorously reviewed and tested to meet our quality standards.”

In sum, transparency and attribution foster trust and community values. They ensure that credit flows to human creators and that we remain honest about how our software is built. It’s akin to an artist listing their tools or inspirations; it doesn’t diminish the art; it contextualizes it. If, like me, you want vibe coding to be accepted widely, being open about using AI and how you mitigate its risks is important.

Bias and Fairness
As you know well by this point in the book, AI models’ output reflects the data they’re trained on. If that data contains biases or exclusionary patterns, the models can produce outputs that are biased or unfair.

You might ask: “How can code be biased? It’s not like an LLM is making hiring decisions or something.” But bias can creep into your coding in subtle ways:

Code often reflects assumptions on its creators’ part. User-facing text or content the AI generates might reflect cultural biases or insensitive language present in its training data. For instance, Microsoft’s Tay, an early chatbot in 2016, infamously learned to parrot racist and misogynistic slurs from Twitter interactions within hours of launch.

Assumptions can also be geared toward specific cultural norms, like a middle-class North American lifestyle (such as assuming car ownership or universal access to certain technologies). A notable example of unexamined assumptions leading to exclusionary products was the initial 2014 release of Apple’s Health app, which lacked a period tracker—a significant oversight likely stemming from a lack of diversity and perspective on the design team. Even in example code, comments, or synthetic data, the model might always use he/him pronouns, reinforcing gender bias.

It is well known that code repositories and the broader software development landscape predominantly reflect Western perspectives and English speakers. As a result, an AI trained on these repositories might overlook crucial internationalization aspects, such as proper support for Unicode and multibyte characters (essential for languages like Chinese, Japanese, Korean, Arabic, Hindi, and many others using non-Latin or syllabary scripts), or it might default to English-centric examples for things like type names. Developers must bring awareness and design and code for internationalization, even if the AI doesn’t spontaneously do so.

If writing algorithms, be wary of certain variables like race, gender, age, etc. The AI might not spontaneously include them unless asked, but if it hallucinates some criteria or if you’re using an AI like Code Assistant on a dataset, apply fairness constraints; the AI won’t inherently know the moral or legal context.

Beyond just coding, models can mirror data bias in their content domain: the historical biases present in their training data. For example, consider an AI tasked with writing code for a credit-scoring algorithm for loan approvals. In the United States, credit scoring systems have a documented history of reflecting and perpetuating racial biases. These biases stem from historical practices like redlining and other forms of systemic discrimination that have had lasting financial repercussions, particularly for Black communities and other marginalized groups. (See Richard Rothstein’s The Color of Law [Economic Policy Institute, 2017] for a comprehensive history of how government policies segregated America.)

If the training data reflects these historical biases, the AI might incorporate discriminatory variables, such as using zip codes (which can be a proxy for racial demographics due to segregated housing patterns) or other seemingly neutral data points that correlate with protected characteristics. If not properly guided, the AI might produce code that leads banks to make unfair lending decisions, thus perpetuating historical inequalities and affecting real people’s lives. Similar issues arise in areas like predictive policing algorithms, where historical arrest data (itself potentially biased) can lead to AI systems that disproportionately target certain communities.

Similarly, if you’re using specialized models (like an AI code assistant fine-tuned for, say, medical software), ensure the model isn’t locked into biases from that domain’s data. For example, historically, some medical guidelines were biased by research studies that predominantly used male subjects, leading to misdiagnoses or less effective treatments for other genders. If AI is recommending code or solutions for medical diagnostics, you need to double-check that it doesn’t inadvertently encode those biases.

There are tools emerging to detect bias in AI outputs, though these are more common in GPT models used to generate content, and AI providers themselves attempt to filter overtly biased or toxic outputs. Code-oriented AIs rarely produce hate speech spontaneously, but it’s good that they have content filters for it. Building in ethical constraints means, in many AI tools, that if a user tries to get the AI to create malware or discriminatory algorithms, it will refuse. Don’t try to break those filters to get unethical outputs.

There are lots of other ways to recognize and mitigate bias at different stages of the development process, though. These include:

Testing with diverse examples
If your AI generates user-facing components or logic that deals with human-related data, test it with diverse inputs. For example, if an AI-generated form validation expects “First Name” and “Last Name,” does it allow single names, which are common in some cultures? If not, that’s a bias in assumption. If it generates sample usernames, are they all like “JohnDoe”? If so, consider incorporating more diversity in the examples.

Prompting for inclusivity
You can explicitly instruct the AI to be neutral or inclusive: “Generate examples using a variety of names from different cultures.” If it always refers to the user as “he,” you might prompt:

Avoid gendered language in this code comment; use neutral phrasing or they/them pronouns.

Also, be cautious about jokes or examples the AI might produce that could be culturally insensitive; you can prompt it to use a professional tone to avoid that. The AI will usually comply. It doesn’t have an agenda; it just outputs what seems normal to it, unless told otherwise. We shape that “normal.”

Hiring diverse teams
Having a diverse team review outputs can catch issues. For example, someone might say, “Hey, our AI always picks variable names like foo/bar, which is fine, but in documentation, all of its personas are male-typed.” Then you can correct that systematically. If all developers are from similar backgrounds, they might not catch a subtle bias. If possible, involve people from underrepresented groups—or at least consider their perspectives—when reviewing AI usage guidelines.

In summary, bias and fairness are about using vibe-coding tools to produce code that is fair to users of all backgrounds and that doesn’t reflect—or, worse, perpetuate—historical discrimination. The way we use these tools in teams should also be fair to developers and other colleagues of varying levels and backgrounds. See Chapter 4 for a discussion of the ethical implications of how AI tools are changing workplaces, especially for junior developers.

Golden Rules for Responsible AI Use
Bringing together a lot of what we’ve covered, it’s worth articulating a set of responsible practices for vibe coding:

Always keep a human in the loop.

Again: never let the AI work unsupervised. Responsible AI-assisted dev means you, the developer, are reviewing every line and making decisions, not deploying raw AI output without human validation.

Take responsibility for your code.

If something goes wrong, it’s not the AI’s fault—it’s the development team’s responsibility. Keeping that mindset avoids complacency. Be prepared to justify your code, whether you wrote it from scratch or accepted AI code. If someone asks you, “Why does the code do this?” don’t say, “I don’t know; Copilot did that.” That’s why one of Chapter 3’s golden rules is “Never commit code you don’t fully understand.” That’s responsible engineering.

Protect users’ privacy and ask for their consent.

Ethically, you owe it to users and your company to keep their secret data secret. When using AI tools, especially cloud-based ones, be careful not to expose sensitive data in your prompts or conversations. For instance, if you’re debugging an issue with a user database, don’t feed actual user records to ChatGPT. Use sanitized or synthetic data instead.

Many tools now allow users (or at least business users) to opt out of having their input data used for training. If you’re an enterprise user, use those settings or use on-prem solutions for sensitive code. If you do feed any user data to a model, or if any AI functionality directly touches users (like a chatbot in your app that uses an LLM), get users’ consent and allow them to opt out if appropriate. A warning like “This feature uses an AI service; your input will be sent to it for processing” is transparent and lets privacy-conscious users decide for themselves.

Comply with laws and regulations.

Keep an eye on legal requirements around AI, which are constantly evolving. For instance, data protection laws like the EU’s General Data Protection Regulation (GDPR) and AI Act consider some AI outputs as personal data if they include any personal data. Training a model on users’ data might require those users’ consent. Regulatory bodies may classify code generation as “general AI” and impose transparency or risk management obligations. Stay informed and work closely with your legal and compliance professionals to avoid breaking any regulations.

While this should go without saying, do not use AI to generate malware, exploit code without ethical justification, or automate unethical or illegal practices.2 While an AI could probably write a very effective phishing email or code injection attack, using it for that purpose violates ethics, the laws of most countries, and likely the AI’s terms of service. Focus on constructive use.

Foster a responsible AI culture in your organization.

If your team adopts vibe coding, encourage discussions about ethics and provide relevant ethics training. Consider having developers and code reviewers use a brief checklist like the one in Figure 9-1.


Figure 9-1. Responsible AI development checklist: essential validation steps including intellectual property review, bias assessment, and security audits before integrating AI-generated code into production systems.
Everyone should feel responsible for ethical AI use; it’s a collective effort, not just the burden of the individual using the tool at any given moment. To formalize this, consider designating an “ethics champion” or a small ethics committee within your team or organization. This individual or group wouldn’t be the sole owner of ethics (as that responsibility remains shared), but they would take the lead on:

Staying abreast of the latest developments in AI ethics, emerging best practices, and new regulatory landscapes

Facilitating discussions about ethical considerations in specific projects

Championing the integration of ethical principles into the development lifecycle

Helping to curate and disseminate relevant resources and training materials to the broader team

Acting as a point of contact for team members who have ethical questions or concerns

Since this field is moving incredibly fast, it’s crucial to work as a team to stay updated on new versions of AI tools and their capabilities, limitations, and evolving best practices for responsible use.

Since this field is moving fast, work as a team to stay updated on new versions of AI tools and best practices. One important concept to integrate into your workflows is the use of model cards. Model cards are essentially standardized documents that provide transparency about a machine learning model. Think of them as nutrition labels for AI models. They typically include details about:

What the model is, its version, and when it was developed

The specific use cases the model was designed and tested for

Scenarios where the model should not be used, due to limitations or potential for harm

How well the model performs on various benchmarks, including evaluations for fairness and bias across different demographic groups

Information about the datasets used to train the model, including any known limitations or biases in the data

Potential risks and societal implications and any mitigation strategies employed

Whenever you are using a pretrained model or evaluating a model for use, look for its model card. If you are fine-tuning or developing models, creating your own model cards is a best practice.

Create guardrails and safety nets.

Practicing responsible design means that your AI-generated systems should have safety nets. For example, if AI suggests an out-of-bounds index fix that might mask an underlying issue, it’s better for the system to fail safely than to cause silent errors. If an AI-generated recommendation system might be wrong, providing ways for users to correct or override it shows respect for their human agency. Strive to build systems that degrade gracefully if AI components misbehave.

Document AI usage decisions within your team.

Keep an internal log of why you used certain AI suggestions (or didn’t): “We tried AI for module X, but it tended to produce too much duplicate code, so we wrote that part manually.” This can help you refine your processes, provide context to new team members about AI’s role in the codebase’s history, and augment your team’s collective memory. It can also be useful during audits.

Proactively work to avoid bias, discrimination, and unfairness.

Be vigilant for signs that your AI usage could lead to discrimination, and work to avoid such situations before they happen. For example, if your app is global, is your AI multilingual or does it favor those who speak English? Do all of your team members have equal access to AI tools and training?

Responsible AI Checklist
Prompting and code generation (developers)

Confirm that your prompts contain no confidential or sensitive data such as client info, PII, or secrets.

Check licensing for all output and confirm it includes no proprietary or GPL code, unless allowed. Use tools like FOSSA for scanner checks.

Test output for bias to ensure code and comments don’t reinforce stereotypes or discrimination.

Confirm security hygiene by prompting for safe defaults. Confirm the code avoids insecure patterns (eval, unsanitized input).

Specify any constraints in prompts, including style, framework, performance needs, and compatibility guidelines.

Code review checks (developers and code reviewers)

Verify that no embedded copyrighted material is used in the code unless licensed.

Confirm that attribution and credit are given when due.

Audit the logic, language, and naming for bias and fairness—especially in user/UI-facing layers.

Ensure that the code doesn’t facilitate harm, misuse, manipulation, or discrimination.

Validate your input sanitization, data handling, and logging, and check for secret leaks.

Confirm the code’s functionality and correctness via unit tests, edge cases, error handling, and test coverage.

Check for inefficient or power-hungry patterns.

Check dependencies to ensure they include no unvetted libraries or hidden license risks.

Check for readability and maintainability: the code should follow style guides and use clear naming conventions.

Check that any unused code has been removed.

Confirm that code comments explain the code’s intent, especially for AI-generated logic.

Confirm that your code-review feedback is respectful, specific, and empathetic.

Governance and process (organization)

Confirm that integrated license scanners, audit logs, and provenance tracking are in place.

Provide training in ethics and AI-assisted coding, and share updates regularly.

Maintain a vetted list of AI tools; prohibit unapproved or high-risk ones.

Put an incident process in place, with escalation channels and whistleblower options for anyone who discovers unethical code.

Monitor responsible AI metrics, such as bias incidents, security findings, and license violations. Maintain a checklist of these metrics and revise it periodically.

Solicit and listen to community feedback. Include diverse perspectives via retrospective meetings or external audits.

How to Use This Checklist
Customize this list to include questions specific to your organization and business domain, as well as your team’s tech, risk tolerance, and values.

Start small: begin with key questions like “Did we avoid sensitive data?” and “Did we scan for licenses?”

Integrate checks and checklists into your workflow via PR templates, CI pipelines, and code-review tools.

Schedule reviews of this checklist every quarter or after major incidents. Use these reviews to iterate on the list, adding new items or deleting unneeded ones.

Treat this checklist not as a rigid rulebook but as a conversation starter, just as pilots and surgeons do with their checklists.

As the AI landscape continues changing and growing, the software industry is likely to introduce AI standards or certifications. It’s early, but your company could even help shape those guidelines by engaging in standardization efforts, like IEEE or ISO working groups on AI software engineering. Ethically, it’s better for the dev community to help set the rules than to leave it solely to regulators or the courts.

Summary and Next Steps
Responsible vibe coding means integrating AI into the software development lifecycle in a way that respects all stakeholders: original creators (by respecting their IP), colleagues (through transparency and fairness), users (through privacy, security, and fairness in outcomes), and society (by not letting misuse cause harm). It’s about leveraging AI’s strengths while diligently guarding against its weaknesses.

I’ve often said that vibe coding is not an excuse for low-quality work. It’s not an excuse for ethical shortcuts either. As the humans in charge, developers must ensure that speed doesn’t compromise values.

Next, Chapter 10 looks at a new technology that’s changing the way we work with AI models: autonomous coding agents.

1 Case information can often be found on court dockets, like those for the US District Court for the Northern District of California and the Ninth Circuit Court of Appeals, or through legal news outlets and case trackers.

2 There are some ethically justified exceptions. Penetration testers and security researchers can ethically use AI to find vulnerabilities that should be fixed, as long as they work under responsible disclosure protocols.


Chapter 10. Autonomous Background Coding Agents
Autonomous background coding agents are rapidly emerging as the next evolution of AI coding tools.  Unlike familiar “copilot” assistants that suggest code while you type, these agents operate more like background junior developers you can dispatch to handle entire tasks asynchronously. Code is generated in an isolated environment spun up for the agent, tests can be run, and the result often comes back as a fully formed pull request for you to review.

In this section, I’ll explore what background coding agents are, how they work, the current landscape of tools (OpenAI Codex, Google Jules, Cursor, Devin, and more), and how they compare to traditional in-IDE assistants. I’ll also examine their capabilities, limitations, and the pragmatic changes they signal for the future of software engineering.

From Copilots to Autonomous Agents: What Are Background Coding Agents?
Traditional AI coding assistants (like Cursor, GitHub Copilot, or VSCode extensions like Cline) are supervised coding agents—interactive helpers that respond to a developer’s prompts or inline context. They’re essentially autocomplete on steroids, generating suggestions in a chat or as you write, but the human developer is in the driver’s seat guiding every step.

In contrast, autonomous background coding agents operate with much greater independence. You give them a high-level task or goal, then “send them off” to work through the problem on their own, without constant supervision. These agents will read and modify your codebase, formulate a plan, execute code (even running tests or commands), and produce a result (often a commit or pull request)—all in an asynchronous workflow.

Think of the difference between a copilot and an autopilot: your copilot (much like GitHub Copilot) is always in the cockpit beside you, awaiting your input; the autopilot (background agent) can fly the plane on its own for a while. This autonomy means that background agents can tackle multistep coding tasks while you focus elsewhere. Using async agents like Codex and Jules is like expanding your cognitive bandwidth: you can fire off a task to the AI and forget about it until it’s done. Instead of a single-threaded back-and-forth with an AI, you suddenly have a multithreaded workflow: the agent works in parallel with you, much like a competent junior dev working in the background.

Crucially, background agents operate in isolated development environments (often cloud VMs or containers) rather than directly in your editor. They typically clone your repository into a sandbox, install dependencies, and have the tools needed to build and test the project. For security, these sandboxes are restricted (with rules like “No internet access unless explicitly allowed”) and ephemeral. The agent can run compilers, tests, linters, and the like without any risk to your local machine. When the task is complete, the agent outputs the code changes (diffs) and a summary of what it did. Usually this comes through as a pull request (with code diffs, commit message, and sometimes an explanation), which you can then review and merge.

To sum up, a background coding agent is an AI-powered autonomous coder that understands your intent, works through an entire task in a sandbox environment by reading and writing code and testing it, and then delivers the results for you to review. It’s not just suggesting a line or two—it can handle larger-scope tasks:

Write a new feature X across the codebase.

Refactor module Y for efficiency.

Upgrade this project’s dependencies.

This is a significant shift in how we might incorporate AI into development workflows, moving from assistive suggestions to delegating actual implementation work.

How Do Autonomous Coding Agents Work?
Under the hood, most background agents follow a similar pattern of operation: plan, execute, verify, and report. Let’s walk through these steps and their capabilities.

Plan
When you give an agent a task (typically via a prompt or command describing what you want), the agent first parses the request and formulates a plan of attack. Some agents explicitly show you this plan before proceeding. For example, Google’s Jules presents an execution plan that you can review and tweak before it starts coding, which “prevents the anxiety of wondering whether the agent understood your request correctly.” A good agent will break the task into substeps:

Step 1: search the codebase for relevant sections; Step 2: make changes in files A, B, C; Step 3: run tests; Step 4: commit changes.

This planning stage is key to effective autonomy: it’s the AI’s way of reasoning about how to accomplish your goal before diving in.

The agent launches a dedicated development environment for the task. Jules, for instance, “clones your codebase into a secure Google Cloud VM” and works asynchronously there. OpenAI’s Codex similarly runs each task in its own cloud sandbox, preloaded with your repository. Tools like Cursor’s background agents use a remote Ubuntu-based machine that has internet access to install packages and can be customized via Docker or snapshots. Ensuring the environment has all needed dependencies (like the correct language runtimes and build tools) is both critical and nontrivial. As I noted in a previous analysis, “Figuring out a smooth experience to spin up just the right environment for an agent is key…and the user experience to configure it is as frustrating, if not more, than it can be for CI pipelines.” Nonetheless, agents are tackling this by allowing configuration files to specify setup steps. The goal is to create a dev environment in the cloud that mirrors what a human developer would need to successfully run the project’s code and tests.

Notably, many agents disable internet access to their code after setup, so they can sandbox the run without unauthorized data exfiltration or unrestricted internet calls. Some allow controlled internet use for specific needs: for example, OpenAI recently enabled optional internet access for Codex tasks like fetching package updates or documentation.

Execute
Next comes the main show: the agent starts writing and modifying code according to the plan. Armed with a large language model (or a mix of models) fine-tuned for coding, it can read multiple files, generate new code, and even create new files if needed. This is where the agent essentially acts like a programmer: locating where changes should be made, editing code, and inserting new logic.

One interesting observation from early runs is that agents often use brute-force text search (like the Unix grep command) to find relevant parts of the codebase. For example, an agent might search for a function name or a keyword to figure out where in the repository to make changes. This seems surprisingly simplistic—shouldn’t they use fancy semantic code search or AST-based analysis? Yet, it’s effective and reliable. As Birgitta Böckeler notes, many coding agents default to straightforward full-text search, perhaps finding it the most broadly effective method despite more advanced techniques existing.

As the agent edits code, some systems provide real-time logs or status updates so you can follow along if you want. OpenAI Codex exposes a log of the agent’s “thoughts” and commands (summarized) as it works through a task. Cursor allows you to “view their status and enter the machine the agent is running in” to observe or even intervene midtask. In practice, though, the idea is you don’t need to babysit—you can let the agent run on autopilot.

Verify
A defining capability of these agents is that they don’t stop at writing code—they often compile the code and run tests to verify their changes. For instance, OpenAI’s Codex is designed to iteratively run tests until it receives a passing result. If an agent can run the project’s test suite (or at least a relevant subset of tests), it can catch mistakes and automatically correct them in subsequent iterations. This is huge: it moves the AI from just generating code to also debugging and validating its code.

In theory, an agent with a robust test harness can attempt a fix, see a test fail, adjust the code, and loop until tests pass—without a human in the loop. In practice, environment issues sometimes thwart this. In one case I studied, Codex wasn’t able to run the full test suite due to environment mismatches (certain tools were missing), resulting in a pull request that still had two failing tests. Had the environment been fully aligned, the agent could have fixed those trivial issues before making the PR.

This underscores why environment setup is so important for autonomous agents: if they can run everything a developer would (linters, tests, builds), they can self-correct many errors automatically. Agents like Devin emphasize this loop—Devin “writes code, finds bugs in the code, corrects the code, and runs its own end-to-end tests to verify it works” as a normal part of its operation. In fact, Devin will even spin up a live preview deployment of a frontend app it built so you can manually verify a feature in the browser, which is a clever extension of the verification step.

Report
Once the agent has a candidate solution (all tests have passed, or it deems the code ready), it prepares the results for you. Depending on the platform, this might come as a PR on GitHub, a diff and explanation in chat, or files ready to merge.

At this point, you—the human—do a review. Here we come back to “Trust but verify”: you trust the agent to produce something useful, but you verify the changes through code review and additional testing. Many agent systems explicitly integrate with the PR review process because it’s a familiar workflow for developers. Jules, for example, plugs into your GitHub and will open a branch and PR with its changes. OpenAI’s Codex presents the diff inside ChatGPT for you to approve or ask follow-up questions. If you find issues or have change requests, you can often feed that back to the agent for another iteration.

Some agents handle this via chat (Devin can take feedback from a linked Slack thread: if you point out a problem or ask for tweaks, it will “start working on a reply” to address it). Others might require a new run with an adjusted prompt or use a review comment interface. Impressively, Devin even responded to a GitHub PR comment asking why it made certain changes—it reacted with an “eyes” emoji to signal it saw the comment, then posted a detailed explanation of its reasoning. (The explanation turned out to be not entirely correct in that case, but the fact that it can discuss PRs says something about how interactive these agents can become.)

If all looks good, you merge the agent’s PR or integrate the changes. If not, you might discard it or have the agent try again. One pragmatic question teams face is what to do if an agent’s output is almost good but not quite. Do you spend time fixing up the last 10%–20% of an agent-generated patch, even if it was a low-priority task you offloaded to the AI? This is what I call the “sunk cost” dilemma for AI contributions. Birgitta Böckeler muses that if an agent PR only partly succeeds, teams will have to decide “in which situations would [they] discard the pull request, and in which situations would they invest the time to get it the last 20% there” for a task that originally wasn’t worth much dev time. There’s no one answer—it depends on the context and value of the change—but it’s a new kind of trade-off introduced by autonomous agents.

In summary, background coding agents handle the end-to-end cycle of coding tasks: understand → plan → code → test → deliver. They essentially simulate what a diligent, methodical developer might do when assigned a task, albeit within the current limits of AI (see Figure 10-1).


Figure 10-1. Autonomous AI agent workflow: self-directed agents plan tasks, execute solutions, verify results, and report outcomes with minimal human intervention.
How Do Background Agents Compare to In-IDE AI Assistants?
It’s worth drawing a clear line between the coding AI tools we’ve had for a couple years (GitHub Copilot, ChatGPT coding mode, etc.) and this new generation of autonomous agents. Both are useful, but they play different roles and have different strengths/weaknesses.

The most obvious difference is their level of autonomy. In-IDE assistants like Copilot or VSCode’s AI extensions work synchronously with you—they generate suggestions or answer questions when invoked, and their scope is usually limited to the immediate context (like the file or function you’re editing or a specific prompt you gave). You decide when to accept a suggestion, ask for another, or apply a change.

With background agents, once you hit “go” on a task, the agent will autonomously perform potentially hundreds of actions (file edits, runs, searches) without further confirmation. It’s operating asynchronously. This requires a higher degree of trust (you’re letting it change things on its own) but also frees you from micromanaging. I often describe it as the difference between having an AI pair programmer versus an AI assistant developer on the team. The pair programmer (Copilot) is with you keystroke by keystroke; the assistant dev (Codex/Jules/etc.) works in parallel on another issue.

The copilot style of AI tools means they excel at microtasks—writing a function, completing a line, generating a small snippet, answering a question about how to use an API. They don’t maintain a long narrative or project-wide understanding, beyond what’s in your editor’s open files or a limited window.

Autonomous agents operate at the project level. They load your entire repository (or at least index it) and can make coordinated changes across multiple modules. They keep track of a multistep plan. For example, GitHub Copilot might help you write a unit test if you prompt it, but a background agent could, on its own, decide to add the corresponding implementation in one file, the test in another, and a modified a config in a third—all as part of one unified task. This makes agents far better suited for things like refactoring a cross-cutting concern (logging, error handling), performing upgrades (which often involve many files), or implementing a feature that touches backend and frontend. IDE assistants couldn’t easily handle those because they lack long-term task memory and whole-repo visibility.

Copilot-style assistants are reactive—they respond to your code or queries. They don’t initiate actions. Background agents are proactive in the sense that once activated, they will take initiative to reach the goal. A Jules or Devin agent might decide, “I need to create a new file here” or “Let me run the tests now,” without being explicitly told at each step. They also can notify you of things proactively, like:

I found another place to apply this change, so I’ll include that too.

They behave more like an employee, who might say, “I noticed X while I was in the code, so I fixed that as well.” That said, autonomy also means they might do something you didn’t expect or necessarily want. The supervised nature of this style of tool means it will only do exactly what you accept (except maybe for subtle missuggestions you didn’t notice). So with great power (proactivity) comes the need for greater oversight.

A major difference is that background agents can execute code and commands, whereas traditional IDE assistants usually cannot (unless you count things like ChatGPT’s Code Interpreter mode, but that’s more for data analysis, not integrated with your project’s build).

Agents will run your test suite, start your dev server, compile the app, maybe even deploy it. They operate in a sandbox, but it’s effectively like having an automated developer who can use the terminal. This is a game changer—it closes the loop of verify/fix. An IDE helper might generate code that looks plausible, but if it didn’t actually run it, there could be runtime issues or failing tests.

With an agent that runs the code, you have a higher chance the output is actually functional. It also offloads the debugging step; if something fails, the agent can try to fix it immediately. The flip side is this requires the agent’s environment to be correct (as discussed earlier), and it opens the door to potential side effects. Imagine an agent running a database migration or modifying data—usually they’re in sandbox mode, so this doesn’t affect production, but be careful.

GitHub Copilot and tools like it live in the editor, which is great for in-the-flow coding. Agents often integrate with project management and DevOps tools, too. For example, you might create a GitHub issue and have an agent pick it up and generate a PR, or trigger an agent run from a CI pipeline for certain tasks (like autofixing lint errors on PRs). In fact, CodeGen advertises its agents’ ability to attach to issue trackers so that when an issue moves to “In Progress,” the AI agent works on it. This kind of integration is beyond what IDE tools do. It hints that AI agents could become part of the CI/CD loop—for instance, automatically attempting to fix build failures or automatically creating follow-up PRs for minor issues. That’s a different mode of collaboration: not just helping a dev write code but acting as a bot user in the team’s toolchain.

Using copilot-type assistants often still feels like programming, just faster—you type, they suggest, you accept, you test. Using a background agent feels more like delegation followed by review. The human effort shifts from writing code to writing a good task description and then reviewing the code produced. I call this “generator versus reviewer asymmetry”—generating a solution (or code) from scratch is hard, but reviewing and refining it is easier. Async agents capitalize on this: they handle the bulk generation, leaving you with the (typically faster) job of vetting and tweaking. This can be a productivity boon, but it also means as an engineer you need to sharpen your code review and verification skills.

Code review has always been important, but now it’s not just for other human colleagues’ code—it’s for AI-generated code as well, which might have different patterns of mistakes. My mantra is that you should treat agent-produced code as if it were written by a slightly overeager junior developer: assume good intentions and decent competence, but verify everything and don’t hesitate to request changes or reject if it’s not up to standards.

In practice, I find that I use copilot-style tools and background agents together. For instance, I might use Copilot or Cursor’s inline suggestions while I’m actively coding a complex piece of logic, because I want tight control over that logic. Meanwhile, I might delegate a peripheral but time-consuming task (like updating all our API client libraries for new endpoints) to a background agent to handle in parallel. They fill different niches. One doesn’t necessarily replace the other. In fact, I foresee IDEs offering a unified experience: a palette of options from “Complete this line” to “Generate a function” to “Hey, AI, please implement this entire ticket for me.” You’d choose the tool depending on the scope.

Combining Multiple AI Models to Maximize Strengths
So far, I’ve often referred to “the AI” as if it’s one monolithic assistant. In reality, there are many AI models, each with different strengths. Some are great at natural language understanding, others excel at generating code, and some might be specialized in certain domains (like a math problem solver or a UI generator). An advanced practitioner of vibe coding can orchestrate multiple AIs together, using each for what it’s best at. This is like having a team of specialists rather than a single generalist.

Consider a future workflow where you have:

A CodeGen AI highly trained on programming that can produce code and fix code efficiently

A TestGen AI, specialized in generating test cases and finding edge cases

A Doc AI that writes clear documentation and explanations

A Design AI that’s skilled at generating UI layouts or graphics

An Optimization AI focused on performance tuning and perhaps even aware of low-level details

You can pipe your task through several of these AIs. For example, you ask CodeGen AI to write an implementation. Immediately, you feed that output to TestGen AI to generate tests for it (or to critique it). Then feed both code and tests to Doc AI to produce documentation or a usage guide. If the code involves user interface, maybe Design AI is used earlier to propose the layout structure that CodeGen AI then implements. By chaining them, you leverage each model’s domain expertise. This is analogous to a software pipeline or assembly line, but instead of different human roles, it’s different AI roles.

Even among similar models, combining them can improve reliability. If you have two code-generation models from different providers or of different architectures, you can have them both attempt the solution and then compare or test both outputs. If one model’s output passes all tests and the other doesn’t, you pick the passing one. If both pass but have different approaches, you might manually choose the more readable one. If one fails, you can even show the failing one the successful code as a hint to learn from. This kind of AI cross-talk can reduce errors since it’s less likely that two different models will make the exact same mistake. It’s like getting a second opinion. You can already find research and tools that use one AI to check another’s reasoning⁠—for instance, one generates an answer and another judges it.

Differentiate Models by Task Type
Use the right tool for the job. Large language models (LLMs) are good generalists, but sometimes smaller, specialized models or tools do better. For example, for arithmetic or certain algorithms, a deterministic tool (or an AI that’s more constrained) might be better. Some advanced dev setups use symbolic solvers or older rule-based AI for specific subtasks and LLMs for others. As an advanced vibe coder, you might maintain a toolbox: when you need regex, you call a regex-specific generator; when you need a commit message, maybe a model fine-tuned for summarization is used. The beauty is these can be integrated via simple scripts or prompt wrappers. For instance, you could have a local script like ai_regex_generator that internally prompts an AI but with some pre- and postprocessing to ensure the output is a valid regex, and maybe tests it on provided examples.

Use an Orchestration System
If you find yourself frequently combining models, you might use or build an orchestration system, an emerging category of frameworks often referred to as AI orchestration or agents. These systems allow you to define a flow; for example:

Step 1: Use Model A to interpret user request.

Step 2: If request is about data analysis, use Model B to generate SQL; if about text, use Model C…

Step 3: Feed the result to Model D to explain it.

This is more relevant if you’re building an app or service powered by multiple AI steps. But even in personal dev, you can script a multistep approach. For example, one custom CLI tool, ai_dev_assist, takes a prompt and behind the scenes uses an AI to classify the prompt into categories like code, design, test, and optimize. Based on the category, it forwards the prompt to the appropriate specialist AI. When it receives the result, it can optionally pipe the result into another AI for review or improvement.

This kind of meta-AI coordinating other AIs sounds complex, but an advanced user can set it up with current technology. It will likely get easier as we begin to see dedicated support in IDEs or cloud platforms.

Human-AI Hybrid Teams
While on the subject of multiple intelligences, let’s not forget human collaborators. An advanced vibe coder also knows when to involve fellow human developers in the loop. For example, you might use AI to generate two or three different design prototypes for a feature, then bring those to your team’s UX designer for feedback. Which one aligns with our brand? Which feels intuitive? If an AI writes a complex piece of code, you might do a code review session with a colleague focusing on that piece, acknowledging that “an AI helped write this, so I want another pair of human eyes on it too.” In a sense, the “multiple model” approach can include humans as just highly advanced models—each entity (human or AI) has unique strengths. The future of development might often be human + AI pair programming or even team programming where some “team members” are AI.

Imagine building a small web application through vibe coding. Your workflow might look like this:

You use a UI Layout AI to generate the HTML/CSS for your page given a description (specialized in frontend).

You use a Content AI to generate some placeholder text or images needed (like marketing text, maybe using a model geared for copywriting).

You then use your main Code AI to generate the interactive functionality in JavaScript, feeding it the HTML so it knows which element IDs to hook into.

You then ask a Testing AI to generate Selenium or Playwright tests for the interface interactions.

Finally, you use a Security AI to scan the code for common vulnerabilities. This could be a model or simply a static-analysis tool augmented with AI.

This multimodel approach covers frontend, backend (if there is one), content, testing, and security in one integrated process. Each AI handled its portion and you, as the orchestrator, ensured they all align.

While today you might have to manually copy outputs from one tool to another or use some glue scripts, tomorrow’s IDEs might let you configure this pipeline so it feels seamless. The key takeaway is: don’t rely on just one AI model if you have access to several. Use the best one for each job and make them work together. It leads to better outcomes and also reduces single-point failure—if one model isn’t good at something, another might cover that weakness.

Combining AI models is an advanced move, but it’s a logical extension of specialization, a principle well known in software engineering (think microservices, each service doing one thing well). Here, each AI service does one thing well. As a vibe coder, your role expands to AI conductor, not just AI prompter. It requires a bit more setup and thought, but the payoff is a symphony of AI collaborators each contributing to a high-quality end product.

Now that you know how they work, let’s meet some of the leading examples and see how they stack up.

Major Players in Autonomous Coding Agents
As I write this in 2025, the autonomous coding agent landscape has rapidly evolved over the past year, with distinct approaches emerging across different platforms. These tools represent a shift from passively completing code to acting as active development partners that can execute complex tasks independently.

Cloud-based command-line agents: OpenAI Codex
OpenAI’s Codex exemplifies the cloud-based agent approach, operating through ChatGPT’s interface or an open source CLI. It spins up isolated sandboxes to execute coding tasks in parallel, handling everything from React upgrades to unit test creation. What distinguishes Codex is its reinforcement-learning training on real coding tasks, enabling it to follow best practices, like running tests iteratively until they pass. While results can vary between runs, Codex typically converges on working solutions for well-bounded tasks. Its strength lies in actual code execution within CI-like environments, representing the first wave of agents that truly “pair” with development pipelines.

Workflow-integrated agents: Google Jules
Google Jules takes a different approach by deeply integrating with GitHub workflows. Running on Google Cloud VMs with full repository clones, Jules emphasizes visible, structured planning—presenting its reasoning and allowing plan modifications before execution. This “plan, then execute” philosophy, combined with real-time feedback capabilities, positions Jules as a supervised assistant rather than a black-box automation. Its GitHub-native design means it operates directly where teams work, creating branches and PRs without context switching. The agent even experiments with novel features like audio changelogs, pointing toward more accessible code review processes.

IDE-integrated agents: Cursor
Cursor’s background agents represent the IDE-centric approach, launched directly from the editor but executing on remote machines. This hybrid model lets developers orchestrate multiple AI workers from their command center while maintaining local control. Cursor provisions Ubuntu instances with customizable environments (via environment.json or Dockerfiles), giving agents full internet access and package installation capabilities. The key innovation is seamless IDE integration: developers can monitor agent progress, intervene when needed, and immediately access changes locally when complete. This approach blurs the line between local AI assistance and cloud execution power.

Team-integrated agents: Devin
Devin positions itself as an “AI teammate” rather than just a tool, integrating with Slack, GitHub, and issue trackers like Jira. Built by Cognition Labs, it uses custom AI models tuned for long-term reasoning and multistep execution. Devin excels at parallel execution of small maintenance tasks like bugfixes, test additions, and linter cleanups that often get deprioritized. Its collaborative design includes status updates, clarification requests, and even automatic preview deployments. While it handles straightforward tasks well, complex issues can still require significant human intervention, highlighting the current boundaries of autonomous coding.

The field is expanding rapidly, with both established players and startups racing to define the category. Microsoft has hinted at “Copilot++,” moving beyond inline suggestions to agent capabilities. Enterprises are being courted by startups like CodeGen (which uses Anthropic’s Claude) promising “SWEs that never sleep.” Meanwhile, open source projects and academic research continue pushing boundaries, exploring how to make code generation more reliable and contextual.

This proliferation suggests that we’re witnessing the birth of a new development paradigm where individual developers orchestrate multiple AI agents, each specialized for different aspects of the software lifecycle. The key differentiators emerging are:

Execution environment (local versus cloud)

Integration depth (IDE versus workflow tools)

Autonomy level (supervised versus independent)

Target use cases (maintenance versus feature development)

Challenges and Limitations
While autonomous coding agents inherit the foundational challenges of AI-assisted development, as discussed throughout this book—particularly the 70% problem, explored in Chapter 3—their autonomous nature introduces distinct complications that warrant separate examination:

The compounding effect of sequential decisions
Unlike interactive AI assistance where humans intervene at each step, autonomous agents make chains of decisions that can compound errors in unique ways. When an agent misinterprets the initial requirements, it doesn’t just generate one flawed function: it builds an entire implementation architecture on that misunderstanding. Each subsequent decision reinforces the original error, creating what I call “coherent incorrectness”: code that’s internally consistent but fundamentally misaligned with actual needs.

This sequential decision making particularly challenges agents that tackle multifile changes. An agent implementing a new feature might correctly modify the backend API but then propagate incorrect assumptions through the frontend, database schema, and test suites. By the time you review the complete pull request, untangling these interconnected mistakes tends to require more effort than the interactive, incremental corrections that are possible with traditional AI assistance.

Environmental brittleness at scale
While Chapter 8 discusses general environment configuration challenges, autonomous agents face unique complications from their sandbox execution model. Each agent run requires spinning up an isolated environment that precisely mirrors your development setup—a challenge that scales poorly. When you’re running multiple agents concurrently, even slight variations in the environment can lead to dramatically different outcomes.

Consider a scenario where five agents work on different features simultaneously. Agent A might have a slightly older Node version in its container, Agent B might lack a specific system library, and Agent C might have different time zone settings. These variations, invisible during execution, surface as subtle bugs that only appear when you begin integrating their work. This “environmental drift” between agent sandboxes represents a new class of integration challenge that is absent from single-developer workflows.

The async coordination paradox
Autonomous agents promise parallel development, but this introduces coordination challenges that are quite distinct from human team dynamics. When multiple agents modify overlapping code sections, they lack the implicit communication channels humans use—there’s no quick Slack message asking, “Are you touching the auth module?” or informal awareness of what colleagues are working on.

This creates what I term the async coordination paradox: the more agents you run in parallel to increase productivity, the more complex integrating them becomes. Unlike human developers, who naturally coordinate through standups and informal communication, agents operate in isolation. You might discover that Agent A has refactored a utility function, while Agent B was busy adding new calls to the old version, creating conflicts that wouldn’t occur if agents had human developers’ natural awareness of each other’s work.

The review bottleneck—amplified
While code review remains essential for all AI-generated code (as discussed in previous chapters), autonomous agents amplify this challenge through sheer volume and timing. Unlike interactive AI assistance, where code arrives incrementally as you work, agent-generated PRs appear as complete implementations—often as multiple PRs arriving simultaneously after overnight runs.

This creates a kind of cognitive overload that’s distinct from the kind you get when reviewing human PRs. With human contributions, you can often rely on commit messages and PR descriptions to reflect a coder’s actual thought processes. Agent PRs, however, require you to reverse-engineer the agent’s “reasoning” from the code itself. When five agents each deliver PRs of 500 lines or more on Monday morning, the review burden shifts from being a collaborative quality check to something more like an archaeological expedition.

Delegating to agents requires trust
Perhaps most significantly, autonomous agents challenge our trust models in ways interactive AI tools don’t. When you delegate a task to an agent and walk away, you’re making an implicit bet about acceptable risk. This differs fundamentally from supervised AI assistance, where you maintain moment-by-moment control.

Consider agentic technologies’ security implications. Autonomous agents with repository write access and execution capabilities present unique attack surfaces. A compromised or misdirected agent doesn’t just suggest bad code—it actively commits it and potentially even deploys it. Our sandboxing and access controls for agents must be correspondingly more sophisticated than for suggestion-based tools (covered in Chapter 8).

Emerging organizational challenges
As teams scale up their agent usage, new organizational patterns are emerging that don’t exist with traditional AI assistance. Who “owns” agent-generated code when the requesting developer is out sick? How do you track agent resource usage across teams? What happens when an agent’s monthlong refactoring project conflicts with urgent feature development?

These aren’t technical limitations but organizational challenges, and they’re unique to autonomous systems. They require new roles (agent coordinators?), new processes (agent impact assessments?), and new tools (agent fleet management?) that extend beyond the individual developer considerations this book has addressed in earlier chapters.

The autonomous nature of these agents—their ability to work independently, make sequential decisions, and operate at scale—transforms them from productivity tools into something approaching team members. This shift demands not just the technical practices discussed throughout this book but entirely new frameworks for coordination, trust, and integration that we’re only beginning to understand.

Best Practices for Using AI Coding Agents Effectively
While many general AI development practices apply to autonomous coding agents, certain aspects of agent-based development require specific consideration. Based on collective experience with tools like Codex, Jules, Devin, and Cursor’s background agents, these practices address the unique challenges of delegating entire development tasks to AI systems operating independently.

Strategically Select the Tasks Autonomous Agents Are Going to Implement
The fundamental difference between AI assistants and autonomous agents lies in their scope and independence. Agents excel at well-defined, encapsulated tasks with clear success criteria—particularly those involving parallel execution of many small tasks. Ideal agent assignments include comprehensive test coverage improvements, systematic dependency updates, bulk refactoring operations, and standardized feature implementations across multiple components.

Consider the difference between asking an AI assistant to help write a single test versus tasking an agent to achieve 80% test coverage across an entire module. The agent can methodically work through each untested function, generate appropriate test cases, run them to verify correctness, and iterate until the coverage target is met. This type of systematic, measurable work is the sweet spot for autonomous agents.

Conversely, tasks that require making significant architectural decisions, interpreting complex stakeholder requirements, or designing novel algorithms remain better suited to human-led development with AI assistance. The key lies in recognizing which aspects of a larger task can be effectively delegated to agents and which require human judgment and creativity.

Leverage Agent-Specific Planning and Oversight Features
Modern autonomous agents  distinguish themselves through sophisticated planning and execution transparency features that demand active engagement. When Jules presents its execution plan before beginning work or when Cursor displays real-time logs of agent activity, these represent critical intervention points that are unique to agent-based development.

The planning phase serves as your primary quality gate. Review proposed plans not just for correctness but for efficiency and alignment with your codebase conventions. If Jules plans to update a Next.js application but omits critical webpack configuration changes, catching this during planning prevents extensive rework later on. This proactive review differs fundamentally from reactive code review and represents a new skill in the developer toolkit.

Runtime monitoring provides another layer of agent-specific oversight. While you need not watch every operation, periodic checks can prevent agents from pursuing inefficient solutions or making unnecessarily broad changes. Cursor’s ability to “enter” the agent’s environment midtask exemplifies how modern tools support intervention without completely abandoning the autonomous workflow. To maximize efficiency, you’ll need to learn when to intervene and when to let the agent self-correct.

Manage Concurrent Agent Operations
Unlike traditional development, where a single developer works on one task at a time, agents enable true parallel development. This capability requires new coordination strategies. When running multiple agents simultaneously—perhaps one updating dependencies while another adds logging infrastructure—you must consider the potential conflicts and dependencies between their work.

Establish clear boundaries for each agent’s scope to minimize merge conflicts. Assign agents to different modules or layers of the application when possible. Consider the order of integration: an agent that is adding new features might need to wait for another agent’s infrastructure improvements to complete. This orchestration resembles managing a distributed team more than it does traditional solo development.

Evolve Your Team Practices to Integrate Agents
The introduction of autonomous agents fundamentally alters team dynamics and review processes. Unlike reviewing a colleague’s carefully crafted PR, agent-generated PRs may contain technically correct but stylistically inconsistent code. Teams must develop new review practices that account for this difference.

Consider establishing agent-specific review checklists that emphasize not just correctness but also alignment with team conventions and architectural patterns. Document common quirks you spot as you work with the agent: perhaps your chosen agent consistently uses certain antipatterns or misses specific optimization opportunities. This institutional knowledge helps reviewers quickly identify and address recurring issues.

Build Feedback Loops with Autonomous Systems
Perhaps most importantly, autonomous agents enable a new form of iterative development in which the feedback loop extends beyond mere code review. When an agent’s pull request needs refinement, you can often send it back and ask for another iteration with specific guidance. This differs from traditional development, where sending work back to a human colleague carries social and time costs.

Work to develop prompting patterns that work well with your chosen agents. When you find successful prompt formulations that consistently yield high-quality results, document them. Create templates for common task types that include all necessary context and constraints. This is a kind of prompt engineering specifically for agents that considers their planning, execution, and revision cycles, and it represents a distinct skill from general AI interaction.

The goal remains unchanged: delivering high-quality software efficiently. Autonomous agents simply provide a new tool for achieving this goal, one you should integrate into your existing practices thoughtfully rather than replacing established methods wholesale. By understanding these agents and leveraging their unique capabilities while maintaining rigorous quality standards, teams can realize significant productivity gains without sacrificing code quality or architectural integrity.

Summary and Next Steps
To wrap up, I’ll echo a sentiment from Chapter 4: AI won’t replace developers, but developers who can use AI effectively may well replace those who can’t. The advent of autonomous coding agents is a leap in that direction—those who learn to harness these “headless colleagues” will be able to do more in less time. It’s an exciting time to be a software engineer, as long as we adapt and continue to hold our work to high standards. The tools may be changing, but the goals remain: build reliable, efficient, and innovative software. With AI agents at our side (or in the background), we have new ways to reach those goals—and perhaps get a good night’s sleep while the bots burn the midnight oil.

Next, the final chapter of this book takes a broader look at the future of AI in coding, including the future of agentic AI.


Chapter 11. Beyond Code Generation: The Future of AI-Augmented Development
Vibe coding may have started with AI generating code from our prompts, but its implications reach far beyond just writing code. As AI technologies advance, they are poised to transform every aspect of the software development lifecycle. In this chapter, I take a speculative yet informed look at how AI’s role in software might expand in the future. I will explore AI-driven testing, debugging, and maintenance; how AI could influence software design and user experience personalization; the evolution of project management with AI assistance; and even the future of programming languages themselves. The aim is to imagine a future where AI isn’t just a code generator but a holistic participant in software engineering—all while grounding the discussion in fundamental principles, so it remains relevant even as specific technologies come and go.

AI in Testing, Debugging, and Maintenance
Imagine a future development environment where as soon as you write a function (whether by hand or via vibe coding), an AI tool immediately writes a suite of unit tests for it, finds potential bugs, and maybe even fixes them—all in a matter of seconds. This scenario is quickly becoming plausible. Let’s break down AI’s potential (and already emerging) contributions in quality assurance and maintenance.

Automated Test Generation
Writing thorough tests is time-consuming and often neglected due to deadlines. As you saw in Chapter 7, current AI assistants can alleviate this by generating tests automatically. For example, given a piece of code, an AI can suggest a set of unit tests covering typical cases, edge cases, and error conditions.

In the future, this could go further: the AI could examine your entire codebase, identify functions or modules with insufficient test coverage, and generate additional tests. It might even simulate inputs that a human tester wouldn’t think of (like fuzz testing), potentially catching corner-case bugs. The benefit is a more robust codebase with minimal manual test writing.

The caveat is that tests are only as good as the AI’s understanding of the specification. Thus, a human should review AI-generated tests to ensure they align with the intended behavior of the software. For instance, the AI might assert a certain output that is technically what the code does, but perhaps the requirement was different—which, as long as a human is in the loop, can actually help to catch a misunderstanding in either the code or the test.

Intelligent Debugging
Debugging often involves tedious searching through logs or stepping through code to locate the source of an error. Chapter 5 showed you how AI can act like a smart debugging companion. Some current AI tools can take an error message and problematic code as input and return an explanation and a code change to fix it.

For a glimpse of how AI-assisted debugging workflows may evolve, consider an AI system that monitors your program’s execution, and when a crash or exception happens, it analyzes the stack trace and variable states to pinpoint the likely cause. Instead of just giving you an error message, it might say:

The application crashed because userProfiles was null when calling getEmail(). This suggests a missing null-check when loading user profiles.

Further, the AI could suggest a fix:

A possible solution is to initialize userProfiles if it’s null or add a condition before calling getEmail(). Would you like me to apply this fix?

Future debugging AIs could integrate directly with runtime environments, catching issues in real time. They might even predict issues before they happen by analyzing code paths:

This function might throw a DivisionByZero exception if called with y = 0; consider handling that case.

This is similar to static analysis but powered by the AI’s learned knowledge of countless codebases and error patterns, making it potentially more insightful or flexible.

Predictive Maintenance and Refactoring
As requirements change over time, code becomes outdated or suboptimal. Maintenance involves activities like refactoring (improving code structure without changing behavior), updating dependencies, and optimizing performance. AI can assist in each of these areas:

Refactoring
A future AI could identify code smells (like duplicate code or long functions) and automatically refactor them. For instance, it might detect that you have similar chunks of code in three places and recommend abstracting them into a single helper function. Or it could transform a deeply nested set of loops into a more readable form. Since the AI has seen many examples of “good” code, it can suggest stylistic improvements to keep the codebase clean and maintainable. We might one day have a mode in our editors where the AI continuously refactors code in the background, with the developer reviewing and approving changes.

Updating dependencies
A future AI service might monitor your project’s dependencies (such as libraries and frameworks) and automatically generate pull requests to update them to newer versions, including any code changes needed to accommodate breaking changes. For example, if a new version of a web framework changes the API, the AI could adapt your code to the new API. This would save developers the repetitive work of reading migration guides and fixing version issues.

Performance tuning
Maintenance often includes improving performance as data scales or usage patterns shift. AI can play a role by analyzing performance profiles and pointing out inefficiencies. For example, an AI might notice that a certain database query in your code is taking a long time and suggest adding an index or rewriting the query. It might identify that a loop in your code is making redundant calculations and propose a cache. This is akin to having a performance expert always keeping an eye on your application—proactively detecting inefficiencies like redundant loops or suboptimal data structures.

The AI might run in a staging environment, simulate heavy loads, and then feed you a report: “Under high load, Module X becomes a bottleneck due to Y. Consider refactoring using approach Z.” In essence, the AI not only finds issues but also educates the team on better patterns.

AI-Driven Design and User Experience Personalization
Beyond the code and logic, AI is set to influence how we design software and how users experience it.  Good software isn’t just correct under the hood; it’s also intuitive, accessible, and satisfying for users. AI will continue to develop new ways to create better user interfaces and tailor experiences to individual user needs.

Generative Design Tools
Today’s AI design tools use techniques from generative adversarial networks or transformers to produce UI mockups from descriptions. A product manager can describe a feature in natural language:

We need a mobile sign-up screen with a welcome message, a playful illustration, and a form for name, email, and password. It should have our brand colors and a friendly look.

An AI design assistant can take this description and generate several candidate UI designs in seconds, complete with layout, placeholder text, and even styled components. The designer or developer then picks the closest one and refines it.

In the future, these tools could be integrated directly into design software or even coding environments so that the line between “designing” and “coding the UI” blurs. The AI might directly output HTML/CSS or Flutter code for the design it generates, making it immediately testable. This would speed up the design iteration cycle tremendously. Instead of sketching by hand or in software, you’d collaborate with an AI that proposes designs based on best practices and vast training data of what users find appealing or usable.

Even in the creative process of design, an AI could be a muse. When a designer is brainstorming a color scheme or an illustration style for an app, an AI tool could generate a mood board of colors or even create custom iconography on the fly. For instance, they might tell the AI:

I need a logo that combines the ideas of code and music.

It would produce a few sample logos mixing symbols of coding (like curly braces) with musical notes. While a professional designer might ultimately handcraft the final asset, the AI’s suggestions can spark ideas and accelerate the exploration phase. In essence, AI can fill the role of a quick prototyper and creative partner that broadens the designer’s palate of options.

It’s important to note that in design and UX, human judgment is paramount. Aesthetic taste, understanding of human emotions, brand identity—these are things an AI can approximate but not inherently possess. Thus, AI in design is a tool to enhance human creativity, not replace it. It can handle the grunt work of producing variants and processing user data, freeing designers to focus on empathy and creative decisions.

For developers, AI-driven design means that the traditional handoff between design and development might become more fluid. Developers could generate UI code with AI in collaboration with designers, or vice versa. It also means frontend developers might spend less time tweaking layouts pixel by pixel and more time ensuring the design aligns with functionality and is implemented accessibly. They might also work on creating the hooks for personalization—writing code that allows the AI to choose between layout A or B based on user data and ensuring both layouts are performant and solid.

In a future of vibe coding, you might “vibe design” as well: just describe the vibe (pun intended) you want for your application’s look and feel, and AI will help materialize it. The result is a holistic AI development process—not just writing backend logic or database queries with AI assistance but crafting the whole product experience in partnership with AI.

AI for UX Research
Another aspect of design is understanding user behavior. AI can analyze usage data from your application (with privacy considerations in mind) to highlight where users struggle. For instance, an AI might detect that many users hover over a certain icon expecting it to be clickable, and it’s not—indicating a UX improvement opportunity. Or it might notice that users from a certain demographic consistently drop off at a particular step of a workflow, suggesting that step might not be intuitive for them.

In the future, AI could even simulate user interactions (using models of user behavior) to predict UX issues before real users encounter them. This “virtual UX testing” could catch things like overly complex navigation or unclear labels during development, when they’re easier to fix.

Personalized User Experiences
Personalization has been a buzzword for a while—in the sense of providing different content to different users based on preferences or history. AI can take personalization to the next level by fine-tuning software behavior and interfaces for each user in real time. For example, an app’s AI could learn that a particular user tends to navigate the app via search rather than menus. The AI could then adapt by making the search bar more prominent for that user or even preloading search results it expects the user might want given the context (like a human assistant anticipating their boss’s needs).

Another scenario would improve accessibility: if the AI detects a user is using screen-reader technology (and thus is perhaps visually impaired), it could automatically switch the application to a high-contrast, larger-font mode with optimized screen-reader labels, even beyond what the static accessibility settings might do. Essentially, software can become adaptive.

Imagine an ecommerce site that rearranges its layout on the fly—some users might see a grid of products and others a list with more details, depending on what seems to engage them more. These changes could be subtle and continuous as the AI experiments and learns—somewhat like how A/B testing works but on an individual level and autonomously.

The Evolution of Project Management with AI
Software development isn’t just writing code and making designs; it’s also planning, coordinating, and making decisions—the domain of project management and team leadership. AI’s analytical and predictive capabilities can greatly assist in managing projects, from allocation of tasks to risk management and decision support. Here’s how AI could reshape the way we plan and execute software projects:

Task allocation
Managing a team involves knowing each developer’s strengths, weaknesses, and current workload, then assigning tasks accordingly. An AI project management assistant could analyze various data points—code commit history, areas of expertise (perhaps gleaned from which parts of the codebase a developer has worked on), even personal productivity patterns (some people code more effectively in the morning, others late at night)—and recommend who should tackle a new task.

For example, if a new feature involves database work and the AI knows Alice has done a lot of database-related tasks successfully and isn’t overloaded, it might suggest assigning the task to Alice. Moreover, the AI could predict how long the task might take by comparing it to similar tasks in the past and considering the individual’s velocity. This helps project managers set more realistic timelines and avoid overburdening any single team member. Over time, such an AI could learn to balance the workload like a skilled manager, ensuring that no one is idle and no one is overwhelmed.

Scheduling and sprint planning
AI can assist in breaking down high-level goals into actionable items. You might feed the AI a feature request or a user story, and it could suggest a list of subtasks required to implement it. Essentially, it could produce a draft plan or a work breakdown structure. During sprint planning (in Agile methodologies), the AI could analyze the backlog and, given the team’s past velocity, suggest which set of tasks fits into the next sprint.

It could even highlight dependencies between tasks, ensuring the plan is logically ordered:

Task B should be done after Task A, as it builds on that functionality.

For long-term roadmapping, AI tools might simulate different scenarios:

If we prioritize Feature X now, the model predicts we risk delaying Feature Y by 2 weeks due to overlapping resource needs.

Having these simulations and data-driven insights can help human managers make informed decisions on priorities.

Risk analysis and management
Risk management often involves anticipating what could go wrong—delays, technical hurdles, integration issues—and planning mitigation efforts. AI is well suited for pattern recognition, so it could analyze historical project data (within the company or even industry-wide, if available) to identify risk factors.

For example, the AI might flag things like:

Projects involving a switch in technology stack have a 30% higher chance of running over schedule based on historical data.

We have slipped in integration testing phase in the last three projects; likely a risk for this project as well.

With this, managers can preemptively allocate more time or resources to those phases. Another angle is monitoring current progress: an AI system could watch the rate of task completion, the rate of bug discovery, etc., and send alerts if it senses trouble:

The team is closing tasks at half the expected rate this sprint; possible blockers need attention.

Essentially, AI can be an ever-vigilant project auditor, spotting issues before they escalate.

Decision support
Project management involves many decisions—like whether to cut a feature to meet a deadline or whether to invest in refactoring instead of adding new features. AI can’t make these decisions because they involve business and human factors, but it can provide data to support them. For instance, if debating a refactor, the AI might report:

If we refactor module Z, based on complexity metrics and team input, it could reduce future development time on related features by 20%. It might add a two-week delay now but pay off in six months.

While these numbers would be estimates, having an objective analysis helps stakeholders weigh trade-offs more concretely.

Another example might be deciding whether to adopt a new library or build in-house. The AI could scan documentation, community support, and known issues about that library and summarize its pros and cons, saving the team hours of research.

Natural-language status queries
Stakeholders or managers could one day query an AI in natural language about project status:

How is the payment integration feature going? What are the blockers?

The AI, having parsed ticket updates, commit messages, and test results, might answer:

The payment integration is 70% complete. One blocker is a failing test related to currency conversion, which two developers are currently debugging. If resolved by tomorrow, the feature is on track for completion by Friday.

This kind of accessible status reporting can improve communication, especially in large teams or teams with nontechnical stakeholders. The AI basically becomes an omniscient project assistant that knows the nitty-gritty details and can summarize them as needed.

Emotional and team health insights
This is a bit speculative, but AI could also gauge team morale or stress by analyzing communication patterns (respecting privacy and boundaries, of course). For example, an AI might detect that code review comments are becoming terse or Jira tickets are getting a lot of “reopen” actions—possibly indicating confusion or frustration—and gently alert a project lead to check in on the team’s well-being. In remote or distributed teams, where such signals are harder to read, an AI that monitors the “digital mood” could be valuable. Of course, this would have to be handled delicately and transparently to avoid feeling invasive.

In all these ways, AI acts as a force multiplier for project managers. It handles the heavy analysis and routine suggestions, allowing human managers to focus on what they do best: making judgment calls, motivating the team, and handling the human side of collaboration. Good project management is as much art as science; AI can strengthen the science part (data, predictions, analysis) so that the art (leadership, vision, adaptability) can shine. Developers should welcome these enhancements too: a well-planned, well-monitored project means clearer goals and fewer nasty surprises. It also means less time in status meetings or updating spreadsheets—since the AI takes care of those details—and more time doing creative development work. 

How Autonomous Agents Could Change Software Engineering
We’re still in the early innings of this technology, but it’s moving fast. It’s worth pondering the longer-term implications and how things might evolve in the next few years. Here’s a vision of the future of software engineering in the age of autonomous coding agents, drawing from current trends and some informed speculation:

AI agents will become a standard part of the dev team
Just as using source control or CI/CD is standard today, having AI agents participate in development could become routine. It might be normal that, every morning, you check an “AI assistant board” showing tasks done overnight by agents⁠—the code reviews will be waiting for you when you log in. There’s already a hint of this today, with agents running “while you sleep.” Engineers might commonly delegate a batch of tasks at day’s end for the AI to attempt by next morning. The mindset of what a “developer’s job” is will shift: less about typing out boilerplate or doing rote updates, more about defining problems, integrating solutions, and guiding the AI. You might say, to be a bit fanciful, that developers become more like product managers for AI developers—they specify what needs doing and ensure that it meets requirements.

Multiagent collaboration will be more common
Right now, each agent largely works in isolation on a task you give it. But the future could see scenarios where multiple agents with different specialties collaborate. One agent might be great at frontend tasks and another at backend tasks, and you give them a coordinated task (or maybe they even figure out how to split it). Alternatively, an agent could explore multiple solution paths in parallel—what Kojo calls multibranch exploration.

Imagine you give a complex problem to an AI and it spins up three subtasks with different approaches or architectures, then chooses the best one—or even asks you which direction you prefer. This could dramatically reduce the time it takes to evaluate different implementations (something that currently might take multiple engineers prototyping over days). Of course, orchestrating that is a nontrivial task, but it’s not out of the question as agent frameworks become more advanced.

Intelligent checkpointing will let AIs ask humans for help
Future agents may be smart enough to ask for guidance proactively at decision points. This isn’t just speculation: there’s active research happening on uncertainty estimation and self-reflection for LLMs, with early signs that models can be trained or prompted to recognize when they’re unsure and ask for help rather than forge ahead. For example, an agent might reach a point where two libraries could be used to implement something and instead of guessing, it pauses (much like a junior dev might) and asks you:

I could use Library A or B for this—do you have a preference?

There’s a growing belief in the industry that adding this kind of “intelligent checkpointing” makes agents feel more trustworthy, like collaborators rather than black boxes. It also aligns well with how humans work in teams—knowing when to ask instead of bluffing. While it’s still early, we’re starting to see more models that support this kind of behavior through techniques like tool-use reflection, planning with uncertainty thresholds, and making explicit affordances for user feedback midrun. Intelligent checkpointing requires the agent to know its own uncertainty, which is a challenge, but researchers are working on AI self-awareness, including confidence.

Agent UX will improve
As we offload more work to agents, we’ll need better ways to keep track of what they’re doing. Kojo proposes an agent inbox—a unified view of what tasks agents are working on, what’s been done, and what needs your attention. This could be a dashboard showing all running agent tasks along with their progress (“3/5 steps completed” or “awaiting review”) and results.

Instead of the current mix of logs and PRs, a clear interface to manage agents will emerge. Perhaps IDEs will have an “Agents” sidebar listing active tasks and a feed of updates. You might also get notifications:

Agent X has finished task Y and opened PR #123.

Agent Z needs input to continue.

This infrastructure will be important to scaling up usage without losing overview. After all, no one wants 10 silent bots doing who knows what with no central control.

Agents will integrate with issue trackers and CI systems
I foresee a tighter loop where an issue in your tracker (Jira, GitHub Issues, Linear) can be addressed by an agent from end to end. In fact, CodeGen’s Linear integration already hints at this.

The workflow might be as follows:

A PM files a ticket with specs.

A developer (or tech lead) approves it for AI.

An AI agent picks it up, does it, and attaches the PR to the ticket.

A human reviews and tests the fix and closes the ticket.

This could make the development process more continuous. Similarly, CI systems might invoke agents automatically when certain checks fail. For instance, if a security scan finds vulnerabilities, an agent could attempt to upgrade the vulnerable library or refactor the risky code and then open a PR with the fix. Or if code coverage drops below threshold after a PR, an agent could generate additional tests to raise it. Think of it as automated maintenance.

As a concrete example, Dependabot currently opens PRs to update dependencies. Not only could an AI agent open the PR, but it could also adjust any code that broke due to the update, run tests, and ensure it’s all good—basically a supercharged Dependabot.

Model improvements will narrow the 30% gap
The major AI models themselves (GPT-4, Gemini, Claude, and the like) will continue to improve their code understanding and generation. As they get more capable, that “last 30%” gap might shrink. We might see agents that hardly ever miss an obvious reuse or edge case, because the model has been trained on even more scenarios or has better reasoning.

With better models, agents will make fewer mistakes, require less oversight, and possibly handle more complex tasks. That said, software is inherently complex, so I suspect there will always be some gap for human judgment. Maybe it becomes the last 5%–10% rather than the last 30%.

I also expect models to become more efficient, making it feasible for those worried about data privacy (or cost) to run local or self-hosted agents. Open source coding models might catch up, to the point where you can have an on-prem agent that’s nearly as good for many tasks as the big cloud agents.

Agents and tooling will become more specialized
We might see specialized coding agents for different domains or roles. Imagine a “BugFixer” agent that you point at a failing test or error log and it zeroes in on the bug, a “PerformanceGuru” agent that focuses on profiling and optimizing hot spots, or a specialized agent for writing documentation and code comments from an existing codebase.

By specializing, agents could incorporate more domain-specific knowledge or tools. We might see an agent that integrates with game engines to help with game dev tasks, or one that’s great at data-engineering pipelines. A team of narrow AI specialists could parallel the distribution of expertise within human teams, where some devs are known for frontend work and others for infrastructure. You could have AI teammates like DocsBot, TestBot, RefactorBot, and SecurityBot, each tuned for those purposes. In fact, Cursor already has something called BugBot for automated PR reviews, which is a step in that direction. BugBot doesn’t write code; it comments on PRs with a focus on bug risks, like a static analysis on steroids.

Developers will undergo a cultural and skill shift
If agents handle more routine coding, the skill sets of developers will shift more toward design, architecture, and oversight, as discussed in Chapter 4. Soft skills, like clearly communicating requirements (to humans and AIs alike), become even more important. Code reading and review skills may well become as essential as code writing skills. We might also place more emphasis on testing: since tests are a critical way to verify AI outputs, being good at writing test cases (or guiding AI to write them) remains valuable.

Essentially, the “human 30%” will concentrate on the higher-level critical thinking and quality-control aspects of software development. I suspect we’ll also see changes in how junior developers ramp up. Maybe they’ll start by managing an AI agent on simple tasks before writing a ton of code themselves, which could be both good (they can deliver value quickly) and challenging (they need to learn the fundamentals and not treat the AI as a crutch). It’s an exciting time for those willing to adapt, but it may be uncomfortable for those who prefer the old ways. As I noted in Chapter 4, a big part of “future-proofing” your career in this AI era is embracing these tools and emphasizing your uniquely human strengths.

New roles and processes will emerge
We might see the rise of roles like “AI Wrangler” or “Automation Lead” in engineering teams—people who are particularly skilled at leveraging AI agents, designing workflows around them, and maintaining their configurations. It’s analogous to how “build/release engineers” emerged when build systems became complex, or “DevOps engineers” as infrastructure automation grew. Similarly, audits to check that AI has not introduced any insecure patterns might become standard in code reviews.

There may be more emphasis on testing culture to provide extra confidence: perhaps every agent PR will have to include tests (written by the agent or a human) to be considered for merge. If AI agents are writing a lot of the code, maybe human engineers should write more of the tests (or vice versa) to ensure independent verification.

In essence, the future with background coding agents looks like one where developers orchestrate and verify, while AI agents execute and implement (see Figure 11-1). Software engineering could become more about supervising a fleet of automated coders and less about doing every step manually. This could unlock massive productivity, reduce the boring grunt work, and even allow teams to tackle technical debt and maintenance tasks they never had time for before. (Imagine clearing out all those minor bugs and inconsistencies because now you can just tell an AI to handle them!) It might also lower the barrier to prototyping new ideas: you could have an AI draft a whole prototype app, then just fine-tune it yourself. We may also get to explore more solutions before settling on decisions, since AI can generate alternatives quickly.

However, our industry must integrate these changes carefully. The human element—with its creativity, intuition, and ethical judgment—remains irreplaceable. AI can amplify our abilities, but it can also amplify mistakes if unchecked.

My vision is optimistic: used wisely, autonomous coding agents will make developers more productive and allow us to focus on the truly challenging and interesting parts of building software, ultimately leading us to build better software faster. Achieving that means cultivating good practices and being aware that our role as developers and engineers is evolving.


Figure 11-1. Multiagent AI collaboration architecture: developers orchestrate specialized AI agents for testing, design, coding, and security to collaboratively develop comprehensive software solutions.
The Future of Programming Languages: Natural-Language-Driven Development?
One of the most intriguing questions about the future of vibe coding is how it will shape programming languages. If we can “just tell the AI what we want,” will we even need traditional syntax and languages? Will English (or any human language) become the new programming language? This section explores the possibilities.

We’ve already seen signs of natural language functioning as code in tools where you describe a task in plain language and the AI writes the code. If this trend continues, we might shift more of the programming effort to specifying the intent and requirements rather than the implementation. Future development environments could allow developers (or even nondevelopers) to write something like this:

Every hour, check our database for inactive users, and send an email reminder to any user who hasn’t logged in for 90 days, using template X. If the email bounces, mark the user as ‘invalid email’ in the database.

The AI could take this specification and translate it into the appropriate code (like setting up a cron job or scheduled function, writing the SQL queries or using the ORM, or calling an email API). Essentially, the programmer’s role becomes more about policy and behavior description.

This doesn’t mean programming languages will vanish overnight. Instead, what might happen is a layering: natural language for high-level orchestration and existing programming languages under the hood for fine-grained control.

One reason programming languages exist is because natural language can be ambiguous. If we remove formal languages entirely, we risk miscommunicating with the machine. AI might bridge this gap by disambiguating based on context and by asking clarifying questions, but there’s likely a limit; certain complex algorithms or optimizations might still require very specific instructions that are easier to convey in code than prose. Thus, it’s conceivable that programmers of the future will need to be bilingual in a sense: fluent in human language to talk to the AI, and fluent in the underlying technical concepts to verify and tweak what the AI produces.

We might also see the rise of domain-specific natural languages—constrained forms of English (or other languages) that AIs can reliably understand, tailored to software domains: for example, a “requirements language” for writing use cases that the AI can convert into tests or code.

Even if they don’t go all the way to full natural language, AI’s influence will likely bring programming languages to higher levels of abstraction. In the past, we moved from assembly to high-level languages and from manual memory management to garbage-collected environments, each time raising the level of abstraction. AI could allow us to define abstractions on the fly. Think of this in terms of “programming with intent,” as discussed throughout this book: you specify a goal and the AI figures out how to achieve it, possibly writing lower-level code as needed.

Another possibility is hybrid languages that blend natural language and code. For example, a future language might allow code like this:

// Create a list of customer names from the orders, excluding duplicates

// and sorted alphabetically, then print it.

list<Customer> customers = orders -> map(order -> order.customerName)

                                   -> unique()

                                   -> sort()

                                   -> print();
In this pseudo example, the comment in English is almost executable by the AI. Maybe, in the actual environment, you wouldn’t even need the lambda syntax; the AI could infer from the comment what you intend and fill it out. Or maybe the language would have constructs that look like English but are formal enough for the AI to parse. Think of it as an evolution of comments and pseudocode into actual code, with AI interpreting the intent behind them.

Coding is already becoming more conversational. Instead of writing a complete program, you start a dialogue with the AI:

Prompt: “I need a function to calculate loyalty points for customers based on their purchase history.”

AI: “Sure. Do all products count equally, or do some products give more points? And how should time factor in (e.g., recent purchases versus older)?”

Prompt: “Electronics give 2x points compared to other categories. And purchases in the last month give a 10% bonus.”

AI: “Understood. Here is the function implementing that… [AI presents code]. Shall I also generate tests for different scenarios?”

Prompt: “Yes, and also make sure it handles the case of no purchase history gracefully (should return 0 points).”

In this scenario, programming is a back-and-forth in natural language with the AI, which writes the code and asks clarification questions. This is akin to pair programming, except the pair partner is an AI. We already see early versions with ChatGPT plug-ins and Copilot Chat, etc. In the future, this could become the dominant mode for creating software: a conversation that progressively refines the software, which is much more fluid than writing a strict static text file from scratch.

Even if much of coding becomes natural language–driven, understanding how code and computers work will remain important. Calculators and spreadsheets didn’t eliminate the need for a mathematician or accountant to understand arithmetic. You need to know when the AI’s output is off somehow. If the AI misunderstands an English instruction, a skilled developer might switch to pseudocode or actual code to pin it down. So while the trivia of syntax might become less crucial (no need to remember the exact order of some API’s parameters if the AI can fill that in), algorithmic thinking and debugging will still be vital. The languages might change, but the underlying logic and problem-solving skills persist.

However, the barrier to entry for programming is already lower. Nondevelopers and domain experts can directly create simple applications by conversing with AI through vibe coding. This democratization is exciting: more people can create software solutions without deep programming knowledge. The professional developers will then tackle the harder problems, integrate those citizen-developed scripts safely, or build the platforms that allow such interactions.

Even as AI helps us code in natural language, AIs themselves might evolve new “languages” that are somewhere in between. Perhaps new programming paradigms will emerge that are inherently AI-friendly—meaning they leave space for the AI to fill in blanks, for instance, a language that allows partial programs with placeholders that an AI can resolve (“[Optimize here for speed]”) or with fuzzy logic that the AI can refine into deterministic logic.

In the end, what’s likely is not a complete replacement of programming languages with English but a fusion of the two: more expressive power for developers and a more intuitive way to tell computers what to do. As Andrej Karpathy aptly puts it, “Maybe the future of programming isn’t about writing perfect code anymore. Maybe it’s about perfectly explaining what you want.” The essence of programming—thinking clearly about a problem and specifying a solution—remains. The form of the specification, however, will evolve to be more natural, with AI as the translator that turns our high-level intentions into low-level execution.

This future holds great promise: faster development, more accessibility, and the ability to create increasingly complex systems by focusing on what we want to achieve rather than the nitty-gritty of how to type it out. As always, each leap in abstraction has led to an explosion in creativity (high-level languages enabled software that assembly could never have scaled to). Natural-language-driven development could unleash another wave of innovation, with vibe coders at the forefront, literally talking new worlds into existence through software.

How Vibe Coding Is Reshaping the Industry
Throughout this book, several fundamental principles and ideas have emerged:

Intent over implementation
Vibe coding shifts the focus from writing step-by-step code to expressing the intent or desired outcome and letting AI handle the implementation details. This changes how we approach problems: we think more about what we want to achieve and less about how to type it out. It’s a higher-level way of thinking about software development.

AI as a collaborative partner
Rather than a tool used in isolation, AI in vibe coding is like a pair programmer or an assistant. It’s interactive and iterative. We saw how important it is to guide the AI (through prompt engineering), to review its output, and to combine our strengths with the AI’s. The future isn’t “AI replacing programmers” but programmers working alongside AI for greater productivity.

Ethics and responsibility
We emphasized that with great power (of AI) comes great responsibility. Mitigating bias, ensuring fairness, keeping processes transparent, and maintaining accountability are all critical. The industry is recognizing that relying on AI without guardrails can cause issues, so best practices around testing AI outputs, documenting AI involvement, and addressing legal questions (like IP rights of AI-generated code) are becoming part of standard procedure.

AI goes beyond code generation
AI’s role extends to testing, debugging, design, project management, and more. This holistic integration means the entire software lifecycle is accelerated and enhanced by AI. Tools will increasingly support these phases—some already do, like AI test generation in IDEs or AI-based project scheduling tools.

Skills are evolving, but fundamentals are evergreen
Programmers who embrace the previously mentioned practices will find that their skill set is evolving—shifting to include prompt engineering, AI oversight, data analysis, and high-level design alongside traditional coding and algorithmic skills. The core problem-solving mindset remains crucial, but the day-to-day tasks look different.

Yet certain fundamentals hold: understanding your problem domain deeply, writing clear specifications (prompts are basically specs), maintaining rigorous testing and validation, and focusing on user needs. AI doesn’t change these; if anything, it amplifies their importance because any ambiguity or lack of clarity can be magnified by AI’s ultrafast execution.

This new paradigm of vibe coding is reshaping the industry in practical ways. Teams that adopt AI tools report significant boosts in productivity: developers can complete features in less time, or handle more complex projects with the same resources. It’s also lowering entry barriers: less-experienced developers can achieve more with AI guidance, potentially leveling up faster. On the flip side, it’s pushing experienced devs to expand their horizons and avoid getting complacent with old workflows.

Companies are starting to hire not just for programming knowledge but also for “AI literacy”—the ability to leverage AI tools effectively. Job descriptions might soon include familiarity with AI coding assistants, just like they include familiarity with version control or cloud platforms today. Being a pioneer in vibe coding thus offers a career advantage.

Importantly, vibe coding democratizes programming to an extent. More people—including those who aren’t traditional software engineers—can participate in software creation by describing what they want. This could lead to a flourishing of software tailored to niche needs, created by domain experts with the help of AI (with professional developers focusing on providing guardrails, platforms, and polished core components for them to use).

It’s an inspirational time. We stand on the brink of a transformation that we as developers get to shape. Think back to the early days of computing: those who embraced the personal computer revolution ended up creating the world we have now. Today, AI in programming is a similar inflection point. Embracing it means being part of defining how software is built for decades to come.

Summary and Next Steps
The future of programming is not something that will just happen to us—it’s something we will create. Each of us in the developer community has a role to play in how vibe coding and AI tools are adopted, regulated, and advanced. This is a call to action for you as a reader and practitioner:

Experiment
Don’t wait for all the answers to be given to you. Go out and try vibe coding in different contexts. Use AI to build something quirky and new. Push the boundaries of what these tools can do. Maybe you’ll discover a novel use case or a limitation that no one has documented yet. Each experiment, whether it succeeds or fails, contributes knowledge to the community.

Share your findings
Write about your experiences or at least discuss them with peers. If you find a technique that works brilliantly, publish it. If you encounter a pitfall, warn others. In this rapid evolution, community knowledge sharing is how we all keep up. You could save someone days of debugging by posting that solution you found to an AI quirk, or spark someone’s creativity by sharing a cool AI-assisted project.

Contribute to tools
If you have the inclination, contribute to the development of AI tools themselves. This might mean contributing code to open source AI frameworks or simply giving detailed feedback to tool makers (many of whom are very eager to hear from users about what to improve). By helping shape the tools, you directly influence how the future will look. Many AI coding assistants today have come a long way because developers like you tested beta versions and provided insight.

Advocate for positive change
Within your organizations or communities, advocate for using AI to improve productivity and also for training people to use it properly. Encourage managers to allow time for learning AI tools or to update policies that might forbid them out of misunderstanding. Show how it can be done securely and beneficially. The more success stories emerge of AI augmenting teams positively, the more the industry will lean into it.

Keep a lifelong student mentality
Adopt the mindset that we are all students in this new era. Stay humble and open-minded. The juniors of tomorrow might come in knowing AI tooling natively (like how today’s new grads might have grown up with more exposure to coding than some older folks did). Be ready to learn from anyone, regardless of experience level, because this is new to everyone in some way. If you keep that student mentality, you’ll always find growth and avoid the trap of thinking you’ve figured it all out.

Balance enthusiasm with prudence
Be enthusiastic about what’s possible—your excitement will inspire others. But also be the voice of prudence when needed, ensuring that excitement doesn’t lead to careless use. For example, champion AI-driven development but also push for unit tests and code reviews on AI outputs. This balanced approach will make vibe coding sustainable and respected.

Mentor the next generation
As you gain mastery, help newcomers. Vibe coding lowers the barriers to entry, meaning more beginners might dive into programming. They’ll need guidance to learn solid fundamentals that AI might abstract away. By mentoring them, you ensure that the next generation of developers doesn’t become overly reliant on AI without understanding. You’ll be passing on the torch of good software engineering practices, now enhanced by AI.

The exponential change we’re seeing is a rare opportunity. Think of previous technological leaps, from the Industrial Revolution to the internet boom—those who engaged with them shaped entire industries. We are at such a juncture with AI in software development. It’s not just about keeping your job or making it easier; it’s about having a say in how technology evolves and how it impacts society.

By reading this book, you’ve shown you’re a forward-thinking person. Now, I encourage you to take that forward thinking and put it into action. Every line of code you write with AI, every prompt you engineer, every colleague you teach, every policy you influence—it all contributes to the future of vibe coding.

In closing, remember that at its heart, coding has always been about creation and solving problems. Vibe coding, powered by AI, is an incredibly powerful new medium for creation. Embrace it with optimism and curiosity. Use it to build things that matter. And as you do, keep the human element at the center—our creativity, our judgment, our values.

The future of programming is being written right now, not just in code but in how we choose to integrate these AI partners into our work. It’s an exciting, uncharted path, and each of us gets to be a pioneer. So step forward, experiment boldly, share freely, and lead with the best of human intellect and spirit. By doing so, you won’t just be adapting to the future—you’ll be actively shaping it.

Happy vibe coding, and I’ll see you in the future you help create!