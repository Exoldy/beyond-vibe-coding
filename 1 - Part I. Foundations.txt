Chapter 1. Introduction: What Is Vibe Coding?
AI is reshaping how we build software, introducing new paradigms for coding that range from free-form prompting to structured assistance. Imagine writing software by simply describing what you want it to do—almost like talking to a teammate—while an AI translates those ideas into code. This is the essence of vibe coding, a prompt-first, exploratory approach where you describe what you want in natural language and let a large language model (LLM) fill in the blanks. The term was recently coined by AI pioneer Andrej Karpathy to describe this new way of programming, where developers “fully give in to the vibes” of AI assistance.

In this book, I’ll dive deeper into what vibe coding means for professional developers and how it compares with—and complements—what I call AI-assisted engineering, a more formal augmented coding process. I’ll explore how the developer’s role is evolving in this AI-first era, what tools and workflows can maximize your effectiveness, and how to address the unique challenges of letting an AI loose on your codebase. I’ll also look at where vibe coding shines, where it struggles, and how to balance the speed of AI generation with the wisdom of human oversight. By the end, you should have a clear picture of how to harness “the vibes” in your own coding practice—responsibly and effectively—to become not just a faster coder but a more creative and impactful software product engineer in the age of AI.

In this chapter, we explore how the role of the developer is transforming from writing detailed instructions for machines to collaborating with AI by expressing intent (see Figure 1-1). We’ll see why this “vibe shift” in programming is such a big deal, how it works at a high level, and what opportunities and challenges it brings.


Figure 1-1. A conceptual illustration of programming with intent. The developer provides a high-level specification (the “intent”), and the AI translates it into code. This highlights the shift from writing code line by line to guiding code generation at a high level.
The AI Coding Spectrum: From Vibe Coding to AI-Assisted Engineering
Over the past year, I’ve observed a fascinating split in how developers—especially intermediate and advanced web developers—embrace AI in their workflow. On one end of the spectrum lies vibe coding. On the other end is what I’ll call AI-assisted engineering: a disciplined method of weaving AI into each phase of software development, from design through testing, under clear constraints. Both approaches leverage powerful AI, but their goals, audiences, and expectations differ markedly. Throughout this book, I’ll explore these two extremes and what they mean for modern web development.

The Vibe-Coding Approach: Code by Conversation
In vibe coding, you leverage powerful LLMs as coding partners, letting them handle the heavy lifting of code generation so you can focus on higher-level goals. As one Business Insider summary puts it, vibe coding “means using AI tools...for the heavy lifting in coding to quickly build software.” As NVIDIA’s CEO Jensen Huang says, thanks to AI, “the hottest new programming language” is English, not Java or Python. Instead of manually typing out every function and bug fix, you interact with the AI in natural language—sketching out features, reviewing suggestions, and iterating based on the AI’s output.

This approach represents a dramatic shift from traditional programming to AI-assisted development. Conventional coding demands careful planning, syntax precision, and often painstaking debugging. Vibe coding flips that script: “It’s not really coding—I just see stuff, say stuff, run stuff, and copy-paste stuff, and it mostly works,” Karpathy quipped to Business Insider, highlighting how AI can turn high-level instructions into working code with minimal manual effort.

Developers move from writing detailed instructions for computers to orchestrating outcomes with the help of AI. As an example, Karpathy describes building a web app by continually accepting the AI’s suggestions: “I ‘Accept All’ always, I don’t read the diffs anymore.…When I get error messages, I just copy paste them in.…Sometimes the LLMs can’t fix a bug so I just work around it or ask for random changes until it goes away.” The code “grows” beyond what he’d normally write himself, yet the project comes together quickly through iterative prompting and fixing. Essentially, vibe coding treats coding as an interactive conversation with your AI pair programmer rather than as a solo slog through syntax and stack traces. The goal is speed and exploration—to get a working solution with minimal friction.

Several trends converged to make vibe coding possible. First, modern AI coding assistants (like OpenAI’s Codex, ChatGPT, Anthropic’s Claude, etc.) have become astonishingly good at generating and correcting code. In the same post, Karpathy notes this is “possible because the LLMs…are getting too good”—they have ingested vast swaths of GitHub code and can produce plausible solutions for many tasks.

Second, new developer tools have emerged to integrate these models seamlessly into the coding workflow (more on these tools in a moment). Finally, the developer community’s mindset is evolving to trust AI assistance for bigger and bigger chunks of work. It’s no longer just autocomplete on steroids; it’s handing over whole functions or files to the AI. In practical terms, vibe coding often feels like having an unlimited supply of eager junior developers to implement whatever you ask for—except they work at the speed of cloud computation.

One of the most eye-popping promises of vibe coding is the productivity boost. Early adopters report being able to create software features or prototypes ten to a hundred times faster than before. For instance, Codeium Windsurf engineer John Hoestje muses, “Why be a 10x engineer when you could be a 100x engineer?” This suggests that, with the right AI-powered IDE, extraordinary productivity is within reach. Tools like Windsurf, an AI-enhanced IDE, “can dramatically accelerate development time, allowing you to achieve that 100x productivity.” While 100x might be an extreme scenario, even more conservative studies find huge gains.

Developers can generate boilerplate code in seconds, fix bugs in the blink of an eye, and even have AI write tests or docs, compressing workflows that used to take days into mere hours. No longer limited by typing speed or memory, a single developer armed with AI can often prototype a full stack application in a weekend—something that might have taken a small team weeks to accomplish in the past. It’s not just hype either; as I noted in a January 2025 blog post for Pragmatic Engineer, surveys show that 75% of developers have already integrated some form of AI into their workflows, and many companies report double- or triple-digit percentage improvements in development velocity. In short, AI pair programmers are turning the mythical “10x engineer” into a very real (and reachable) 100x engineer phenomenon.

To understand how revolutionary this is, consider a concrete example. A developer wants to build a simple web app that counts words in a podcast script and estimates reading time. Instead of starting from scratch, they open an AI-powered coding environment and tell the AI their idea. Within minutes, the AI produces a working prototype. The developer then says, “Make the stats counters bright colors and add a PDF export,” and the AI updates the code accordingly. The result is a functional tool, deployed with one click—all achieved in under 10 minutes. This real-world scenario (reported by a creator using Replit’s AI) shows how vibe coding enables extremely rapid, iterative development driven by high-level requests. Similarly, nonengineers are jumping in: the same article describes one laid-off marketer with no coding background who used an AI coding assistant to build 100 simple web tools that collectively reached the top of Product Hunt. When the barrier to creating software drops this low, we’re not just increasing productivity for seasoned developers⁠—we’re fundamentally expanding who can develop software in the first place.

However, vibe coding comes with serious caveats. Because you’re deferring so much to the AI, you might end up with code that “works” in the happy path but hides a minefield of bugs or poor design decisions. Without a solid plan or constraints, an LLM might generate a solution that lacks proper error handling, security checks, or scalability. In fact, AI-generated code can sometimes be built on sand: it appears solid but has hidden issues that only surface under real-world conditions. I’ve seen cases where a developer vibed their way to a complete feature in record time, only to discover later that the code was inefficient and hard to maintain. This kind of “house of cards” code can collapse under pressure.

For example, imagine asking an AI to “whip up a user login system.” The AI might produce a working authentication flow quickly, but perhaps it uses a simplified encryption method or a known vulnerable library. If you deploy that without deeper inspection, you’re taking on faith that everything is sound. Seasoned engineers know that’s risky: code running in production has to be understood and trusted. As one expert put it, “Vibe coding your way to a production codebase is clearly risky. Most of the work we do as software engineers involves evolving existing systems, where the quality and understandability of the underlying code is crucial.” Vibe coding, at its extreme, can bypass those quality gates.

Another challenge is that vibe coding tends to downplay upfront planning. Traditional software engineering values designing for clarity and constraint-thinking through data models, choosing appropriate patterns, and writing out at least a minimal spec. Vibe coding flips this: it starts with no scaffolding, diving straight into implementation via prompts. That can lead to a meandering development process. You might prompt your way into a corner—say the AI chooses a state management approach or library you didn’t intend, and now you have to either steer it back or live with it. Without an initial blueprint, the final architecture might be haphazard. This is fine for a quick proof of concept, but it’s troublesome in a larger codebase where consistency matters.

Vibe coding isn’t inherently “bad.” In fact, its emergence is part of the ongoing democratization of programming. It lowers the barrier to creating software, much like early low-code platforms or scripting languages did. A motivated nonengineer with a clear idea could potentially build a simple app through vibes alone. And for experienced developers, vibe coding can be a powerful brainstorming tool—it’s like pseudo coding but with immediate, runnable results. The key is recognizing its limits. Speed without discipline can lead to brittle software, so vibe coding requires a vigilant human in the loop. I often remind developers (and myself) that “vibe coding is not an excuse for low-quality work.” It should be the start of a solution, not the end.

The AI-Assisted Engineering Approach: Structure with an AI Partner
On the opposite end of our spectrum is AI-assisted engineering—a more structured, methodical way of building software with AI as a copilot at every step. Here, the developer remains very much in the driver’s seat. AI-assisted engineering includes using AI across the traditional software development lifecycle (SDLC), such as AI-powered autocomplete, chat, code migrations, bug detection, test generation, and both granular (function, module, component) and full code generation (see Figure 1-2).


Figure 1-2. The plan-first AI-assisted engineering workflow: developers create specifications, provide targeted prompts to AI systems, review generated code snippets, and integrate approved solutions into their projects.
You begin with a plan (even if it’s lightweight), outlining what you need to build and defining the constraints and acceptance criteria up front. Then you incorporate AI tools in a targeted manner to accelerate or enhance parts of that plan. In contrast to prompt-first vibe coding, we might call this “plan-first” development with AI support. This could be as formal as a mini-product requirements document (a short PRD for a feature) or as simple as a checklist of tasks. The crucial difference is that you ground the work in clear intent and constraints before letting the AI loose.

Consider a React developer tasked with creating a new interactive dashboard component. In an AI-assisted engineering approach, they might begin by writing down the component’s responsibilities and API:

Dashboard component shows a list of analytics cards, supports filtering by date range, and has refresh and export buttons. It should fetch data from our API (with proper error handling), and it must follow our design system for styling.

This outline is essentially a spec. The developer might even sketch a quick data model or identify existing utility functions to reuse. Only then do they bring in the AI: for instance, using an AI-enabled IDE or coding assistant to generate the skeleton of the component based on that description. The AI might provide a starting implementation of the React component with placeholders for data fetching and stubbed event handlers. Because the developer provided clear guidance, the AI’s output is more likely to align with the project’s needs (such as using the right design system classes or calling the correct API endpoints). The code isn’t a surprise; it’s the product of a well-formed request.

AI-assisted engineering doesn’t stop at code generation for a single component. It permeates the entire development lifecycle in a controlled fashion. For routine coding tasks, an AI autocompletion tool like GitHub Copilot can suggest the next few lines as you type, saving keystrokes when you’re implementing known patterns. For example, as you write a unit test, your AI helper might autosuggest assertions based on the function name. Speaking of tests, you might use AI to generate test cases once a feature is in place—feeding the component’s spec or code into a prompt to get suggestions for edge cases you should check. The idea is to augment the engineer’s work, not replace it. You’re still thinking through the logic and verifying correctness; the AI just offloads some of the grunt work.

When it comes to code migration or refactoring, AI can be a godsend. Imagine needing to convert a class-based React component to a modern function component with hooks. Rather than doing it all manually, you could ask an AI assistant to transform the code or at least outline the steps. With a good understanding of the old and new patterns, an LLM can produce a draft of the refactored code, which you then review and polish. This structured use of AI tackles well-defined tasks (like “migrate this code from Redux to React Context API”) one by one rather than handing the AI an open-ended “build whatever” mandate.

Perhaps the most dramatic form of AI-assisted engineering is using AI to generate a full mini-application or feature from a detailed specification. Several tools now allow you to input a description of an app, something akin to a mini-PRD, and get back a working codebase or prototype. For instance, a developer could supply a spec for:

a to-do list app with React frontend and Node.js backend, supporting user authentication and real-time updates

The AI tool would scaffold the project, create the key components, and set up the database schema.

This isn’t magic; it’s an accelerated version of what a diligent engineer might do when starting a new project (setting up directories, choosing libraries, writing boilerplate code). The important thing is that the AI’s creativity is bounded by the constraints given in the spec. The result is a minimum viable product (MVP) that adheres to the requirements you provide. An experienced developer, treating this output correctly, will not assume it’s production-ready on the first generation. Instead, they’ll treat it as a first draft. They’ll run the app, write or regenerate tests to validate each feature, review the code for any inconsistencies or insecure configurations, and refine as needed. In short, they’ll apply all their usual engineering rigor—just accelerated by an AI’s ability to produce bulk code from a blueprint.

The goals of AI-assisted engineering are different from those of vibe coding. The aim here is not just to get working code quickly but to get high-quality code more efficiently. It’s about boosting productivity while preserving (or even improving) the reliability of the outcome. A team practicing AI-assisted engineering might say, “We want to deliver this feature two times faster but with zero compromise on our standards.”

The audience for this approach is typically professional developers and teams who have established processes (code review, testing, deployment pipelines) that they aren’t willing to abandon. These are intermediate to senior engineers who see AI as a powerful new tool in their toolbox, not a replacement for the toolbox. They likely have seen what happens when you cut corners, so they value practices that keep software maintainable. (By way of comparison, the audience for vibe coding includes solo developers hacking together demos, product-minded folks with some coding knowledge, and even relatively new programmers who leverage AI to compensate for gaps in their expertise.)

The expectations in AI-assisted engineering are that humans remain in control of decisions, and the AI provides suggestions or accelerators. Code quality, performance, and security remain paramount, so every AI-generated piece is subject to the same scrutiny as if a junior developer wrote it. Treat the AI as your intern, not your replacement. You might delegate tasks to it, but you must review its work. Just as you’d never deploy code written by a human intern without a code review, you shouldn’t deploy AI-written code without understanding it. This mindset keeps the engineering discipline front and center.

Different Mindsets, Different Expectations
Vibe coding and AI-assisted engineering are two distinct mindsets. Vibe coding is top-down and exploratory: you start with a broad idea and let the implementation emerge through interaction with the AI. It’s a bit like improvisational jazz—minimal structure, lots of room for creative riffs, and you discover the shape of the song as you play. AI-assisted engineering is systematic and iterative: more like classical composition, where you begin with a theme or motif (your requirements) and methodically develop it, perhaps using some improvisation (AI suggestions) within the measures of a written score. Both can produce “music,” but the process and the kind of result will differ.

For an intermediate or advanced web developer, your expectations for each approach are key. If you’re vibe coding, you expect to be surprised. The AI might come up with an approach you wouldn’t have written yourself—maybe it uses a different library or a programming idiom you’re less familiar with. Part of the allure is learning from those surprises or quickly getting past things you find tedious. But you also need to expect hiccups. Vibe-coding enthusiasts should go in with eyes open that they’ll be responsible for that tricky last stretch. The magic is real, but it’s not total.

If you’re practicing AI-assisted engineering, your expectations are more measured and arguably more realistic for long-term projects. You expect the AI to save you time and perhaps inspire a solution or two but not to do your whole job. In fact, a good AI-assisted engineer might use vibe-style prompting in microdoses within a larger framework. For example, while implementing a well-specified module, they might momentarily switch into “vibe mode” to ask, “Hey AI, generate a quick utility function to format these dates,” then immediately switch back to engineer mode to integrate and check that function. The mindset is that AI is a collaborator that works under your guidance. You allocate tasks to it where it excels (like boilerplate, repetitive code, broad-stroke implementations), and you handle the rest yourself (critical logic, integration, final review).

Expectations here include improved productivity, fewer rote mistakes (an AI is less likely to misspell a variable name, for instance), and possibly a broader solution search space (the AI might suggest an algorithm you hadn’t thought of). But you also expect to invest time in validation.  Debugging AI-assisted code is still debugging: you run tests and step through the code in the debugger if needed. The difference is that you might find yourself debugging code the AI wrote for you, which is a new experience that comes with a learning curve. Chapter 5 will discuss this experience in detail.

The two approaches’ goals highlight a fundamental difference between them: vibe coding optimizes for velocity in the short term, whereas AI-assisted engineering optimizes for sustained velocity and reliability. A vibe coder might say, “I need to get this app running by tonight to see if the idea works.” An AI-assisted engineer would say, “I need to build this feature fast, but it should be robust enough to live in our codebase for years.” The former is satisfied if the code basically functions; the latter cares that the code is clean enough for others to build upon.

These differences naturally appeal to different audiences. Less-experienced developers or those outside the engineering discipline might lean toward vibe coding because it lowers the barrier to entry and provides instant gratification. I’ve met product managers and designers dabbling in code via vibe prompts, treating the AI almost like a superpowered Stack Overflow that gives them full solutions. On the flip side, seasoned developers and engineering teams tend to favor AI-assisted engineering. They’ve been burned by fragile code before, so they start from a place of “let’s do this right, even if we use new tools to go faster.” They put in a bit more effort up front (writing that mini-PRD, setting up the project structure) in exchange for long-term payoffs.

Finding Your Place on the Spectrum
It’s tempting to ask: which approach is better? The truth is, vibe coding and AI-assisted engineering aren’t mutually exclusive categories: they represent two ends of a spectrum, and real-world workflows often blend elements of both. A developer might start a project with a burst of vibe coding to scaffold something novel, then switch into engineering mode to firm it up. Or they might generally follow an AI-assisted discipline but occasionally—for a trivial one-off script or a throwaway prototype—say, “You know what, I’ll just vibe code this and see what I get.” The key is understanding the trade-offs and using the right approach for the right context.

Think of vibe coding as a high-speed exploratory vehicle: it can take you off the beaten path quickly, and it’s great for discovery. AI-assisted engineering is more like a reliable train on a track: you have to lay down rails first (plan), but it’s a safer bet and more likely to reach a defined destination without derailing. Intermediate and advanced developers should be capable of driving both vehicles, but they’ll choose based on the task at hand. If the goal is to innovate or ideate rapidly (say, in a hackathon or when validating an idea’s feasibility), vibe coding provides momentum. Just remember to tighten things up if you plan to reuse that code. If the goal is to build a maintainable product feature in a professional setting, leaning toward AI-assisted engineering ensures you don’t end up with a black-box chunk of code in your codebase that nobody truly understands.

One fascinating thing I’ve observed is that as developers gain experience with AI tools, their usage often naturally shifts from the vibe end toward the engineering end. Initially, the novelty of having an AI generate entire blocks of code from a single prompt is alluring—who wouldn’t want to try essentially “talking” an app into existence?

But after the honeymoon, pragmatism kicks in. Developers start to see where the AI shines and where it stumbles. They learn to break problems down and feed them to the AI in pieces rather than asking for the whole solution in one go. In effect, they move from being “prompt artists” to becoming AI “orchestra conductors”—still utilizing the AI’s creative power but guiding it with a skilled hand and following a clear score. In my own practice, I’ve become more deliberate with prompts, often writing small pieces of pseudocode or comments and asking the AI to complete them instead of just asking open-ended questions. This way, I get the benefits of vibe-like fluidity but within a structure I control.

It’s also worth noting that tooling is evolving to support the entire spectrum. On one side, we have chat-based interfaces and natural-language coding environments explicitly designed for vibe coding, where you might not even see the code until you ask for it. On the other, IDEs are adding AI features that seamlessly blend into traditional coding: for example, AI linters that suggest improvements, documentation generators that explain code, and version-control bots that can automatically create a pull request and suggest changes for review. These tools encourage an engineering mindset by fitting into the usual development workflow (edit, review, test, etc.) while still leveraging AI.

The distinction between vibe coding and AI-assisted engineering might even blur over time as best practices emerge. We may find that what today feels like “vibing” will gain more guardrails, and what feels like “structured engineering” will become more fluid. In fact, I’d argue that the ideal future is one where we can move up and down this spectrum effortlessly: exploring creative solutions with AI when we want to but always reining things in with solid engineering practices when it’s time to harden and ship the software.

This spectrum of approaches represents a significant evolution in how we work with AI tools today. Yet even as we refine our techniques for collaborating with AI—whether through rapid vibe coding or structured engineering workflows—a more fundamental transformation is taking shape. The very nature of programming itself is changing. We’re moving away from the traditional paradigm where developers must translate their ideas into explicit instructions and toward a future where we can express our intentions directly and let AI handle the translation into code.

This shift challenges our most basic assumptions about what it means to be a programmer. For generations, our value has been tied to our ability to think like machines—to break down problems into discrete, logical steps that computers can execute. But what happens when machines become capable of understanding what we want, not just what we tell them to do? This is where programming with intent enters the picture, representing not just a new tool or technique but a fundamental reimagining of the developer’s role.

Beyond Lines of Code: Programming with Intent
For decades, programming has meant writing instructions: line after line of code telling the computer how to do something. Each function, loop, and conditional had to be carefully crafted by a human. Programming with intent flips this script. Instead of focusing on the low-level implementation, the developer focuses on the outcome or goal: what you want the program to accomplish. You express that intent in a high-level way (often in natural language), and the AI system figures out the code to fulfill it.

Think of it this way: traditional coding is like giving someone step-by-step directions, while intent-based coding is like telling them your destination and letting them figure out the best route. By focusing on the what instead of the how, developers can work at a higher level of abstraction. This approach isn’t entirely new—tools like visual programming, low-code platforms, and code generators have long promised to raise the abstraction level. But today’s AI advancements are finally making it practical to describe complex behaviors in plain language and get working code in return.

The Rise of the Prompt: From Instructions to Descriptions
At the heart of this shift is the humble prompt. A prompt is the input or question you give to an AI coding system. In essence, it’s a description of what you want the program to do rather than an instruction for how to do it. This can feel very different from writing code. For example, instead of writing a loop to parse a file, you might prompt:

Read this CSV file and extract the email addresses of all users older than 18.

The AI will attempt to generate code that accomplishes that description.

Why is this happening now? The rapid progress of LLMs in understanding and generating text, including programming languages, has been a game changer. These AI models have been trained on vast amounts of code and natural language text. They can interpret a prompt that looks like a description of software behavior and translate it into actual code that implements that behavior. In other words, they’ve learned the patterns of how humans describe tasks and how those tasks translate into code.

This rise of prompt-based development means that, as a developer, you increasingly write descriptions of features and logic in natural language or pseudocode and let the AI handle the heavy lifting of writing syntactically correct code. The prompt becomes your new unit of thought. It’s a concise expression of intent. We’ve gone from telling the computer, “Do X, then Y, then Z” to saying, “I need X, Y, and Z done” and trusting the AI to fill in the blanks.

It’s important to note that writing a good prompt is itself a skill (which we’ll dive into in Chapter 3). A vague prompt can lead to incorrect or inefficient code, just as a vague requirement can confuse a human programmer. The better you can articulate your intent in the prompt, the better the AI’s output will match your needs. This is why many are calling prompt writing the new programming literacy.

How It Works: The Iterative Cycle and AI’s Role in Code Generation
So how does an AI go from your free-form description to actual, functioning code? The magic lies in LLMs’ ability to interpret context and generate text. The large in “large language model” refers to the number of parameters (the internal configuration) it has, often billions or more, which enable it to capture the complexities of natural and programming languages. These models have been trained on public code repositories, forums, documentation, and Q&A sites, learning both the syntax of programming languages and the semantics of how code is used to solve problems. When you interact with an AI coder, you’re tapping into this expansive learned knowledge. Let’s break it down in simple terms:

Understanding the prompt
When you provide a prompt (for example, “Generate a function that checks if a number is prime”), the AI model analyzes the text of that prompt. Modern models from Google, OpenAI, and Anthropic have been trained on countless examples of language and code, so they use statistical patterns to infer what you’re asking. Essentially, the AI tries to predict the most likely completion of the prompt with code that makes sense.

Leveraging context
These AI systems often take into account additional context beyond just the single-line prompt. For instance, if you’re working in an IDE with an AI assistant, the model might also consider the current file content, your coding style, comments, and even related files. All this context helps the AI generate code that fits your project. It’s similar to how a human developer reads surrounding code and documentation to understand what to do next.

Generating code
Once the model has understood (or at least made a best guess about) your intent, it proceeds to generate code.  Under the hood, it does this one token at a time (a token is a piece of a word or code symbol) using probabilities learned during its training. The model doesn’t “think” in the conventional sense; it doesn’t have a compiler or runtime checking the code. It’s simply very good at continuing text in a way that has a high chance of being correct code because it has seen so many examples before. If the prompt and context are clear, the code it produces can be remarkably accurate and even follow best practices it has seen in its training data.

Validating with human oversight
Importantly, the AI doesn’t run off and deploy your application for you. You remain in the loop. You review the generated code, test it, and can accept or modify it. In many cases, the AI might also offer an explanation of the code if asked, helping you understand the result. The AI’s role is like an assistant that drafts the code for you—but you, the developer, are still the decision maker who ensures the code is correct and fits the project’s needs.

What’s truly impressive is that this process happens in seconds or less. The high-level overview is that your description (prompt) goes into a prediction engine (the LLM), which produces likely code as output. While the inner workings of models involve complex math and neural network layers, at the user level, it feels almost like collaborating with an expert who can instantly recall how to implement just about anything.

One of the key things to understand about vibe coding (intent-based programming) is that it’s an iterative, collaborative process between the human and the AI. You don’t just write one perfect prompt and then sit back as the AI writes an entire program flawlessly. In practice, you engage in a back-and-forth, a feedback loop that gradually takes a vague idea to polished code.

Here’s how a typical cycle might look:

Step 1: You describe what you want
This is your initial prompt or request. For example:

Generate a function to calculate monthly loan payments given principal, interest rate, and term.

Step 2: AI provides an initial solution
The AI generates code for that function, complete with parameters and formula for loan payments. It might even include comments explaining the formula.

Step 3: You review and test
You look at the code. Does it make sense? Does it handle edge cases? You run a quick test: what if the interest rate is 0? Does it behave correctly? You notice it might not handle that scenario well.

Step 4: You refine your request or code
If the code isn’t perfect (and often it won’t be on the first try), refine it. Maybe you prompt the AI again (“Modify the function to handle a 0% interest rate gracefully”), or edit the code yourself and tell the AI, “Explain this part,” if something is unclear. This guidance helps correct any misunderstandings.

Step 5: AI refines the solution
The AI takes your feedback or new prompt and adjusts the code. Now the function checks for zero interest and handles it appropriately.

Step 6: Repeat as needed
You continue this loop until satisfied. Perhaps next you ask the AI to also generate unit tests for this function to ensure it works correctly. It does so, and you run them to verify all is well.

This collaboration is much like a pair-programming scenario where one partner is the human and the other is an AI assistant. The human sets the direction and knows the high-level requirements, while the AI offers suggestions, writes boilerplate, and speeds up the tedious parts. Neither is effective alone for complex tasks: the AI relies on the human for direction and validation, and the human offloads some work to the AI to move faster.

Crucially, the iteration isn’t just about fixing errors; it’s also about evolving the solution. You might start with a very rough prompt and then progressively refine your intent as you see what the AI produces.

This encourages a mindset of experimentation. If the first attempt isn’t right, you haven’t wasted much time—just refine the prompt or tweak the code and try again. In traditional coding, writing a module only to throw it away can be frustrating, but with AI-generated code, the cost of a false start is low, encouraging exploration of different approaches.

Productivity, Accessibility, and the Changing Nature of Programming
Why is programming with intent such a big deal? This shift has several profound implications:

Boosting developer productivity
Perhaps the most immediate benefit is speed. Developers can accomplish tasks faster when the AI handles the rote work. Routine code that might take hours to write by hand (like setting up database models, API endpoints, or data cleaning scripts) can often be generated in minutes. Early studies on AI coding assistants back this up: developers using tools like GitHub Copilot have been shown to complete tasks significantly faster (one study found a 55% time reduction on a given task with Copilot assistance). When you multiply these gains across an entire project, it hints at a future where software development cycles shorten dramatically and teams can iterate more quickly.

Keeping developers “in the flow”
Beyond raw speed, there’s a psychological benefit. Writing boilerplate or looking up syntax can break a programmer’s flow and train of thought. With an AI handling many of those interruptions, developers can stay focused on the problem they’re solving. Many users report that with AI help, they feel less frustrated by tedious tasks and can concentrate on the creative and design aspects of coding. In other words, it can make coding more enjoyable by offloading the boring parts, which in turn can improve the quality of the work (a happier coder often produces better code).

Lowering the barrier to entry
Programming has traditionally required learning the exacting grammar of code and the quirks of various libraries and frameworks. With intent-based programming, some of that burden shifts to the AI. A newcomer might not remember the exact syntax to open a file or the parameters of a graphing function, but if they can describe what they want, the AI can fill in those details. This doesn’t mean anyone can code complex systems with zero knowledge (you still need to understand what the program should do), but it does mean that the ramp-up to producing useful results is shorter. It’s conceivable that domain experts (like a biologist or an economist) could write prototypes in their field by describing their needs, even if they’re not professional developers. In this sense, programming becomes more accessible to people who have the ideas and intent but not deep coding skills.

Changing developer roles and skills
As AI takes on more code generation, the role of the human developer evolves. Skills like architectural design, problem decomposition, and validation become even more important. You might find yourself spending more time deciding what to build and reviewing why the code works (or doesn’t) than typing out the syntax. The nature of “knowing how to code” may shift toward “knowing how to get the AI to code.” This could democratize certain aspects of software development while also elevating the level at which professionals operate. We’ll likely see new best practices centered around how to effectively guide AI (a topic I’ll introduce in Chapter 3 and revisit throughout the book).

Productivity versus creativity
Interestingly, as AI handles more routine coding, human developers can focus on higher-level creative tasks like refining the user experience, brainstorming new features, or tackling tricky algorithmic problems that AI might not solve well on its own. In this ideal scenario, the AI increases productivity on the repetitive 80% of coding, freeing your mental energy for the inventive 20%. It’s a shift in how we allocate our effort.

However, it’s not all rainbows and sunshine. This new style of development also raises challenges:

Trust and correctness
Can you trust the code an AI writes? If you don’t see every line, there’s a risk of mistakes going unnoticed. Developers need to thoroughly test and review AI-generated code. The onus is on the human to ensure the output is correct, secure, and efficient. Blindly trusting AI output is risky, as we’ll discuss.

Losing some low-level skills
If you rely on AI for routine coding, will you gradually lose your ability to write that code from scratch or debug issues deep in the weeds? It’s a concern akin to overreliance on calculators weakening arithmetic skills. Developers will need to consciously balance convenience with maintaining a solid understanding of the fundamentals.

Shifting job landscape
As programming with intent becomes widespread, the industry might value different skills. There may be less demand for people who are good at just cranking out boilerplate logic, and more demand for those who can design systems, integrate components, and verify correctness. The nature of software jobs could shift, with AI handling more implementation and humans focusing on design and oversight.

Additionally, one of the most critical factors in “vibe coding” is context window size. Gemini offers the longest context window of all AI models, which can be game changing when working with large projects. Some models now support context windows of over a million tokens, allowing them to maintain awareness of entire applications. Developers can feed entire codebases to an AI for comprehensive understanding.

We’ll delve into these trade-offs more at the end of the chapter. But first, let’s familiarize ourselves with the emerging tools that enable this new way of coding.

A Glimpse of the Tools: The Emerging Ecosystem
Vibe coding may be a philosophy, but it’s enabled by a new generation of AI-powered tools. Experienced developers who want to embrace this workflow will need to get acquainted with some key platforms and models that make AI-assisted coding effective.

This section is a quick tour of the essential tools in the vibe coder’s toolkit. These include Visual Studio Code (VSCode) with its growing ecosystem of AI features and extensions, next-gen AI-integrated IDEs like Cursor and Windsurf, LLMs like Claude (in its various versions), and ChatGPT. This section does not cover background coding agents, but I discuss them in detail in Chapter 10.

As you read this section, don’t worry about memorizing specific tool names or features; the landscape is evolving fast. The goal is to understand the types of solutions available.

VSCode + Copilot: Microsoft’s Integrated AI Development Platform
VSCode has transformed from the world’s most popular code editor into a comprehensive AI-assisted development platform through its deep integration with GitHub Copilot. This evolution represents Microsoft’s vision for keeping AI capabilities within the familiar VSCode environment that millions of developers already use daily.

GitHub Copilot is an AI-powered coding assistant integrated into VSCode. It provides code suggestions, explanations, and automated implementations based on natural language prompts and existing code context. What sets this integration apart is its seamless nature—Copilot isn’t just an add-on but feels like a natural extension of the editor itself.

The core of VSCode’s AI capabilities centers on three main modes of interaction. First, there’s inline code autocompletion, where Copilot provides inline code suggestions as you type, ranging from single-line completions to entire function implementations. As you write code, ghost text appears with suggestions that you can accept with Tab or partially accept word by word.

Second, there’s the chat interface, accessible through a sidebar panel where you can have conversations about your code, ask questions, or request specific implementations. Third, and perhaps most powerful, is the agent mode that uses tool calling to access a growing set of capabilities inside Visual Studio. When given a goal, it selects and executes the right tools step-by-step. This agent mode can analyze your codebase, propose edits across multiple files, run terminal commands, respond to build errors, and self-correct in a loop until the task is completed.

What makes VSCode’s Copilot implementation particularly compelling is its support for the Model Context Protocol (MCP). MCP provides a standardized way for AI models to discover and interact with external tools, applications, and data sources. This means Copilot in VSCode can connect to databases, invoke APIs, access documentation, and integrate with your entire development ecosystem. For instance, with the GitHub MCP server enabled, you can ask Copilot to “create an issue for each bug we discussed,” and it will interact directly with GitHub’s API to create those issues. The extensibility through MCP transforms Copilot from a code generator into a comprehensive development assistant that understands not just your code but your entire workflow.

To leverage VSCode with Copilot effectively in professional development, start by exploring the different interaction modes based on your task complexity. For simple code completions and refactoring, rely on the inline suggestions and the sparkle icon that appears near errors—click it for AI-powered fixes.

For more complex tasks, switch to agent mode by opening the chat panel and selecting “Agent” from the drop-down. Agent mode is optimized for making autonomous edits across multiple files in your project. It is particularly useful for complex tasks that require not only code edits but also the invocation of tools and terminal commands. The combination of VSCode’s familiar interface with Copilot’s evolving AI capabilities offers a compelling option for teams that want enterprise-grade AI assistance without leaving their established development environment.

VSCode + Cline: The Open Source Autonomous Coding Agent
Before exploring purpose-built AI IDEs, it’s worth examining how Cline (formerly Claude Dev) transforms VSCode into a powerful AI-assisted development environment. Cline represents a different philosophy from Microsoft’s Copilot. Rather than being a tightly integrated assistant, it functions as an autonomous coding agent that can take on complex, multistep development tasks from start to finish. This open source extension brings capabilities to VSCode that often exceed those found in proprietary AI editors, all while maintaining the flexibility and extensibility that VSCode users expect.

What distinguishes Cline is its truly agentic approach to software development. When you give Cline a high-level request like “Create a REST API for user management with authentication,” it doesn’t simply generate boilerplate code. Instead, it analyzes your project structure, plans the implementation across multiple files, creates proper folder hierarchies, installs necessary dependencies, and can even run tests to verify the implementation. Throughout this process, Cline maintains transparency by showing you each planned action—file creations, modifications, and terminal commands—and giving you the opportunity to approve or modify each step. This human-in-the-loop design provides the perfect balance between automation and control, allowing developers to leverage AI’s capabilities while maintaining oversight of their codebase.

Cline’s technical capabilities extend far beyond code generation. It can use browser automation to research API documentation, debug complex issues by analyzing error traces across multiple files, and even interact with external services through its MCP support. For debugging, you can paste an error message, and Cline will trace through your codebase to identify the root cause, propose a fix, implement it, and add appropriate error handling to prevent similar issues. Its MCP integration means Cline can connect to your database to understand schemas before generating queries, access your project management tools to align implementations with requirements, or interact with any other MCP-compatible service. This extensibility transforms Cline from a code generator into a comprehensive development partner that understands your entire technical ecosystem.

For teams, Cline offers several compelling advantages. Being open source, teams can inspect its code, contribute improvements, or fork it for custom needs—crucial for organizations with specific security or compliance requirements. It supports multiple AI providers including Anthropic’s Claude, OpenAI’s models, Google’s Gemini, and even local models through Ollama, giving teams flexibility in model selection based on performance, cost, or data residency requirements.

To use Cline effectively, craft detailed prompts that include project context and constraints, leverage its ability to analyze your entire codebase before making changes, and take advantage of its iterative development capabilities. After Cline implements a feature, you can immediately test it and request refinements in the same conversation context. The combination of VSCode’s mature ecosystem with Cline’s autonomous capabilities offers teams a powerful, flexible, and cost-effective path to AI-assisted development without abandoning their existing tools and workflows.

Cursor: The AI-Driven Code Editor
One of the flagship tools of the vibe-coding movement is Cursor, an AI-enhanced IDE that has quickly gained popularity among developers seeking a more fluid coding experience. Cursor is essentially an AI-first code editor (a fork of VSCode, in fact) that builds state-of-the-art code generation and understanding right into your development environment.

Its tagline is “The AI Code Editor,” and it’s designed to let you write and modify code using plain language instructions. For example, you can highlight a function and ask Cursor to “optimize this function” or “add error handling here,” and it will instantly suggest the code changes. Cursor’s AI is project-aware—it indexes your codebase and understands the context of your files, so it can make more relevant suggestions (far beyond a simple autocomplete). Cursor IDE integrates LLM capabilities into its core interface. It’s ChatGPT that knows your codebase.

Under the hood, Cursor leverages advanced language models (often Anthropic’s Claude or OpenAI’s models, depending on your setup) to power its features. It has a chat sidebar where you can have conversations about your code, and even a “Composer” mode for multistep code generation. Andrej Karpathy himself has used Cursor’s Composer with a model called “Sonnet” in his vibe-coding experiments. This setup allowed him to literally talk to the editor (using voice-to-text via “SuperWhisper”) and have code appear, which he would then accept or refine.

Cursor can not only generate code but also edit existing code when instructed. For example, you can ask:

Could you make it easier to switch certificates in the transport listener?

Cursor will understand you’re referring to your code and propose direct edits in the relevant file or read from relevant files, such as a specification markdown file (see Figure 1-3). In the free version, it often provides the diff in the chat for you to approve; in the pro version, it can auto-apply changes to your workspace.


Figure 1-3. Cursor’s interface exemplifies the newer breed of IDEs integrating AI. By indexing your project and iterating on prompts, tools like Cursor enable “leaving your editor running, grabbing coffee, and coming back to fully working features,” delivering exponential productivity gains.
To use Cursor effectively in a professional workflow, you should take advantage of its capabilities systematically. Start by opening a chat in Cursor and describe the feature or fix you want. For instance: add a user login form with email and password, including validation and error messages. Cursor will generate the needed code (creating new files or modifying existing ones) in a draft state. You can review these changes (it shows a diff or preview) and then hit “Apply” to merge them into your codebase. Many developers follow this loop: prompt → review → accept. If the suggestion isn’t perfect, you can refine your prompt (for instance, “Use Tailwind CSS for styling the form”) or just ask Cursor to fix any issues you spot (“Now, handle the case where the email is already registered”). In essence, you converse with your code until it looks good.

Cursor also excels at understanding errors and logs. If you run your code and get a traceback or error message, you can paste it into the Cursor chat, and often the AI will analyze it and suggest a fix. This turns debugging into a cooperative experience: rather than you manually searching Google or Stack Overflow, Cursor’s AI can often pinpoint the problem and even write the patch. That said, it’s wise to verify the fixes, as the AI might not always get it right on the first try.

Another pro tip: use Cursor’s ability to take multiple files into account. You can select a set of files (or let it know about project context in the prompt) so that it considers your whole codebase when generating code. For example: add a new API endpoint in the backend to support the login form, and connect it to the frontend form we just made. Cursor will recall the frontend code it just wrote and help craft the corresponding backend logic. This project-wide context is a game changer compared to earlier coding assistants that only worked file by file.

In summary, Cursor is like having an AI pair programmer inside your IDE, 24/7. It’s intuitive (you chat with it in plain language), and it can update your code directly. The more you practice breaking down tasks and prompting Cursor with clear instructions, the more you’ll find you can accomplish in a short time. It’s particularly great for iterative development: you build a bit, run and see output, then immediately ask Cursor to adjust or extend the code, and repeat.

Windsurf: An AI-Powered IDE with Full Codebase Indexing
Another rising star in the vibe-coding toolbox is Windsurf, an AI-driven development environment that takes code understanding to the next level. Windsurf is built by the team behind Codeium, and it differentiates itself by indexing your entire codebase and using retrieval techniques to feed the relevant pieces to the AI model as you work. In practical terms, this means Windsurf is extremely good at handling large projects where the answer to your question might be spread across many files. Its core uses something called retrieval-augmented generation (RAG), which is a fancy way of saying it looks up the parts of your code that are relevant to your prompt and provides that context to the AI so that its suggestions are consistent with your existing code.

What does this look like for a developer? Let’s say you’re new to a big codebase and need to add a feature. With Windsurf, you can ask in natural language:

Where in the codebase is the user authentication logic handled?

It will search through the index and point you to the right file or even function. Then, you might open a chat (Windsurf calls it the “Cascade” view, triggered by Cmd+L) and say:

Add a phone-based two-factor authentication to the login flow.

Because Windsurf has the context of your auth logic, it can generate changes spanning multiple files (database, API, frontend) to implement this, making informed choices that line up with how your system is structured.

Windsurf’s Write mode can boldly apply changes for you: it will create new files or edit existing ones automatically rather than just suggesting diffs in a sidebar. This can be a huge time-saver: instead of copy-pasting from suggestions, you see your project evolving in place. Windsurf essentially tries to take actions on your behalf when it’s confident, behaving like an autonomous junior dev implementing features across the codebase. (Cursor’s philosophy is a bit more conservative, asking for confirmation, although its Pro version has an “auto-apply” feature too.)

To leverage Windsurf effectively, it helps to understand its strengths:

Codebase Q&A
You can query your codebase in plain English, almost like a custom Stack Overflow for your project. This is great for large legacy projects where finding where something is defined can take hours. Windsurf will answer in seconds by pulling from the indexed code.

Global context suggestions
Because it feeds relevant files into the model, Windsurf can handle tasks like “Refactor the payment module to use the new logging utility we wrote” very well, as it knows about both the payment module and the logging utility.

Modes of operation
Windsurf has multiple modes (Autocomplete, Chat, Command, and Cascade, as mentioned). The Cascade is like a superchat, where it can consider a broader context. The Write mode (within chat) actually executes changes. You, as the engineer, can decide how much autonomy to give it.

For a team, Windsurf can be integrated into daily development much like Cursor. When picking between them, some developers prefer Windsurf for its speed and boldness (noting that it feels faster to generate and apply changes) and for working with very large projects due to its indexing. On the other hand, Cursor’s interface might feel more familiar to VSCode users. It’s not necessarily an either/or choice—some engineers keep both handy, or teams might standardize on one.

In sum, Windsurf is an excellent tool if you want an AI coding assistant that truly “reads the docs/code” before writing. It minimizes the chances of hallucinated functions or misnamed variables because it can look things up. To get the most out of it, feed it clear instructions and let it rip in Write mode for big tasks, but also feel free to use it in a more controlled fashion for delicate changes. Always review the changes it makes (it will show them to you), especially for critical code. Windsurf is smart, but it’s not infallible. Used wisely, it’s like a hyperintelligent IDE that knows your entire project and can implement ideas across it, giving a serious boost to your throughput.

AI Models: The Landscape for Code Generation
The AI coding landscape has transformed dramatically, with multiple powerful models now competing for developers’ attention, including models from the Claude, Gemini, and OpenAI families. Where once a single model might have dominated, today’s ecosystem offers a rich selection of options, each with distinct strengths that make them suitable for different coding scenarios.

Understanding Model Categories
Today’s coding models generally fall into several categories based on their approach and strengths:

Speed optimized
These prioritize quick responses and are ideal for real-time code completion and rapid iteration. They typically offer lower latency at the cost of slightly reduced accuracy on complex tasks.

Deep reasoning
These take more time to “think through” problems but excel at complex debugging, architectural decisions, and multistep problem solving. Models with advanced reasoning capabilities can break down complex bugs step-by-step.

Multimodal powerhouses
Some models can process not just code and text but also images, diagrams, and even video content. This makes them particularly valuable for understanding visual documentation or working with UI/UX elements.

Open source alternatives
DeepSeek stands out by offering a comparable level of AI power to closed-source models without requiring payment or sign-up, though it may lack some features like image generation or web browsing capabilities.

Choosing the Right Model for Your Task
Rather than seeking a single “best” model, successful developers now match models to specific tasks:

For rapid prototyping and general coding, models optimized for speed and broad language support work well.

For complex debugging and system design, deep reasoning models that can trace through logic methodically are a good choice.

For working with large codebases, choose models with extensive context windows that can maintain project-wide awareness.

For budget-conscious teams, open source models provide excellent value without subscription costs.

Many tools now support multiple AI models, including OpenAI, Claude, and Gemini variants, along with proprietary models, allowing developers to switch between them based on the task at hand.

Practical Tips for Any Model
Regardless of which AI model you choose, certain practices consistently improve results. First, provide rich context. Don’t just ask for “a payment processing function.” Instead, share your data models, existing code patterns, error-handling approaches, and any specific requirements. The more context you provide, the better the output will align with your codebase.

Most modern coding models excel at reviewing their own output. After receiving generated code, ask the model to check for potential issues, suggest improvements, or explain its reasoning. This self-critique often catches subtle bugs or suggests optimizations.

Use the model’s ability to maintain conversation context. Start with a basic implementation, then progressively refine it through follow-up requests. This iterative approach often yields better results than trying to specify everything up front.

Each model has subtle differences in how it approaches problems. Some are more verbose in their explanations, while others are more concise. Some default to newer syntax, while others play it safe. Learning these tendencies helps you craft better prompts.

Major Models
The AI coding landscape evolves monthly, with new models regularly challenging established leaders. The competition has become so intense that developers benefit from unprecedented choice and capability improvements. What matters most isn’t picking the “perfect” model but understanding how to leverage the strengths of whatever tools are available.

Many development teams now use a portfolio approach—leveraging fast models for routine tasks, powerful models for complex challenges, and specialized models for specific domains like database optimization or frontend development. Some IDEs even allow seamless switching between models midtask.

Success comes from understanding these options and strategically applying them to accelerate your development workflow.

Google Gemini: The Multimodal Coding Powerhouse
Google’s Gemini family represents a fundamental shift in AI-assisted development through its native multimodal capabilities. Unlike models that were primarily trained on text and code, Gemini was architected from the ground up to seamlessly understand and work across text, code, images, video, and other data formats. This makes it exceptionally powerful for modern development workflows where visual context matters as much as textual information.

The multimodal nature of Gemini proves particularly valuable in web development scenarios. Developers can share screenshots of design mockups, and Gemini can generate pixel-perfect implementations that match the visual style. It excels at understanding charts, diagrams, and UI elements, making it an ideal partner when translating visual designs into functional code. This capability extends beyond simple image recognition: Gemini can reason about visual elements, understand design patterns, and maintain aesthetic consistency across an entire project.

Gemini’s integration with development workflows through popular editors (VSCode, Cursor, Windsurf) and plug-ins like Cline and Code Assist offers developers powerful customization options that scale from individual preferences to team-wide standards. Developers can create custom commands for repetitive tasks, establish rules that apply to every code generation, and maintain consistent coding patterns across large codebases. The generous free tier makes it accessible to students, hobbyists, and startups, while enterprise features support complex organizational requirements.

What distinguishes Gemini in the coding landscape is its ability to think deeply about problems while maintaining practical speed. The model can alternate between quick responses for simple tasks and extended reasoning for complex challenges, adapting its approach based on the problem at hand. This flexibility, combined with its visual understanding capabilities, makes it particularly effective for full stack development where both backend logic and frontend aesthetics matter equally.

Claude: The Reasoning Virtuoso
Anthropic Claude’s approach to coding assistance centers on transparency and deep reasoning capabilities. The Claude family, particularly the Sonnet models, has established itself as exceptionally capable at complex software engineering tasks that require careful analysis and step-by-step problem solving. What sets Claude apart is its ability to show its thinking process, allowing developers to follow along with its reasoning and verify its logic before implementing solutions.

The Artifacts feature represents a paradigm shift in how developers interact with AI coding assistants. Rather than simply providing code in a chat interface, Claude creates a dedicated workspace where code can be viewed, edited, and previewed in real time. This interactive environment is particularly powerful for frontend development, data visualization, and any scenario where immediate visual feedback accelerates the development process. Developers can iterate on designs, test functionality, and refine implementations all within the same conversation.

Claude demonstrates exceptional performance on real-world software engineering benchmarks, consistently ranking among the top models for tasks like bug fixing, feature implementation, and code refactoring. Its strength lies not just in generating code but in understanding the broader context of software projects. Claude can analyze existing codebases, identify patterns and antipatterns, suggest architectural improvements, and maintain consistency with established coding styles. This makes it invaluable for both greenfield projects and legacy system maintenance.

The model’s approach to memory and context management enables it to build understanding over extended coding sessions. When working with large projects, Claude can extract and retain key information about the codebase structure, design decisions, and project-specific patterns. This accumulated knowledge allows it to provide increasingly relevant and contextual suggestions as development progresses, making it feel more like a team member who grows familiar with the project over time rather than a stateless assistant.

ChatGPT: The Versatile Coding Companion
ChatGPT has established itself as the Swiss Army knife of AI coding assistants, valued not for specialized features but for its remarkable versatility and broad knowledge base. Its position in the developer toolkit is unique. While other models might integrate directly into IDEs or offer specialized coding environments, ChatGPT serves as an always available programming consultant that developers keep open in their browsers throughout the workday.

The conversational interface of ChatGPT makes it exceptionally effective for exploratory problem solving and learning. Developers regularly use it for rubber-duck debugging, pasting in problematic code and thinking through issues in natural conversation. Its extensive training enables it to recognize patterns across virtually every programming language, framework, and tool in common use. Whether debugging a regex expression, understanding an obscure error message, or exploring unfamiliar library documentation, ChatGPT can provide relevant insights drawn from its comprehensive knowledge base.

ChatGPT’s strength lies in its ability to bridge the gap between human intent and code implementation. It excels at bidirectional translation—converting natural language descriptions into working code and explaining complex code in plain English. This makes it invaluable for documentation, code reviews, and knowledge transfer within teams. Developers can paste unfamiliar code and receive clear explanations of its functionality, or describe desired behavior and receive appropriate implementations across multiple programming paradigms.

The model’s versatility extends beyond traditional programming languages to configuration files, scripts, data formats, and domain-specific languages. While specialized coding tools excel within their focused domains, ChatGPT provides valuable assistance across the entire spectrum of software development tasks. This breadth makes it particularly useful when working at the boundaries between different technologies or when encountering problems that span multiple domains. Its ability to maintain context across extended conversations allows developers to explore complex problems iteratively, refining solutions through collaborative dialogue.

Choosing the Right Model for Your Needs
The availability of these powerful AI coding assistants represents a fundamental shift in software development practices. Rather than viewing them as competing options, successful developers recognize that each model family brings unique strengths to different aspects of the development process. Google’s Gemini excels when visual context and multimodal understanding are crucial, particularly in UI/UX development and when working with design specifications. Anthropic’s Claude shines in scenarios requiring deep reasoning, complex refactoring, and transparent problem-solving approaches. The OpenAI family of models provides unmatched versatility and broad knowledge, making it ideal for learning, debugging, and cross-domain challenges.

Many development teams now employ a portfolio approach, leveraging different models for different tasks within the same project. A typical workflow might involve using Gemini to translate design mockups into initial implementations, Claude for complex architectural decisions and code reviews, and ChatGPT for general problem solving and documentation. This multimodel approach maximizes productivity by matching each tool’s strengths to specific development challenges.

As these models continue to evolve, the key to effective AI-assisted development lies not in choosing a single “best” option but in understanding how to orchestrate multiple AI assistants to accelerate and enhance every aspect of the software development lifecycle.

This ecosystem is young and rapidly changing. New players and capabilities are emerging every few months. The key takeaway is that you don’t have to build your own AI from scratch to leverage programming with intent—there are plenty of tools that bring this power to your fingertips. Throughout this book, I’ll discuss various platforms and how they fit into the vibe-coding workflow.

The Benefits and Limitations of Vibe Coding: A Nuanced View
It’s important to recognize the scenarios where AI-assisted development truly shines⁠—and where it might still fall flat. Let’s explore some ideal use cases where vibe coding excels, as well as situations where today’s AI still struggles or requires heavy human intervention.

Ideal Use Cases for Vibe Coding
Just as certain architectures are suited for certain problems, vibe coding has its “sweet spots” in the software development landscape.

Zero-to-one product development
Vibe coding is a game changer for getting a brand-new project off the ground. The term zero to one (popularized by Peter Thiel) refers to creating something new from scratch. With AI, you can go from a blank canvas to a functional prototype at lightning speed. Need to stand up a web app that’s never existed before? You can generate boilerplate code for your frontend, backend, database schema, and even deployment scripts in one frenetic session of prompting. This is perfect for startups or hackathon projects where the goal is to validate an idea quickly. Instead of spending weeks setting up the “scaffolding” of a project (all the repetitive setup code), you can have the AI do it in minutes.

Many developers have recounted how they built an MVP over a weekend with the help of AI pair programmers—something that might have taken them a month working solo before. By quickly materializing the idea into a working product, you can start testing it with users or stakeholders much sooner. The AI is great at the generic stuff (setting up routing, basic UI components, standard CRUD operations), which frees you to focus on the novel aspects of your product.

However, once your MVP gains traction and moves toward production, your approach must shift. This is where AI-assisted engineering becomes essential. While vibe coding has helped you explore and validate quickly, scaling now requires more deliberate practices. You’ll need to refactor that rapidly generated code with proper error handling, add comprehensive test coverage, and establish clear architectural boundaries. The transition from prototype to product marks the natural evolution from vibe coding’s exploratory freedom to engineering’s structured discipline. Smart teams recognize this inflection point and adjust their AI usage accordingly—maintaining velocity while introducing the guardrails necessary for sustainable growth.

Feature prototyping and CRUD applications
A lot of software engineering, especially in business apps, involves CRUD—create, read, update, delete—functionality around data. This is formulaic work that AI is exceptionally good at because it’s seen countless examples. If you need to add, say, a new “Inventory” module to your system with CRUD screens and APIs, vibe coding will handle that extremely well. It can produce database migrations, ORM models, API endpoints, and UI forms with validation—basically the full stack—largely error-free because these patterns are so common in its training data. Even if your app has custom rules, you can specify those in a prompt and get a decent first pass. The result: what used to be a week-long task of boring wiring-up becomes an afternoon of prompting and testing. For internal tools or admin panels (which are essentially big CRUD apps), you might almost entirely lean on AI to generate them, given how straightforward yet time-consuming they normally are.

The engineering approach becomes crucial when these CRUD operations involve complex business logic, data validation rules, or integration with existing systems. While vibe coding can generate the basic structure quickly, AI-assisted engineering ensures that your inventory module properly handles edge cases like concurrent updates, maintains referential integrity, and follows your organization’s established patterns. For instance, you might use vibe coding to generate the initial CRUD scaffolding, then switch to engineering mode to implement domain-specific rules like inventory threshold alerts, multiwarehouse allocation logic, or integration with your existing authentication and authorization systems. The key is recognizing when to transition from rapid generation to careful refinement.

Glue code and integration
Need to integrate two services or APIs together? That often involves reading docs and writing code to transform data from one format to another. AI models have often been trained on API documentation and code examples, meaning they can expedite integration work. Ask ChatGPT to show how to call Service A’s API from Language B—chances are it will produce example code with the right endpoints and maybe even an auth example. Combining multiple systems (like hooking up a payment gateway with your order system or connecting a third-party analytics SDK) becomes easier when the AI can suggest the boilerplate and edge cases to handle. It excels at these standard integration patterns.

Modern framework utilization
AI coding assistants have effectively read the manuals on all popular frameworks: React, Angular, Django, Rails, Node/Express, Flutter—you name it. This means that if you’re using well-known frameworks, the AI can generate idiomatic code for those frameworks. For instance, it can spit out a new React component with hooks and state management or a new Django model with the proper admin class and serializer. The benefit is you don’t have to remember every little detail—the AI fills in the gaps. Vibe coding performs especially well with modern web development tasks like generating HTML/JSX with the right classes or hooking up controller endpoints, because these are tasks AI models have seen over and over. It’s like having a framework expert always by your side to write the boilerplate while you decide on the specifics of what the feature should do.

Repetitive code generation
Sometimes you need to create lots of similar code (like many similar endpoints or classes for each type in some schema). This can be tedious and error-prone for a human. AI, on the other hand, loves repetitive structures—once you show it one or two examples, it can churn out the rest consistently. This bulk code generation can save a ton of time. For instance, if you’re writing data model classes for 50 types of records, you can prompt one example and ask the AI to generate classes for all 50 types following that pattern. It will likely do so flawlessly and in seconds. The result: you avoid a whole day of monotonous coding.

When AI-assisted engineering should take precedence
While vibe coding excels in certain scenarios, AI-assisted engineering becomes indispensable in others. Understanding these situations helps developers choose the right approach from the start, avoiding costly rewrites or technical debt. Complex algorithmic implementations require the engineering approach. When you’re building sophisticated data structures, implementing performance-critical algorithms, or solving novel computational problems, you need precise control over every aspect of the implementation.

Here, AI serves as a knowledgeable assistant rather than a code generator. You might ask it to explain algorithmic approaches or review your implementation for correctness, but you maintain direct control over the architecture and optimization decisions. The AI helps you think through problems rather than solving them wholesale.

Mission-critical systems demand engineering rigor from the outset. Financial transactions, healthcare applications, security infrastructure, and other high-stakes domains cannot afford the exploratory nature of vibe coding. In these contexts, every line of code needs careful consideration, comprehensive testing, and often regulatory compliance. AI assists by suggesting best practices, identifying potential vulnerabilities, and helping ensure compliance with standards, but the developer maintains tight control over the implementation.

The cost of failure in these systems far outweighs any speed advantages from rapid generation. Legacy system integration presents unique challenges where engineering discipline proves essential. When working with decades-old codebases, proprietary protocols, or systems with extensive technical debt, vibe coding’s pattern matching often fails. These scenarios require deep understanding of existing constraints, careful planning of integration points, and methodical refactoring. AI can help by explaining legacy code patterns or suggesting modernization strategies, but the actual implementation requires the precision that only structured engineering provides.

Performance optimization represents another domain where engineering trumps vibing. While AI can generate functional code quickly, it rarely produces optimal solutions for performance-critical paths. Tasks like memory management, cache optimization, parallel processing, and latency reduction require deep understanding of hardware, operating systems, and algorithmic complexity. Here, AI serves best as a research assistant, helping you explore optimization techniques or benchmark different approaches, while you make the informed decisions about implementation.

In these scenarios, AI’s pattern recognition and speed align perfectly with the task. Essentially, vibe coding thrives on tasks that are well-trodden territory in programming (like CRUD or typical web app structures) and tasks that benefit from rapid trial and error (prototypes, new ideas). It’s like having a junior developer who has read every GitHub repo and can instantly recall how it’s usually done and write it for you to review. That’s incredibly powerful for getting things moving quickly.

Recognizing the transition points
The art of modern AI-enhanced development lies not in choosing one approach over the other but in recognizing when to transition between them. Successful developers develop an intuition for these inflection points. Starting a new feature? Begin with vibe coding to explore possibilities quickly. Notice the code becoming complex or touching critical systems? Shift to engineering mode. Building a proof of concept for a client demo? Vibe coding gets you there fast. Converting that proof of concept into a production system? Time for engineering discipline.

This fluidity—the ability to move seamlessly between rapid exploration and careful construction—distinguishes truly effective AI-augmented developers. They understand that vibe coding and AI-assisted engineering are complementary tools in their toolkit, each suited for different phases of the development lifecycle. The goal isn’t to pick a side but to leverage both approaches strategically, maximizing both velocity and quality throughout the software development process.

Where AI Still Struggles
As impressive as current AI coding tools are, they are not magic. There are classes of problems that remain difficult for AI to handle reliably, often requiring human insight or traditional coding techniques. Knowing these limitations helps set the right expectations and lets you plan when to lean in versus when to take back the reins.

The limitations include the following:

Deeply complex systems
If you’re dealing with very complex algorithms or novel problems that the AI likely hasn’t seen, it may flounder. For example, writing a brand-new algorithm from a research paper or doing something like writing a compiler or highly concurrent system—these involve intricate logic that requires true understanding and often creative leaps. AI can try, but it might get things subtly wrong.

In complex domains like these, the AI’s tendency to make approximately correct but not exactly correct code can lead to a lot of back-and-forth. As Chapters 3 and 4 will discuss, the final 30% or so of correctness is very hard for the AI to nail down. This is related to what I call the 70% problem—AI gets you most of the way quickly, but the last part is tough. An experienced developer might use AI to generate skeletons or helper functions for such complex tasks but do the core logic themselves.

Low-level optimizations and systems programming
Current AI models are primarily trained on high-level languages and abstractions. If you need to do low-level bit-twiddling, write highly optimized C code for a specific microcontroller, or generate vectorized SIMD instructions, the AI might not be reliable. It might produce code that looks plausible but isn’t truly optimal, or even correct, on a hardware level.

Similarly, for things like memory management or real-time constraints, the AI doesn’t have a real concept of those (it doesn’t simulate a CPU cache in its head). So for performance-critical code, you’ll want to either thoroughly test AI suggestions or write those parts manually. That said, AI might still help by providing a starting template or explaining assembly, but you cannot blindly trust it in these scenarios.

Unique or niche frameworks
If you’re using a very new or obscure framework that wasn’t around during the AI’s training, it won’t know about it. In such cases, the AI might try to generalize or might produce code that looks like it fits but actually call functions that don’t exist (hallucinations) or use outdated versions of the API. For example, if a new web framework version came out last month with breaking changes, the AI won’t know about those changes. It might give you code for the old version. In these cases, you have to fall back on documentation and perhaps even help train the AI by feeding it context from the docs within your prompt (basically teaching it on the fly).

Creative UI/UX design
If you ask AI to design a completely novel user interface or experience, it’s not great at that creative leap. It can generate UI code for known patterns (like a standard form or a dashboard), but if you want an innovative UI that doesn’t have clear precedents, the AI might not give you something inspiring. It might just stitch together familiar components. Human designers and frontend devs are still very much needed to dream up new user experiences. In coding terms, AI can make you a standard-looking interface quickly, but for that special custom feel, you’ll guide it or hand-tweak.

Interpreting intent and requirements
Sometimes AI struggles when requirements are implicit or contradictory.  It has no true understanding of the end goal beyond what you explicitly tell it. If requirements are vague (“make it efficient”—what does that precisely mean?), the AI might guess incorrectly what you care about (memory versus speed, for instance). Humans are better at clarifying intent, especially with nontechnical stakeholders. AI can also misinterpret instructions, especially if there’s domain-specific context it’s unaware of (like business rules). It might produce a logically correct solution that doesn’t actually solve the real problem because the nuance was lost in translation.

A good example scenario combining these: imagine developing a new 3D graphics engine (complex system) in Rust (system-level, performance critical). You have novel algorithms for rendering (unique problems). AI could maybe help write some boilerplate, but you’d largely rely on human ingenuity for the core. The AI might get you started with setting up a window and a basic render loop (common tasks), but for the bespoke parts, you’d proceed with traditional careful coding and perhaps get some algorithmic help from AI in pseudocode form. And if you asked it to optimize a hot loop in assembly, you’d have to verify every instruction.

AI also lacks true problem-solving insight. At the end of the day, it’s pattern matching. So if your problem requires an aha! insight, the AI might just flail around, presenting things that look like code but don’t solve it. This is where a human stepping back, thinking abstractly, or drawing on real experience can save the day. Once you have the insight, you can then use the AI to implement it quickly.

Understanding these strengths and weaknesses ensures you’ll deploy vibe-coding techniques in the right situations. To maximize success, leverage the AI for what it’s good at (the known patterns), and apply your creativity to the unique parts of your application. Be ready to intervene in those areas where AI is known to struggle. For instance, do a careful review of any security-sensitive code it writes, because it might miss an edge case or two.

Use AI to complement human strengths: let it handle breadth (lots of code, boilerplate) while you handle depth (complex logic, architecture). Use it as a booster where it excels, and don’t be afraid to take the wheel on those tougher stretches of the road. This plays to the strengths of both and yields the best outcome. Knowing when to use AI and when to rely on human skill is what will make you a highly effective developer in this new era.

Every new technology comes with its advantages and its caveats. As we embrace the productivity and creativity boost from AI-assisted development, it’s important to approach it with a nuanced understanding of its limitations and trade-offs. Key benefits include:

Faster development cycles
Projects can move from concept to prototype to finished product more quickly. AI can generate scaffolding code (like setting up the boilerplate for a new project) in a flash, so you spend more time on the unique parts of your application.

Enhanced prototyping and experimentation
Because the cost of trying something is lower (just describe what you want to the AI and get a quick draft), developers may feel freer to experiment. You can prototype multiple approaches to a problem by prompting the AI in different ways, then pick the best one. This iterative ideation can lead to more creative solutions.

Knowledge at your fingertips
LLMs are trained on a vast corpus of programming knowledge. It often “knows” obscure APIs or error message solutions. In practice, it can surface solutions or ideas you might not have thought of, making you a more effective problem solver.

Consistency and standardization
In team settings, an AI assistant can help enforce coding standards and best practices by generating code in a consistent style. If configured with your project’s style guide, it could ensure everyone’s code follows similar patterns. Even without explicit training, AI models often produce idiomatic code (since they learned from millions of examples). This can reduce the effort involved in code reviews, since its functions may look familiar and adhere to common conventions by default.

Some of the limitations and trade-offs to consider include:

Variable output quality
These models are not infallible. They might produce code that looks correct but has subtle bugs or inefficiencies. They might choose an outdated approach because their training data included a lot of older code. As a developer, you must remain vigilant. Just as you wouldn’t copy-paste code from the internet without understanding it, you shouldn’t accept AI code thoughtlessly. Part II of this book will discuss techniques to validate and test AI-generated code thoroughly.

Ambiguity in prompts leads to ambiguity in code
If your prompt is underspecified, the AI has to guess your intent—and it might guess wrong. For example, if you tell it to “sort a list of names,” it might default to alphabetical sorting, but maybe you meant something else (like sorting by the length of the name). The AI won’t know the difference unless you clarify it. This is why specificity in prompts (Chapter 2’s topic) is vital—you’ll learn to anticipate what details you need to spell out.

Overreliance and skill atrophy
If new developers always rely on AI to write their code, will they develop the same depth of understanding of algorithms and debugging? There’s a risk of skill atrophy, similar to how relying on GPS for navigation might weaken your own sense of direction. To mitigate this, it’s important to use AI as a learning tool (pay attention to the code it provides and ask why) and sometimes practice coding without it to ensure you retain your fundamental skills.

Privacy and security concerns
Using cloud-based AI coding tools often means sending your code (which might be proprietary or sensitive) to a third-party service for analysis. Companies need to consider this. Many tools are addressing it by allowing on-premises models or giving assurances about not storing code, but it’s still a consideration. Also, there’s a risk that AI might inadvertently generate code that is very similar to something in its training data, which could be under an open source license (like GPL). While unlikely (and measures are in place to prevent verbatim long outputs), it highlights the need to review and understand what the AI produces before integrating it. Chapter 8 dives into questions of security and reliability.

Bias in AI output
AI models can reflect biases present in their training data. In a coding context, this might be as benign as preferring certain variable names or as significant as using examples that assume particular user attributes. For instance, it might use foo/bar for every example variable (because many examples did), or it might assume things about user locales. It’s usually not a huge issue in code generation compared to other AI applications, but it’s worth being aware of this possibility. More subtly, the AI might be biased toward solutions it saw more often, even if those aren’t the best for your case. Chapter 9 discusses bias and other ethical considerations.

Human factors and trust
Not all developers are immediately comfortable with this style of work. Coding has a certain pleasure and artistry to it, and some may feel that is diminished by AI involvement. There can also be an initial lack of trust—“Did it really do this right?”—which only good practices and time can overcome. Teams adopting AI should allow a period of adjustment and encourage sharing of experiences and tips. Over time, as with any tool, most will find a balance where the AI’s contributions are valued and human expertise focuses on what humans do best.

Summary and Next Steps
The vibe shift toward programming with intent offers tremendous potential to make software development faster, more accessible, and in many ways more enjoyable. But realizing that potential means understanding the new dynamics: how to communicate with AI effectively, how to verify its output, and how to integrate it responsibly into your development process.

My perspective, forged from working with these tools and observing many projects, is that AI’s best use lies in combining the creative “vibe” with solid engineering hygiene. Encourage the wild ideas and rapid drafts that AI can offer—those are the new superpowers at our disposal. But channel them with the wisdom that software development has accumulated over decades: the importance of planning, testing, and understanding what you build.

When we strike that balance, we get the best of both worlds. We get software that is built faster and potentially more imaginatively but also software that we trust, maintain, and grow with confidence. That, ultimately, is how we elevate our craft in the age of AI: not by choosing vibes over engineering, or vice versa, but by mastering the whole spectrum between.

Next, Chapter 2 explores the art of crafting prompts and collaborating with AI. With the foundational concepts from this chapter in mind, you’re ready to explore the practical side of this new programming era. This will set the stage for hands-on examples and deeper prompting techniques in subsequent chapters.

Chapter 2. The Art of the Prompt: Communicating Effectively with AI
In vibe coding, prompts are the new source code.

The way you communicate your intent to the AI has a direct impact on the quality of the code it generates. Writing a good prompt is both an art and a science, often called prompt engineering. This chapter will equip you with techniques to get the most out of your AI coding assistant. We’ll start with some fundamentals about why prompts matter and then delve into a toolbox of prompting techniques, from simple to advanced. By learning how to craft effective prompts and how to iteratively refine them (Figure 2-1), you’ll be able to cocreate with AI more efficiently and accurately.


Figure 2-1. An illustration of a chatbot assisting with coding. The developer and AI engage in a dialogue: the developer provides instructions or questions (prompts), and the AI responds with code or answers. Communicating effectively with the AI through well-crafted prompts is key to getting accurate and useful code generation.
Prompt Engineering Fundamentals
If vibe coding is a conversation between you and an AI model, prompt engineering is the skill of speaking the AI’s language to get the best results. A well-crafted prompt can be the difference between an irrelevant or buggy code suggestion and a perfect solution. Mastering prompt engineering means understanding how to guide the AI effectively, how to provide context, and how to iterate with the AI when the first answer isn’t quite right.

When you program with an AI, you are essentially programming through the AI using natural language. The prompt you provide is like a high-level programming language that the AI interpreter then translates into actual code. Just as a compiler’s output is only as good as the source code fed to it, an AI’s output is only as good as the prompt.

Why are prompts so important? LLMs, despite their sophistication, are not mind readers. They respond only to the input they’re given. Ambiguous or poorly worded prompts can lead to irrelevant or incorrect code, while a clear and specific prompt can yield a spot-on solution on the first try. In traditional coding, you spend time thinking about algorithms and writing code; in vibe coding, you spend time thinking about how to convey your requirements to the AI. It’s a shift in what “writing code” means: you might write a paragraph instead of a function, but you still need to be precise and logical.

Think of writing prompts as being like writing documentation or user stories for a very literal and pedantic junior developer, one who will do exactly (and only) what the documentation says, and who has a lot of knowledge but no common sense beyond patterns they have seen. If your instructions (prompts) leave room for interpretation, the AI might fill the gaps in ways you didn’t intend. Thus, learning to communicate with the AI is as crucial as learning a programming language’s syntax used to be.

Another reason prompts are crucial is reproducibility and future-proofing. If you discover a prompt that reliably generates good code for a certain pattern or task, that prompt becomes a valuable piece of knowledge (almost like a snippet or template). You might save it or reuse it in similar contexts. In teams, developers might share effective prompt patterns with each other, similar to how they share coding best practices.

Finally, as models get better and more integrated, they may allow more complex interactions. Being good at prompting will let you harness new capabilities quickly. For instance, some advanced systems allow you to attach extensive instructions or provide entire reference documents as part of the context for the model. Knowing how to structure that input is key to leveraging such power.

So treat prompt writing as a new essential skill. In many ways, prompting is programming. The main difference is you’re writing in a language (like English) that the AI then converts into code. But you still have to be clear, logical, and anticipate edge cases in your description.

Specificity and Clarity: Writing Prompts That Deliver
One of the golden rules of prompting (which I’ll lay out more fully in Chapter 3) is to be specific and clear about what you want. Unlike a human collaborator, an AI doesn’t truly understand your goal beyond the words you provide. A common mistake is giving the AI a very high-level prompt like “Make a website” and expecting magic. The AI works better with concrete details.

Always assume it knows nothing about your project beyond what you provide. Include relevant details such as the programming language, framework, and libraries, as well as the specific function or snippet in question. If there’s an error, provide the exact error message and describe what the code is supposed to do. Any vagueness or room for interpretation can lead to unintended outputs.

For example, instead of “Write a sorting function,” you could say:

Write a Python function sort_by_lastname(customers) that takes a list of customer records (each with a first_name and last_name field) and returns a list sorted by last_name alphabetically. Include a brief docstring and handle the case of missing last names by treating them as empty strings.

This prompt sets clear expectations about the language (Python), the function name and purpose, the input structure, the sort key, additional requirements (docstring), and an edge case. It’s likely to produce exactly what you need or very close to it. Essentially, think like a spec writer: the more precisely you specify the task, the less guesswork the AI has to do and the fewer revisions you’ll need.

Strategies for specificity include:

Mention the language or environment
If you want a solution in JavaScript, say so: “Write a JavaScript function...” versus just “Write a function...” If you want it for a specific framework or version, include that (“Using React Hooks...” or “in Python 3...”).

Define the scope of the output
Do you want just a single function? A full file or module? Tests included? For example, “Provide only the function implementation” and “Provide a complete runnable script” can yield different responses.

Include requirements and constraints
In the login example, we specified password length and attempt limit. Think of edge cases or constraints and put them in the prompt. If you need the code to be optimized for performance or use a certain algorithm, say so: “using O(n) time and O(1) space” or “using a binary search approach.”

Avoid ambiguous references
Don’t use words like it without a clear antecedent. Instead of “Process it and return the result,” say, “Process the array and return the resulting array.”

Name your desired output format
If you want the AI to output just code or code with comments or an explanation, you can instruct that: “Give only the code, no explanation” or “Provide code and a brief comment for each step.”

A clear prompt sets the AI up for success. If you find the AI’s answers often need a lot of correction, examine whether your prompts might be underspecified.

Here’s what not to do:

Don’t write a whole novel
Long-winded prompts that include irrelevant info can confuse the model or cause it to focus on the wrong thing. Be concise but complete in your description. For instance, you usually don’t need to preface with “You are a world-class programmer...” in a coding context (some people do that in general ChatGPT usage, but for coding tasks, it’s often unnecessary and could add noise).

Don’t assume the AI will fill in details by itself correctly
If something is important (like thread safety, handling of special characters, etc.), mention it. If it’s not mentioned, assume the AI might not handle it.

Avoid open-ended “creative” prompts when you need deterministic outputs
For example, saying, “Write some code to analyze data” might cause the AI to guess what analysis you want. Instead, specify:

Calculate the average and standard deviation of a list of numbers.

In summary, say exactly what you mean. The more the AI “knows” about what you truly want, the better it can deliver. If you find yourself having to correct the AI multiple times, ask: could my initial prompt have been clearer?

Iterative Refinement: The Feedback Loop with the AI
Even with clear prompts, you won’t always get the perfect answer on the first try. Think of interacting with the AI as a conversation or an iterative development process. This is the feedback loop I touched on in Chapter 1.

When the AI gives you code, review it critically, just as you would code written by a human. Does it meet the requirements? If not, identify what’s missing or wrong. Then provide feedback or a refined prompt. This can be done in a conversational AI by simply continuing the dialogue, or in an editor by writing another comment for the AI to respond to.

By providing feedback to the AI, you steer it closer to your desired outcome. In a sense, you are training it on the fly for your specific problem. Advanced prompt engineering is like the loop in Figure 2-2: Prompt → AI output → Review → Refine prompt → AI output →...until satisfied. Keeping each iteration’s changes small is useful; if you overhaul the prompt too much, you may lose some good parts of the previous output.


Figure 2-2. Advanced prompt engineering loop.
For example, you might prompt:

Write a function that takes a list of integers and returns their sum.

The AI then returns a function, but its code assumes a non-empty list and doesn’t handle an empty list well. You could then reply:

That looks good. However, please modify it to return 0 if the list is empty.

The AI would then update the function accordingly. In this way, you didn’t have to prompt from scratch; you just told the AI to make an adjustment. The AI already had the context of the previous code it gave.

If you’re using an inline assistant, refinement might look like editing the code and perhaps writing a comment like # TODO: handle empty list and then seeing if the AI suggests a fix for that.

Another refinement approach is reprompting with more info if the first output wasn’t right. Suppose you said, “Sort a list of names,” and it gave code sorting case-sensitively but you wanted case-insensitive. You could rephrase:

Sort a list of names case-insensitively.

Or even:

The previous code sorts case-sensitively. Modify it to be case-insensitive.

In debugging, for more complex logic bugs (where no obvious error message is thrown but the output is wrong), you can prompt the AI to walk through the code’s execution. For instance:

Walk through this function line by line and track the value of total at each step. It’s not accumulating correctly—where does the logic go wrong?

This is an example of a “rubber duck” debugging prompt: you’re essentially asking the AI to simulate the debugging process a human might do with prints or a debugger. Such prompts often reveal subtle issues like variables not resetting or incorrect conditional logic, because the AI will spell out the state at each step. If you suspect a certain part of the code, you can zoom in:

Explain what the filter call is doing here and if it might be excluding more items than it should.

Engaging the AI in an explanatory role can surface the bug in the process of explanation.

After the explanation, it’s often effective to directly ask for what you need:

What might be causing this issue, and how can I fix it?

This invites the AI to both diagnose and propose a solution. If the AI’s first answer is unclear or partially helpful, don’t hesitate to ask a follow-up question:

That explanation makes sense. Can you show me how to fix the code? Please provide the corrected code.

In a chat setting, the AI has the conversation history, so it can directly output the modified code. If you’re using an inline tool like Copilot in VSCode or Cursor without a chat, you might instead write a comment above the code:

// BUG: returns NaN, fix this function and see how it autocompletes 
In general, though, the interactive chat yields more thorough explanations.

Another follow-up pattern: if the AI gives a fix but you don’t understand why, ask:

Can you explain why that change solves the problem?

This way, you learn for next time, and you double-check that the AI’s reasoning is sound.

LLMs thrive on examples and corrections. If you point out what’s wrong or give a quick example, the AI can incorporate it:

If input is [], it should return 0, but now it errors.

This iterative process is normal. In fact, trying to cram every detail into one prompt might be less effective than a couple of back-and-forth turns. Use that to your advantage.

Be patient and specific in your feedback. Instead of saying, “No, that’s wrong,” say what’s wrong or what’s needed:

This code doesn’t handle negative numbers correctly. It should treat them as 0 in the sum.

Also, if the AI goes off track, you can steer it back: sometimes resetting or rephrasing is easier than trying to salvage a very incorrect attempt. Use your judgment. If the AI output shows that it is completely misunderstanding you, clarify your prompt from scratch.

As you refine, you’ll also learn how the AI interpreted your prompt. This can inform how you write future prompts. You might realize, “Oh, it took ‘login system’ to mean an entire UI. Next time I’ll specify backend only.”

Think of it like debugging code: if the AI output is wrong, the “bug” might be in your prompt, not in the AI’s processing. Just as you’d examine and fix your code when it produces incorrect results, you should refine your prompts when the AI generates unexpected or incorrect output. The conversation between you and the AI is like a debugging session where each exchange helps you pinpoint and fix the issue.

Comparing Two Prompts
Imagine you have a simple Node.js function meant to convert a list of user objects to a lookup map by user ID. However, it’s throwing an error. Here’s the buggy code:

// Buggy function: converts array of users to a map by ID
function mapUsersById(users) {
  const userMap = {};
  for (let i = 0; i <= users.length; i++) {  
    // Note: using <= will go out of bounds on the last iteration
    const user = users[i];
    userMap[user.id] = user;
  }
  return userMap;
}

// Example usage:
const result = mapUsersById([{ id: 1, name: "Alice" }]);
This code is supposed to iterate through the user’s array and build an object where keys are user IDs. However, there is a bug: the loop uses i <= users.length instead of i < users.length. This will cause users[i] to be undefined on the last iteration, likely leading to a runtime error (like attempting to access the property ID of undefined). Let’s see how an AI assistant might respond to two different prompts about this issue (see Figure 2-3).


Figure 2-3. Prompt quality impact comparison: vague prompts produce generic or incorrect code, while specific, well-structured prompts generate accurate, contextually appropriate solutions.
Poor prompt
Here’s an example of a poor prompt:

Why isn’t my mapUsersById function working?

AI’s response to the poor prompt (simulated):

It’s hard to say without more information. Are you sure the function is being called with a proper array of users? Perhaps the issue is that users is empty or not an array, causing the loop not to execute properly. It could also be that there’s an error when setting the property on userMap. Make sure that each user has an id property. If users is undefined or null, that would also cause it to not work.

This answer is quite vague. The prompt gave no details except the function name, so the AI is guessing at generic causes (“Maybe the array is empty or not an array,” “Make sure each user has an ID”). It did not identify the actual bug.

This is the kind of response you get when the question is underspecified—the model can only regurgitate common debugging tips. If you’re a developer, this isn’t very helpful because it doesn’t address the real mistake. The lack of context and specificity in the prompt yields a generic answer: essentially, the AI equivalent of “Have you tried turning it off and on again?” This aligns with the community advice that vague prompts lead to irrelevant or generic answers.

Improved prompt
Let’s try this again:

I have a JavaScript function mapUsersById that should convert an array of user objects into a map (object) keyed by user ID. However, it throws an error when I run it. For example, when I pass [ {id: 1, name: "Alice"} ], I get TypeError: Cannot read property 'id' of undefined. Here is the function code:

function mapUsersById(users) {
  const userMap = {};
  for (let i = 0; i <= users.length; i++) {  
    const user = users[i];
    userMap[user.id] = user;
  }
  return userMap;
}
It should return { "1": {id: 1, name: "Alice"} }. What is the bug and how can I fix it?

The AI’s response to the improved prompt (simulated):

The error Cannot read property 'id' of undefined suggests that at some point, user is undefined, and then you try to access user.id. Looking at your loop, the issue is with the loop bounds. You are using i <= users.length. If users.length is 1 (for one user in the array), the loop runs with i = 0 and i = 1. When i is 1, users[1] is undefined (since the only index in the array is 0). Thus user becomes undefined on that iteration, and user.id throws a TypeError. The fix is to change the loop condition to i < users.length so that i goes from 0 to users.length-1. For example:

for (let i = 0; i < users.length; i++) {
  const user = users[i];
  userMap[user.id] = user;
}
This way, i will be 0 for the first (and only) element, and then the loop stops. With that change, your function will no longer hit an undefined user and should work as expected, returning {"1": {id: 1, name: "Alice"}} for the given input.

This response directly pinpoints the bug (using <= in the loop) and explains why it causes the user to be undefined. It then provides the correct code solution (i < users.length) and confirms the expected outcome. The improved prompt succeeded because it followed our principles: it specified the language (JavaScript), described the function’s purpose, included the exact error message and a sample input, and even provided the code snippet in question.

Notice how the AI used the error message as a clue to focus on the loop bounds—a targeted prompt enabled the AI to engage in true problem solving, effectively simulating how a human debugger would think: “Where could undefined come from? Likely from the loop indexing.” This is a concrete demonstration of the benefit of detailed prompts.

Prompting Techniques: A Toolbox for Effective Communication
Now let’s get into specific techniques that can supercharge your prompting skills. These are like patterns or recipes you can use when a straightforward instruction isn’t enough or when you want to guide the AI in a certain way.

By mastering these techniques, you can handle an array of situations: instructing the AI in plain English, giving it examples, making it explain or structure its output, or setting it into different mindsets or roles. All of these help you guide the AI to produce exactly what you need.

Prompting techniques are not mutually exclusive; you will often use several together for best results, especially on complex tasks.

A Note on Style
When you use these techniques, adapt your tone to the model. Many models respond well to polite or neutral instructions. You don’t need to use archaic or overly formal language. Direct but polite often works: “Please do X” or “Let’s do Y.” For example, with chain-of-thought (CoT) prompting, a popular phrase is “Let’s think step-by-step.” Models like GPT-4 recognize this as a cue to show reasoning.

Zero-Shot Prompting
Zero-shot prompting is simply asking the model to do something without providing any examples or additional guidance beyond the instruction. Essentially, the model is solving the task from “zero” examples.

When to use: This is the most common scenario: you just ask for what you want in plain language. If the task is standard and the prompt is clear, this is often sufficient.

Example:

Write a Python function that checks if a number is prime.

This is zero shot. The AI will likely produce a prime-checking function using a loop or trial division.

Pros: It’s quick and relies on the model’s learned knowledge. Modern models are surprisingly good at zero-shot responses for many programming tasks, especially if they’re common (like prime checking, sorting, or string manipulation).

Cons: If the task is unusual or output format is specific, zero shot might yield a result that doesn’t quite match what you need on the first try, because the model might have multiple ways to interpret it.

Usually, it’s a good idea to try zero shot first for simple things. If the result is off, you may then shift to refining or other techniques.

One-Shot and Few-Shot Prompting
One-shot prompting means you provide exactly one example of what you want (input and desired output) as part of the prompt; few-shot prompting means providing a few examples (typically two to five) before asking the model to perform the task on a new input.

This is like showing the model, “Here’s how I solve one instance. Now you do the next one similarly.”

When to use: This type of prompting is useful when the model might not know exactly the format or style you need or when the task is a bit unusual. By giving examples, you reduce ambiguity.

Example (one shot): Suppose you’re using a language or a certain style that the model might not have seen as much. Let’s say you want pseudocode in a specific format. Your prompt might be:

Convert the following English instructions to Python-like pseudocode.

Example instruction: “Calculate the factorial of n”:

Example pseudocode:

function factorial(n):

    if n <= 1:

        return 1

    else:

        return n * factorial(n-1)

Instruction: "Find the largest number in a list"

Pseudocode:
You’ve provided one example (factorial) and the format you want. Now the model is more likely to produce pseudocode for the “largest number” instruction in a similar format (with a function, with if/else or loop logic as needed).

Example (few shot): Let’s say you want the AI to use a specific algorithm. You might give it a smaller example of that algorithm in action as a hint. Or if the task has multiple correct answers but you prefer a certain one, an example can push it toward that.

Few-shot prompting is powerful for formatting; for instance:

Convert The Following English Statements To SQL Queries.\N1."Get All Employees Hired After 2020” → Select * From Employees Where Hire_Date > ’2020-01-01’;\N2. “List Customer Names Who Made A Purchase In The Last Month” → Select Name From Customers Join Purchases On ... Where Purchase_Date > ...;\N3. “Count Of Products That Are Out Of Stock” →

Here, once you give two examples of English-to-SQL, the AI is likely to answer the third query correctly by following the pattern. Few-shot examples can be applied to coding too: show the AI the style you want in a small sample and then ask for more. It’s like giving it a minitraining dataset within your prompt.

Pros: You can achieve outputs in very specific styles. This technique also helps the model handle tasks that involve following a pattern or applying a concept repetitively.

Context Window
The term context window refers to the maximum amount of text (measured in tokens) that a language model can process in a single interaction, including both the input prompt and the generated response. This represents a limitation of current AI models—once you reach this threshold, the model cannot process additional information. When you’re crafting prompts, everything you include (instructions, examples, data, and the space needed for the response) must fit within this fixed capacity.

Cons: Few-shot prompting in particular makes the prompt longer (which uses up context window). For very large/complex examples, it might “eat” a lot of the model’s capacity. But usually a small example or two is fine.

Tip: If you want the model to strictly adhere to a certain output structure, giving an example can nearly guarantee it matches that structure rather than offering a free-form reply that you have to parse.

Chain-of-Thought Prompting
Chain-of-thought (CoT) prompting involves asking the model to think step-by-step or show its reasoning before giving the final answer. In other words, you encourage the model to break down the problem.

When to use: This is useful for complex problems that involve reasoning and multistep computations, or when you suspect the model might make a mistake if it jumps straight to the answer. It’s also useful if you want an explanation in the output.

Combinatorial Math
Combinatorial math deals with counting, arranging, and selecting objects according to specific rules or constraints. Common problems include calculating permutations (arrangements where order matters), combinations (selections where order does not matter), and other counting principles. Key notation includes “n choose k” (written as C(n,k) or nCk), which represents the number of ways to choose k items from n total items, calculated using the formula n!/(k!(n-k)!). These calculations frequently appear in probability, statistics, and discrete mathematics applications.

Example: Instead of just asking a combinatorial math problem like “What is 12 choose 4?” and getting an answer, you might say:

Solve 12 choose 4 step-by-step.

The model might then outline:

12 choose 4 = 12!/(4!*8!) = ... = 495
In coding, CoT can be useful for tricky algorithmic tasks. You might try the following prompt:

Explain step-by-step how to merge two sorted lists, then provide the Python code.

The model would first outline something like:

We will use two pointers starting at the heads of each list, compare the elements, append the smaller to a result list, and move that pointer, and so on…

Then it might give the code. This ensures it has structured the solution correctly before coding.

Another use is debugging or understanding output:

Walk through the logic to determine if the number 19 is prime, then give the result.

The model might list divisions by primes, then conclude:

19 is prime.

Pros: Improves correctness on tasks requiring reasoning. There’s research evidence that prompting the model to “think out loud” can lead to better results on math and logic tasks. It also gives you insight into the model’s process, which can be instructive or help you trust the answer more.

Cons: The output is longer (which might not be what you want in final code). Also, some interfaces (like typical code completions) aren’t set up to show reasoning separate from code. This technique is more common in Q&A or chat scenarios. However, you can instruct the model to include the reasoning as comments in the code, which is a neat way to get thoroughly commented code.

Role Prompting
Role prompting means you ask the AI to assume a certain identity or role that might influence how it responds.

When to use: This is useful when you want to influence the style or detail of the answer or get a certain perspective. For instance, an AI taking on an “expert” role might give a more advanced solution or more explanation, while a “beginner” role might make it explain more basic concepts.

Examples:

You are a Python instructor. Explain the following code and then modify it to be more Pythonic.

Act as a security analyst. Here’s some code. Identify any security vulnerabilities.

Pretend you are a linter that checks code for style issues.

This can significantly affect the response. Assigning the AI a security analyst role might make it focus on things it otherwise wouldn’t mention (like data validation, secure coding practices, or potential vulnerabilities). An instructor role might make it provide clearer explanations and perhaps not assume prior knowledge.

In coding, you might say before asking for code:

You are an expert C++ programmer well-versed in optimization, instructing a junior developer.

The result will likely use more advanced C++ features and explain why certain choices were made, balancing technical sophistication with educational clarity.

Pros: This technique steers the tone and depth of the answer. This can tailor the solution to a certain level of complexity or thoroughness. It’s useful if you want either a very simple solution (tell it to act as a novice and maybe it’ll avoid complex tricks) or a very optimized one (tell it to act as a performance guru).

Cons: Sometimes the model might focus more on the persona than needed (an “instructor” might start explaining things you already know). Also, some AI safety systems are more sensitive to certain role descriptions—particularly those that might suggest deception, authority impersonation, or potentially harmful activities—though straightforward technical and professional roles like “data analyst” or “software engineer” typically work without issues.

Contextual Prompting
Contextual prompting means giving the AI additional context or information beyond the immediate task description. AI models don’t have persistent memory of your entire project unless you provide it in the prompt (or through some integrated context window in advanced IDE integrations). So if you want the AI to write code that fits into your existing codebase, give it that context. Basically, you supply relevant data or background as part of the prompt.

When to use: Use when solving a problem requires knowing certain data or definitions that the model might not know or might not recall correctly from training. Or use when you want to ensure consistency with some external info (like an API spec or previous part of conversation).

Examples:

If you have a data structure and you want code that works with it, you might paste its definition:

Given the class below, implement the function X.

class Node:

    def __init__(self, value, next=None):

        self.value = value

        self.next = next

# Now write a function to count the nodes in a linked list starting at head.
By including the class definition, you make the AI much more likely to use Node.value and Node.next properly in its code.

If you want to use a specific API, include a snippet of the documentation in the prompt:

Using the requests library, fetch the data from the API. (The API returns JSON with format: {...})

If you include even a short example of API usage from docs, the AI can mimic it.

For disambiguation:

Using the term student to refer to high school students, write a function that…

If student could be ambiguous in context, you’ve clarified it.

Pros: You’re grounding the AI in the context you care about. It’s less likely to make wrong assumptions if you supply the facts. This is extremely helpful if the AI otherwise might not remember or know your specific use-case details.

Cons: This technique makes prompts longer. Also, the model might occasionally regurgitate the provided context into the answer (like copying lines from a documentation snippet into the code if not careful). But usually it uses it appropriately.

Tip: If you have a large context (like a big schema or many lines of code), sometimes it’s better to summarize the key elements for the model rather than including everything verbatim. This approach helps you stay within context limits while ensuring the model receives the most relevant information. However, if the content is small enough, just include it raw.

Constraints are also useful to mention: performance constraints (“Optimize for O(n log n) or better”), compatibility constraints (“Must run on Python 3.8”), or library choices (“Use standard library only, no external dependencies”). These act like guardrails and ensure the AI doesn’t suggest something outside acceptable bounds.

Metaprompting
Metaprompting is giving instructions about the output itself, not just what the solution should do. It’s like telling the AI how to format or approach the solution.

When to use: Useful when you need the answer in a specific format or style or when you want to control how the AI works through the problem.

Examples:

First, explain the approach in two sentences, then provide the code.

This ensures the AI doesn’t launch straight into code:

Do not use any libraries in the solution.

This places a constraint on the solution:

Format the output as JSON.

This is useful if you’re using the AI to produce data, not code:

Only provide the function body, without the definition line.

This is handy if you want to insert the function into existing code:

If the input is invalid, instead of error, return None.

This is not exactly the output format, but it’s instructing the AI how to behave for certain cases.

Pros: You get exactly what you need, how you need it, without extra editing. This is crucial for some scenarios. If you plan to automatically use the AI’s output in a pipeline, then you really want consistent formatting.

Cons: If the instructions conflict with the model’s default style, sometimes it might partially follow them or you have to emphasize them. For instance, even if you say “only code, no explanation,” occasionally the model might include a tiny comment or so. Usually, phrasing it as a direct imperative helps:

Do not include any explanation; output only code inside a single code block.

Models like GPT follow that quite well.

Self-Consistency (Multiple Outputs and Majority Voting)
Self-consistency is more of a strategy than a prompt style. The idea is to get multiple outputs for the same prompt and then decide on the best or most common one. As Sander Schulhoff of Learn Prompting notes, self-consistency leverages the notion that if you ask the model multiple times (with slight randomness) and many of its answers agree, that consensus is likely correct.

When to use: This is useful for complex problems where you’re unsure the model’s first answer is correct, especially if you can’t verify it easily yourself, or if you want a confidence check from the AI by seeing whether it gives the same answer repeatedly.

How to use manually: On some platforms (like ChatGPT), you can click “Regenerate answer.” Or you can copy the prompt into a new session and see if it gives the same result. If you get three answers and two are the same and one is different, you might trust the two (assuming the problem has a single correct answer).

In programming context, if it’s generating code for something deterministic, usually it will give very similar code each time (with small variations in variable names or style). But if it’s an algorithmic question (like “What’s the output of this code?”), you could check multiple runs.

This technique is more powerful in noncoding tasks (like logic puzzles) but worth noting.

Another angle—ensemble prompting: You can actually ask the model within one prompt to consider multiple possibilities:

Give two different solutions to this problem.

Then perhaps you can see which one you like or test both. This is like self-consistency in one shot because you get multiple answers.

Pros: This technique can increase confidence in the solution if multiple attempts converge. Also, you might get variety (which is good if you want to choose the most elegant solution among many).

Cons: It’s time-consuming to do multiple calls and compare outputs.

In practice, if I’m unsure about an answer, I’ll often repose the question differently to see if I get the same answer. If I do, I’m more confident it’s correct.

ReAct (Reason + Act) Prompting
ReAct is a more advanced prompting technique that combines reasoning and acting. It gets the model not only to think, like CoT does, but also to take actions like making a calculation, calling an API, or using a tool. (See the ReAct Prompt Engineering Guide for more). In current practice, this is often used with frameworks like LangChain, where the AI can output a special format that a program interprets as an action (like a command to execute or a query to run), then feed the result back.

For our scope (without such an execution environment in the loop), you can still do a form of ReAct by instructing the AI to first outline a plan, then output the result. It’s similar to CoT but specifically oriented to using tools or performing subtasks.

Example:

Using Python, determine the current weather in Paris and print it.

Unless the AI has browsing capabilities, it cannot truly get the current weather. A ReAct approach would have the AI first reason through the problem by stating:

I need to access current weather data for Paris, which requires calling a weather API.

The AI would then attempt to use an available tool to make this API call. If successful, it would receive actual weather data; if no such tool is available, it might acknowledge the limitation or work with hypothetical data. Finally, the AI would write the Python code to display the weather information, incorporating whatever data it was able to obtain through this reasoning and action process.

Without external tool access, ReAct might not be particularly relevant for simple prompting tasks. However, when evaluating AI tools for your organization, determining whether they can access current information from the internet represents a critical capability assessment. Many AI models operate with knowledge cutoffs, meaning their training data only extends to a specific date, which can result in outdated information for rapidly changing topics.

If you are using an environment where the AI can execute code (such as Jupyter integrations or similar platforms), you could implement ReAct by instructing the system:

First write a test for this function, run it, then adjust the code accordingly.

This demonstrates the ReAct pattern through a reasoning step (writing the test), followed by an action (executing the test), and then code adjustment based on the results. However, orchestrating such workflows through pure prompts requires advanced prompting techniques and appropriate technical infrastructure.

Simpler use: You can simulate a Q&A where the AI has intermediate steps that mimic actions:

Think step-by-step and if you need to, do calculations.

It’s effectively CoT but with a more imperative tone.

Pros: When available, it can solve problems that require external info or iterative trial (like the AI can correct itself by actually running code). In debugging contexts, an AI that can execute code to test it is fantastic.

Cons: This technique is not widely accessible without specific tooling. And if you just prompt that way in plain ChatGPT, it will either imagine the actions or just do CoT.

For our purposes in prompt writing, keep in mind that some systems (like OpenAI’s tool-using agents or others) exist, but in vanilla prompting we mostly do CoT, and we ourselves handle actions like running the code or tests.

Advanced Prompting: Combining Techniques and Handling Complexity
Prompting techniques can be combined. For instance, you might do a few-shot prompt that also demonstrates CoT in the examples. Or you might combine a role with CoT:

As a senior engineer, think step-by-step through the problem, then give the code.

Now that we’ve explored various prompting techniques, let’s see them in action with a scenario or two, then discuss how to review and refine the AI’s output (which leads into the next chapter about understanding and owning the generated code).

Imagine you have a function that isn’t working. You might use a combination of role and CoT prompting:

You are a Python debugger. Let’s think step-by-step to find the bug in the following code.

This would be followed by the code. The AI might respond with an analysis of each line and pinpoint the bug.

Or let’s say you want to generate code for a somewhat complex algorithm, ensure it’s well commented, and also get test cases for it. A combined prompt might look like this:

You are an expert Python developer. Let’s solve this step-by-step. We need a function merge_sorted_lists(list1, list2) that merges two sorted lists into one sorted list. First, explain the approach, then provide the Python code with comments. After that, give 2–3 example tests in code to demonstrate it works.

This single prompt is quite comprehensive. The first sentence sets a role. The second requests step-by-step reasoning. The third gives the main task. The fourth sentence asks for code with explanatory comments, and the fifth even asks for tests.

The AI might then output an explanation, then the code with inline comments, then some test cases at the end. This is an advanced use, but it shows how you can direct the AI through a multifaceted response.

Know the Model’s Limits
Prompt engineering also involves knowing what not to ask and how to avoid pitfalls. If a prompt is getting too large or includes too many instructions, the model might get confused or truncate some output. If you find it starts ignoring parts of your prompt, you might need to simplify or do it in parts. If an AI model sometimes produces incorrect facts or code (it “hallucinates”), you learn to double-check and not use it as a factual oracle. If you find it tends to give overly verbose code, you can preempt that with “Make the solution as concise as possible.” If it sometimes uses functions that don’t exist, you might instruct, “Use only the API functions listed below” and list them. The better you understand the AI’s behavior, the more you can mold your prompts to get around any weaknesses.

If a task is very complex, you can also break it into subtasks for the AI. For example, you might first prompt:

List the steps to implement a basic compiler for a simple arithmetic expression language.

Once the AI gives the steps, you tackle each step with separate prompts, maybe even in separate files or sessions:

Now implement step 1: tokenization.

This is like doing system design with the AI: you can outline then refine each piece. It leverages the AI’s ability to assist in planning (not just coding).

Stateful Conversation Versus One-Shot Prompting
In a chat setting, you have a conversation history, known as state. You can build up context by discussing with the AI. In an IDE completion setting, the context is mostly your file content and comments. Both allow cumulative context in different ways. Use conversation if you need the AI to remember what was said (like refining an answer). Use fresh prompts or file context if you want to ensure it’s focusing only on what’s relevant now. Sometimes wiping away the context prevents the model from sticking to a potentially wrong earlier assumption.

By practicing with these techniques on various examples, you’ll become adept at knowing which approach to use and when:

If output format is important, give examples (few shot) or explicit formatting instructions.

If logic is tricky, use CoT or step-by-step.

If the solution can vary in quality, set a role (like “seasoned engineer”) to get a better style.

If the model isn’t complying, maybe break your prompts into pieces, simplify them, or use stronger wording for constraints.

Common Prompt Antipatterns and How to Avoid Them
Not all prompts are created equal. By now, we’ve seen numerous examples of effective prompts, but it’s equally instructive to recognize antipatterns—common mistakes that lead to poor AI responses. This section covers some frequent prompt failures and how to fix them.

The vague prompt
This is the classic “It doesn’t work, please fix it” or “Write something that does X” without enough detail. The question “Why isn’t my function working?” will generally get a useless answer. Vague prompts force the AI to guess the context and often result in generic advice or irrelevant code.

The fix is straightforward: add context and specifics. If you find yourself asking a question and the answer feels like a Magic 8–Ball response (“Have you tried checking X?”), stop and reframe your query with more details (error messages, code excerpt, expected versus actual outcome, etc.). A good practice is to read your prompt and ask, “Could this question apply to dozens of different scenarios?” If the answer is yes, it’s too vague. Make it so specific that it could only apply to your scenario.

The overloaded prompt
This is the opposite issue: asking the AI to do too many things at once. For instance:

Generate a complete Node.js app with authentication, a frontend in React, and deployment scripts.

Or even, on a smaller scale:

Fix these 5 bugs and also add these 3 features in one go.

The AI might attempt it, but you’ll likely get a jumbled or incomplete result, or it might ignore some parts of the request. Even if it addresses everything, the response will be long and harder to verify.

The remedy is to split the tasks. Prioritize: do one thing at a time, as we emphasized earlier. This makes it easier to catch mistakes and ensures the model stays focused. If you catch yourself writing a paragraph that uses “and” multiple times in the instructions, consider breaking it into separate prompts or sequential steps.

Missing the question
Sometimes users will present a lot of information but never clearly ask a question or specify what they need, like dumping a large code snippet and just saying, “Here’s my code.” This can confuse the AI—it doesn’t know what you want.

Always include a clear ask:

Identify any bugs in the above code.

Explain what this code does.

Complete the to-dos in the code.

A prompt should have a purpose. If you just provide text without a question or instruction, the AI might make incorrect assumptions (like summarizing the code instead of fixing it, etc.). Make sure the AI knows why you showed it some code. Even a simple addition like “What’s wrong with this code?” or “Please continue implementing this function” gives it direction.

Vague success criteria
This is a subtle one. Sometimes you might ask for an optimization or improvement, but you don’t define what success looks like—for example, “Make this function faster.” Faster by what metric? If the AI doesn’t know your performance constraints, it might micro-optimize something that doesn’t matter or use an approach that’s theoretically faster but practically negligible. Or “Make this code cleaner”: “cleaner” is subjective. We dealt with this by explicitly stating goals like “reduce duplication” or “improve variable names,” etc.

The fix: quantify or qualify the improvement:

Optimize this function to run in linear time (current version is quadratic).

Refactor this to remove global variables and use a class instead.

Basically, be explicit about what problem you’re solving with the refactor or feature. If you leave it too open, the AI might solve a different problem than the one you care about.

Ignoring AI’s clarification or output
Sometimes the AI might respond with a clarifying question or an assumption:

Are you using React class components or functional components?

I assume the input is a string—please confirm.

If you ignore these and just reiterate your request, you’re missing an opportunity to improve the prompt. The AI is signaling that it needs more info. Always answer its questions or refine your prompt to include those details.

Additionally, if the AI’s output is clearly off (like it misunderstood the question), don’t just retry the same prompt verbatim. Take a moment to adjust your wording. Maybe your prompt had an ambiguous phrase or omitted something essential. Treat it like a conversation: if a human misunderstood, you’d explain differently; do the same for the AI.

Inconsistency
If you keep changing how you ask or mixing different formats in one go, the model can get confused. Two examples include switching between first person and third person in instructions or mixing pseudocode with actual code in a confusing way.

Try to maintain a consistent style within a single prompt. If you provide examples, ensure they are clearly delineated (use Markdown triple backticks for code, quotes for input/output examples, etc.). Consistency helps the model parse your intent correctly. Also, if you have a preferred style (say, ES6 versus ES5 syntax), consistently mention it; otherwise, the model might suggest one way in one prompt and another way later.

Vague references like “the above code”
When using chat, if you say “the above function” or “the previous output,” be sure the reference is clear. If the conversation is long and you say, “Refactor the above code,” the AI might lose track or pick the wrong code snippet to refactor.

It’s safer to either quote the code again or specifically name the function you want refactored. Models have a limited attention window, and although many LLMs can refer to prior parts of the conversation, giving it explicit context again can help avoid confusion. This is especially true if some time (or several messages) passed since the code was shown.

Summary and Next Steps
The art of prompting is iterative and creative. As models evolve, prompt best practices might change (for instance, future models might better understand intent with less wording). But the underlying principle remains: communicate effectively, and the AI will serve you better.

In essence, mastering prompt engineering is like mastering a new programming language—the language of instructions for AI. It’s a blend of technical writing, foresight, and interactive debugging of the prompt itself. But once you get good at it, the AI truly starts to feel like an extension of your own mind, because you can reliably extract the solutions you envision (or even those you don’t fully envision yet but can guide the AI to discover) with minimal friction. This skill will likely become as fundamental as knowing how to google things or how to use a debugger—it’s part of the modern developer’s skill set in the age of vibe coding.

If AI can solve about 70% of a problem, how do you approach it as a partner in coding? Chapter 3 looks at how developers really use AI and sets out some “golden rules” for vibe coding.